{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aa847b4c66564c2d8e6ca3b162e2000b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_36416683505240ccafc572176b58e290"
          }
        },
        "606475cd6b6a4ded900979cfd76f0beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5a8645bef354d1c8e6a11d0828814c9",
            "placeholder": "​",
            "style": "IPY_MODEL_4411af1d195343ddafb2c9a25286dc14",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "5cdc4eadde904a3d926156900023ae00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_747959c0990e4a61a39d8259c5eb78b6",
            "placeholder": "​",
            "style": "IPY_MODEL_9decb42068e84edf909b8d475480a1c3",
            "value": ""
          }
        },
        "ea77bc9c5fea4125abe8370f6778c532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_1cd1c4d8a10646bc97e72370d41c1275",
            "style": "IPY_MODEL_54c1e7ae89e04114bc7d723919a458e2",
            "value": false
          }
        },
        "3ce89a388d3b44a0bba2cf9b5665b7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_5bb5f98e4de244668835049b8a32fbc1",
            "style": "IPY_MODEL_bc836e7e92054f6194cecb7151311b83",
            "tooltip": ""
          }
        },
        "f0f64be9effd43e982240eaa15736426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a5897e6b17248379cad5df327c419b8",
            "placeholder": "​",
            "style": "IPY_MODEL_60cb1f4ad19d4187bb48b199c95d4d18",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "36416683505240ccafc572176b58e290": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "b5a8645bef354d1c8e6a11d0828814c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4411af1d195343ddafb2c9a25286dc14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "747959c0990e4a61a39d8259c5eb78b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9decb42068e84edf909b8d475480a1c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cd1c4d8a10646bc97e72370d41c1275": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54c1e7ae89e04114bc7d723919a458e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bb5f98e4de244668835049b8a32fbc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc836e7e92054f6194cecb7151311b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "2a5897e6b17248379cad5df327c419b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60cb1f4ad19d4187bb48b199c95d4d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6207bc36c3a4f12b475b425aa4062c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a597d746982f4ac0ae3e83bbefed99c5",
            "placeholder": "​",
            "style": "IPY_MODEL_1ab674caf68641f0909d102ea7c05caf",
            "value": "Connecting..."
          }
        },
        "a597d746982f4ac0ae3e83bbefed99c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ab674caf68641f0909d102ea7c05caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bb28457aa7b4920b26386bd01f20be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8fbdcdbb4214b779f8117248bd1aa0f",
              "IPY_MODEL_d863c9627526417ab63d25a521a6ff43",
              "IPY_MODEL_bee198ad44f9479fb3b600f6974be603"
            ],
            "layout": "IPY_MODEL_3dc8ecc15a7e4009a4a194925f7850a1"
          }
        },
        "e8fbdcdbb4214b779f8117248bd1aa0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6777ff73039149378291476f05659cd2",
            "placeholder": "​",
            "style": "IPY_MODEL_2dc03a5067ed46c58fb758cdc0a66376",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d863c9627526417ab63d25a521a6ff43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77191c39346c404d82184a46738093bc",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1be5349b7c74dfb83b03df2626245bd",
            "value": 3
          }
        },
        "bee198ad44f9479fb3b600f6974be603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebb2f4f0a6d04511850dd5b2c5409507",
            "placeholder": "​",
            "style": "IPY_MODEL_a314c6523a2148278db6067e7241c1d4",
            "value": " 3/3 [00:04&lt;00:00,  1.37s/it]"
          }
        },
        "3dc8ecc15a7e4009a4a194925f7850a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6777ff73039149378291476f05659cd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dc03a5067ed46c58fb758cdc0a66376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77191c39346c404d82184a46738093bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1be5349b7c74dfb83b03df2626245bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ebb2f4f0a6d04511850dd5b2c5409507": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a314c6523a2148278db6067e7241c1d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c30437ba4294798bab8b4727a800d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c83c2b7751924e6b889c5341f190b948",
              "IPY_MODEL_916e0bd964da4dbd906d22262e917816",
              "IPY_MODEL_abda45a72adc421891b77000b3996e45"
            ],
            "layout": "IPY_MODEL_713118603e354f08a59c885fd834f384"
          }
        },
        "c83c2b7751924e6b889c5341f190b948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78ff022aabf24d89ad82a3060f0dab2e",
            "placeholder": "​",
            "style": "IPY_MODEL_3adc900e287a40e9868c3e21b5881369",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "916e0bd964da4dbd906d22262e917816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46d25ba7299b4276b2197dbaf6fc61f4",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34d1989a8e374687b6a992473ffd4f43",
            "value": 3
          }
        },
        "abda45a72adc421891b77000b3996e45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_308f9d3c13cc410e9c96c1a5f7d0468a",
            "placeholder": "​",
            "style": "IPY_MODEL_cd6e859c3057445d80aa925bbcf8be26",
            "value": " 3/3 [00:05&lt;00:00,  1.84s/it]"
          }
        },
        "713118603e354f08a59c885fd834f384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78ff022aabf24d89ad82a3060f0dab2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3adc900e287a40e9868c3e21b5881369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46d25ba7299b4276b2197dbaf6fc61f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34d1989a8e374687b6a992473ffd4f43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "308f9d3c13cc410e9c96c1a5f7d0468a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd6e859c3057445d80aa925bbcf8be26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/billvo2212/reproducing-research-paper-result/blob/main/RSITMD_training_eval_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RSITMD Training & Evaluation for RSGPT\n",
        "\n",
        "This notebook trains and evaluates RSGPT on the RSITMD dataset.\n",
        "\n",
        "**Key Fixes from Previous Version:**\n",
        "1. Fixed `iters_per_epoch` (was 10000, now ~830 based on dataset size)\n",
        "2. Fixed evaluation imports to avoid circular import errors\n",
        "3. Added proper kernel restart handling for evaluation\n",
        "4. Fixed METEOR calculation\n",
        "5. Added checkpoint resume capability"
      ],
      "metadata": {
        "id": "intro_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1: SETUP AND INSTALL DEPENDENCIES"
      ],
      "metadata": {
        "id": "step1_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1.1: Install Dependencies\n",
        "# Core stack (CUDA 12.1 wheels work well on Colab A100/L4/T4)\n",
        "!pip -q install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install transformers==4.41.2 accelerate==0.31.0 peft==0.11.1 bitsandbytes==0.43.1 sentencepiece\n",
        "!pip -q install decord==0.6.0 datasets==2.20.0 rouge-score==0.1.2 pycocoevalcap\n",
        "\n",
        "# Java (for METEOR evaluation)\n",
        "!apt-get -yqq update && apt-get -yqq install openjdk-17-jre-headless\n",
        "\n",
        "# Install additional evaluation dependencies\n",
        "!pip install -q nltk\n",
        "\n",
        "# Install iopath and other potentially missing packages\n",
        "!pip install -q iopath omegaconf timm einops\n",
        "\n",
        "# Install additional packages that RSGPT might need\n",
        "!pip install -q webdataset braceexpand\n",
        "\n",
        "!apt-get update -qq\n",
        "!apt-get install -y default-jdk -qq\n",
        "\n",
        "import os\n",
        "os.environ['_JAVA_OPTIONS'] = '-Xmx8g'\n",
        "\n",
        "# Download NLTK data for METEOR\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# Login to Hugging Face if your base LLM requires it\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()  # or set HF_TOKEN env var"
      ],
      "metadata": {
        "id": "install_deps",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "aa847b4c66564c2d8e6ca3b162e2000b",
            "606475cd6b6a4ded900979cfd76f0beb",
            "5cdc4eadde904a3d926156900023ae00",
            "ea77bc9c5fea4125abe8370f6778c532",
            "3ce89a388d3b44a0bba2cf9b5665b7b6",
            "f0f64be9effd43e982240eaa15736426",
            "36416683505240ccafc572176b58e290",
            "b5a8645bef354d1c8e6a11d0828814c9",
            "4411af1d195343ddafb2c9a25286dc14",
            "747959c0990e4a61a39d8259c5eb78b6",
            "9decb42068e84edf909b8d475480a1c3",
            "1cd1c4d8a10646bc97e72370d41c1275",
            "54c1e7ae89e04114bc7d723919a458e2",
            "5bb5f98e4de244668835049b8a32fbc1",
            "bc836e7e92054f6194cecb7151311b83",
            "2a5897e6b17248379cad5df327c419b8",
            "60cb1f4ad19d4187bb48b199c95d4d18",
            "d6207bc36c3a4f12b475b425aa4062c8",
            "a597d746982f4ac0ae3e83bbefed99c5",
            "1ab674caf68641f0909d102ea7c05caf"
          ]
        },
        "outputId": "4c446b8d-3ec6-440f-803c-7244ebe33b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.9/780.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m153.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m143.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m148.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package libatspi2.0-0:amd64.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
            "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../01-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../02-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../03-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package at-spi2-core.\n",
            "Preparing to unpack .../04-at-spi2-core_2.44.0-3_amd64.deb ...\n",
            "Unpacking at-spi2-core (2.44.0-3) ...\n",
            "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
            "Preparing to unpack .../05-openjdk-11-jre-headless_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../06-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../07-openjdk-11-jre_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../08-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package openjdk-11-jdk-headless:amd64.\n",
            "Preparing to unpack .../09-openjdk-11-jdk-headless_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jdk-headless.\n",
            "Preparing to unpack .../10-default-jdk-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jdk-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
            "Preparing to unpack .../11-openjdk-11-jdk_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jdk.\n",
            "Preparing to unpack .../12-default-jdk_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jdk (2:1.11-72build2) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../13-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../14-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libatk1.0-data.\n",
            "Preparing to unpack .../15-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
            "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk1.0-0:amd64.\n",
            "Preparing to unpack .../16-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
            "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
            "Preparing to unpack .../17-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
            "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../18-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../19-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../20-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../21-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../22-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../23-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service → /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Setting up openjdk-11-jdk-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode\n",
            "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up openjdk-11-jdk:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Setting up default-jdk-headless (2:1.11-72build2) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up default-jdk (2:1.11-72build2) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Setting up at-spi2-core (2.44.0-3) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa847b4c66564c2d8e6ca3b162e2000b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1.2: Mount Google Drive and Setup Paths\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURE YOUR PATHS HERE\n",
        "# ============================================\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/data/RSGPT\"       # RSGPT repo path\n",
        "DATA_ROOT = \"/content/drive/MyDrive/data/rsgpt\"          # General data root\n",
        "OUT_ROOT = \"/content/drive/MyDrive/outputs/rsgpt\"        # Output directory\n",
        "\n",
        "# RSITMD specific paths\n",
        "RSITMD_BASE = f\"{PROJECT_ROOT}/dataset/RSITMD\"           # RSITMD dataset location\n",
        "\n",
        "# Create directories\n",
        "!mkdir -p {RSITMD_BASE}/images\n",
        "!mkdir -p {OUT_ROOT}/rsitmd_finetuned\n",
        "\n",
        "print(f\"✓ Directories configured:\")\n",
        "print(f\"  Project: {PROJECT_ROOT}\")\n",
        "print(f\"  RSITMD Dataset: {RSITMD_BASE}\")\n",
        "print(f\"  Output: {OUT_ROOT}\")"
      ],
      "metadata": {
        "id": "mount_drive",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32eecdd5-22e7-4ac2-8b89-f29227837be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✓ Directories configured:\n",
            "  Project: /content/drive/MyDrive/data/RSGPT\n",
            "  RSITMD Dataset: /content/drive/MyDrive/data/RSGPT/dataset/RSITMD\n",
            "  Output: /content/drive/MyDrive/outputs/rsgpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2: DOWNLOAD AND PREPARE RSITMD DATASET"
      ],
      "metadata": {
        "id": "step2_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.1: Check RSITMD Dataset\n",
        "import os\n",
        "import json\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 2.1: Check RSITMD Dataset\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check for annotation file\n",
        "ann_file = f\"{RSITMD_BASE}/dataset_RSITMD.json\"\n",
        "if os.path.exists(ann_file):\n",
        "    print(f\"✓ Found annotation file: {ann_file}\")\n",
        "    with open(ann_file) as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"  Total images: {len(data.get('images', []))}\")\n",
        "else:\n",
        "    print(f\"❌ Annotation file not found: {ann_file}\")\n",
        "    print(\"\\nPlease download RSITMD dataset and place dataset_RSITMD.json in:\")\n",
        "    print(f\"  {RSITMD_BASE}/\")\n",
        "\n",
        "# Check for images\n",
        "img_dir = f\"{RSITMD_BASE}/images\"\n",
        "if os.path.exists(img_dir):\n",
        "    img_files = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png', '.tif'))]\n",
        "    print(f\"✓ Found {len(img_files)} images in {img_dir}\")\n",
        "else:\n",
        "    print(f\"❌ Images directory not found: {img_dir}\")\n",
        "\n",
        "# Show sample annotation\n",
        "if os.path.exists(ann_file):\n",
        "    print(\"\\nSample annotation entry:\")\n",
        "    print(json.dumps(data['images'][0], indent=2)[:500] + \"...\")"
      ],
      "metadata": {
        "id": "check_dataset",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4225d50b-3727-41e0-a6cb-284e61cb14a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 2.1: Check RSITMD Dataset\n",
            "============================================================\n",
            "✓ Found annotation file: /content/drive/MyDrive/data/RSGPT/dataset/RSITMD/dataset_RSITMD.json\n",
            "  Total images: 4743\n",
            "✓ Found 4744 images in /content/drive/MyDrive/data/RSGPT/dataset/RSITMD/images\n",
            "\n",
            "Sample annotation entry:\n",
            "{\n",
            "  \"filename\": \"baseballfield_452.tif\",\n",
            "  \"imgid\": 0,\n",
            "  \"sentences\": [\n",
            "    {\n",
            "      \"tokens\": [\n",
            "        \"there\",\n",
            "        \"is\",\n",
            "        \"a\",\n",
            "        \"baseball\",\n",
            "        \"field\",\n",
            "        \"beside\",\n",
            "        \"the\",\n",
            "        \"green\",\n",
            "        \"amusement\",\n",
            "        \"park\",\n",
            "        \"around\",\n",
            "        \"the\",\n",
            "        \"red\",\n",
            "        \"track\"\n",
            "      ],\n",
            "      \"raw\": \"There is a baseball field beside the green amusement park around the red track.\",\n",
            "      \"imgid\": 0,\n",
            "      \"sentid\": 0\n",
            "    },\n",
            "    {\n",
            "      \"tokens\": [\n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3: CREATE TRAIN/VAL/TEST SPLITS"
      ],
      "metadata": {
        "id": "step3_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3.1: Create Train/Val/Test Splits\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 3.1: Create Train/Val/Test Splits\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load annotations\n",
        "with open(f\"{RSITMD_BASE}/dataset_RSITMD.json\", 'r') as f:\n",
        "    rsitmd_data = json.load(f)\n",
        "\n",
        "# Parse RSITMD format\n",
        "print(\"Parsing RSITMD format...\")\n",
        "\n",
        "image_captions = {}\n",
        "train_images = []\n",
        "val_images = []\n",
        "test_images = []\n",
        "\n",
        "for img in rsitmd_data['images']:\n",
        "    filename = img['filename']\n",
        "\n",
        "    # Extract captions from sentences\n",
        "    captions = [sent['raw'] for sent in img['sentences']]\n",
        "    image_captions[filename] = captions\n",
        "\n",
        "    # Use the existing split field\n",
        "    split = img.get('split', 'train')\n",
        "    if split == 'train':\n",
        "        train_images.append(filename)\n",
        "    elif split == 'val':\n",
        "        val_images.append(filename)\n",
        "    elif split == 'test':\n",
        "        test_images.append(filename)\n",
        "    else:\n",
        "        train_images.append(filename)  # Default to train\n",
        "\n",
        "# Statistics\n",
        "total_images = len(image_captions)\n",
        "total_captions = sum(len(caps) for caps in image_captions.values())\n",
        "\n",
        "print(f\"\\n✓ Parsed successfully!\")\n",
        "print(f\"Total unique images: {total_images}\")\n",
        "print(f\"Total captions: {total_captions}\")\n",
        "print(f\"Average captions per image: {total_captions / total_images:.1f}\")\n",
        "\n",
        "# Split sizes\n",
        "print(f\"\\nSplit sizes (from dataset):\")\n",
        "print(f\"  Train: {len(train_images)} images ({100*len(train_images)/total_images:.1f}%)\")\n",
        "print(f\"  Val:   {len(val_images)} images ({100*len(val_images)/total_images:.1f}%)\")\n",
        "print(f\"  Test:  {len(test_images)} images ({100*len(test_images)/total_images:.1f}%)\")\n",
        "\n",
        "# If no val/test splits exist, create them\n",
        "if len(val_images) == 0 or len(test_images) == 0:\n",
        "    print(\"\\n⚠️ No predefined val/test splits found. Creating 70/15/15 split...\")\n",
        "    all_images = list(image_captions.keys())\n",
        "    random.seed(42)\n",
        "    random.shuffle(all_images)\n",
        "\n",
        "    n_total = len(all_images)\n",
        "    n_train = int(0.70 * n_total)\n",
        "    n_val = int(0.15 * n_total)\n",
        "\n",
        "    train_images = all_images[:n_train]\n",
        "    val_images = all_images[n_train:n_train + n_val]\n",
        "    test_images = all_images[n_train + n_val:]\n",
        "\n",
        "    print(f\"  Train: {len(train_images)} images\")\n",
        "    print(f\"  Val:   {len(val_images)} images\")\n",
        "    print(f\"  Test:  {len(test_images)} images\")\n",
        "\n",
        "# Store for later use\n",
        "RSITMD_SPLITS = {\n",
        "    'train': train_images,\n",
        "    'val': val_images,\n",
        "    'test': test_images,\n",
        "    'image_captions': image_captions\n",
        "}\n",
        "\n",
        "# Calculate average caption length\n",
        "all_caption_words = sum(len(cap.split()) for caps in image_captions.values() for cap in caps)\n",
        "avg_caption_len = all_caption_words / total_captions\n",
        "print(f\"\\nAverage caption length: {avg_caption_len:.1f} words\")\n",
        "\n",
        "print(\"\\n✓ RSITMD_SPLITS created!\")"
      ],
      "metadata": {
        "id": "create_splits",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "911306f0-0f82-446e-962d-a36f148a02c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 3.1: Create Train/Val/Test Splits\n",
            "============================================================\n",
            "Parsing RSITMD format...\n",
            "\n",
            "✓ Parsed successfully!\n",
            "Total unique images: 4743\n",
            "Total captions: 23715\n",
            "Average captions per image: 5.0\n",
            "\n",
            "Split sizes (from dataset):\n",
            "  Train: 4291 images (90.5%)\n",
            "  Val:   0 images (0.0%)\n",
            "  Test:  452 images (9.5%)\n",
            "\n",
            "⚠️ No predefined val/test splits found. Creating 70/15/15 split...\n",
            "  Train: 3320 images\n",
            "  Val:   711 images\n",
            "  Test:  712 images\n",
            "\n",
            "Average caption length: 10.4 words\n",
            "\n",
            "✓ RSITMD_SPLITS created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3.2: Create RSGPT Instruction Format\n",
        "import json\n",
        "import random\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 3.2: Create RSGPT Instruction Format\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def create_instruction_format(image_list, image_captions, split_name):\n",
        "    \"\"\"\n",
        "    Create RSGPT instruction format.\n",
        "    \"\"\"\n",
        "    annotations = []\n",
        "\n",
        "    # Standard prompts for captioning\n",
        "    prompts = [\n",
        "        \"Describe the content of the image.\",\n",
        "        \"What is shown in this remote sensing image?\",\n",
        "        \"Briefly describe the content of the image.\",\n",
        "        \"Provide a description of this aerial image.\",\n",
        "        \"What can you see in this satellite image?\"\n",
        "    ]\n",
        "\n",
        "    for idx, filename in enumerate(image_list):\n",
        "        captions = image_captions.get(filename, [])\n",
        "        if not captions:\n",
        "            continue\n",
        "\n",
        "        annotations.append({\n",
        "            \"image_id\": idx,\n",
        "            \"filename\": filename,\n",
        "            \"text_input\": prompts[idx % len(prompts)],\n",
        "            \"text_output\": captions\n",
        "        })\n",
        "\n",
        "    return {\"annotations\": annotations}\n",
        "\n",
        "# Create instruction files for each split\n",
        "for split_name in ['train', 'val', 'test']:\n",
        "    image_list = RSITMD_SPLITS[split_name]\n",
        "    data = create_instruction_format(image_list, RSITMD_SPLITS['image_captions'], split_name)\n",
        "\n",
        "    output_path = f\"{RSITMD_BASE}/rsitmd_cap_processed_instruction_{split_name}.json\"\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "    print(f\"✓ Created {split_name}: {len(data['annotations'])} samples\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nSample annotation:\")\n",
        "print(json.dumps(data['annotations'][0], indent=2))"
      ],
      "metadata": {
        "id": "create_instruction_format",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be85d024-6c89-4348-f1ff-ae94c1afbd26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 3.2: Create RSGPT Instruction Format\n",
            "============================================================\n",
            "✓ Created train: 3320 samples\n",
            "✓ Created val: 711 samples\n",
            "✓ Created test: 712 samples\n",
            "\n",
            "Sample annotation:\n",
            "{\n",
            "  \"image_id\": 0,\n",
            "  \"filename\": \"storagetanks_4442.tif\",\n",
            "  \"text_input\": \"Describe the content of the image.\",\n",
            "  \"text_output\": [\n",
            "    \"There is a lot of grass on the ground.\",\n",
            "    \"There is a lot of grass on the ground.\",\n",
            "    \"There is a lot of grass on the ground.\",\n",
            "    \"There is a lot of grass on the ground.\",\n",
            "    \"There is a lot of grass on the ground.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ciZq6LzqvTEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3.3: Create COCO-format GT for Evaluation\n",
        "import json\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 3.3: Create COCO-format GT for Evaluation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def create_coco_gt(image_list, image_captions, output_path):\n",
        "    \"\"\"\n",
        "    Create COCO-format ground truth for pycocoevalcap evaluation.\n",
        "    \"\"\"\n",
        "    coco_format = {\n",
        "        \"info\": {\"description\": \"RSITMD\", \"version\": \"1.0\"},\n",
        "        \"licenses\": [],\n",
        "        \"type\": \"captions\",\n",
        "        \"images\": [],\n",
        "        \"annotations\": []\n",
        "    }\n",
        "\n",
        "    ann_id = 0\n",
        "    for img_id, filename in enumerate(image_list):\n",
        "        captions = image_captions.get(filename, [])\n",
        "        if not captions:\n",
        "            continue\n",
        "\n",
        "        # Add image\n",
        "        coco_format[\"images\"].append({\n",
        "            \"id\": img_id,\n",
        "            \"file_name\": filename\n",
        "        })\n",
        "\n",
        "        # Add all captions for this image\n",
        "        for caption in captions:\n",
        "            coco_format[\"annotations\"].append({\n",
        "                \"id\": ann_id,\n",
        "                \"image_id\": img_id,\n",
        "                \"caption\": caption\n",
        "            })\n",
        "            ann_id += 1\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(coco_format, f, indent=2)\n",
        "\n",
        "    return len(coco_format['images']), len(coco_format['annotations'])\n",
        "\n",
        "# Create GT files for val and test\n",
        "for split_name in ['val', 'test']:\n",
        "    output_path = f\"{RSITMD_BASE}/rsitmd_{split_name}_gt.json\"\n",
        "    n_images, n_anns = create_coco_gt(\n",
        "        RSITMD_SPLITS[split_name],\n",
        "        RSITMD_SPLITS['image_captions'],\n",
        "        output_path\n",
        "    )\n",
        "    print(f\"✓ Created {split_name} GT: {n_images} images, {n_anns} captions\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Dataset Preparation Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nCreated files:\")\n",
        "!ls -la {RSITMD_BASE}/*.json"
      ],
      "metadata": {
        "id": "create_coco_gt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a90eca-8ef3-4a5f-d2cf-ee99c0291e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 3.3: Create COCO-format GT for Evaluation\n",
            "============================================================\n",
            "✓ Created val GT: 711 images, 3555 captions\n",
            "✓ Created test GT: 712 images, 3560 captions\n",
            "\n",
            "============================================================\n",
            "Dataset Preparation Complete!\n",
            "============================================================\n",
            "\n",
            "Created files:\n",
            "-rw------- 1 root root 5678376 Dec 19 01:10 /content/drive/MyDrive/data/RSGPT/dataset/RSITMD/dataset_RSITMD.json\n",
            "-rw------- 1 root root  361855 Feb  2 00:11 /content/drive/MyDrive/data/RSGPT/dataset/RSITMD/rsitmd_cap_processed_instruction_test.json\n",
            "-rw------- 1 root root 1687719 Feb  2 00:11 /content/drive/MyDrive/data/RSGPT/dataset/RSITMD/rsitmd_cap_processed_instruction_train.json\n",
            "-rw------- 1 root root  362740 Feb  2 00:11 /content/drive/MyDrive/data/RSGPT/dataset/RSITMD/rsitmd_cap_processed_instruction_val.json\n",
            "-rw------- 1 root root  509417 Feb  2 00:11 /content/drive/MyDrive/data/RSGPT/dataset/RSITMD/rsitmd_test_gt.json\n",
            "-rw------- 1 root root  510095 Feb  2 00:11 /content/drive/MyDrive/data/RSGPT/dataset/RSITMD/rsitmd_val_gt.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 4: REGISTER DATASET AND CREATE CONFIGS"
      ],
      "metadata": {
        "id": "step4_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4.1: Register RSITMD Dataset Builder\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 4.1: Register RSITMD Dataset Builder\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create Dataset Builder\n",
        "rsitmd_builder_code = '''\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "from rsgpt.datasets.builders.base_dataset_builder import BaseDatasetBuilder\n",
        "from rsgpt.datasets.datasets.rsicd_instruction_dataset import RSICDInstructionDataset\n",
        "from rsgpt.common.registry import registry\n",
        "\n",
        "@registry.register_builder(\"rsitmd_instruction\")\n",
        "class RSITMDInstructionBuilder(BaseDatasetBuilder):\n",
        "    train_dataset_cls = RSICDInstructionDataset\n",
        "    eval_dataset_cls = RSICDInstructionDataset\n",
        "\n",
        "    DATASET_CONFIG_DICT = {\n",
        "        \"default\": \"configs/datasets/rsitmd/defaults.yaml\",\n",
        "    }\n",
        "\n",
        "    def build_datasets(self):\n",
        "        logging.info(\"Building RSITMD Instruction datasets...\")\n",
        "        self.build_processors()\n",
        "\n",
        "        build_info = self.config.build_info\n",
        "        storage_path = build_info.storage\n",
        "\n",
        "        datasets = dict()\n",
        "\n",
        "        # Load train split\n",
        "        train_ann_path = os.path.join(storage_path, \"rsitmd_cap_processed_instruction_train.json\")\n",
        "        if os.path.exists(train_ann_path):\n",
        "            datasets[\"train\"] = self.train_dataset_cls(\n",
        "                vis_processor=self.vis_processors[\"train\"],\n",
        "                text_processor=self.text_processors[\"train\"],\n",
        "                ann_paths=[train_ann_path],\n",
        "                vis_root=os.path.join(storage_path, \"images\"),\n",
        "            )\n",
        "            logging.info(f\"Loaded RSITMD train dataset: {len(datasets['train'])} samples\")\n",
        "\n",
        "        return datasets\n",
        "'''\n",
        "\n",
        "builder_path = f\"{PROJECT_ROOT}/rsgpt/datasets/builders/rsitmd_builder.py\"\n",
        "os.makedirs(os.path.dirname(builder_path), exist_ok=True)\n",
        "with open(builder_path, 'w') as f:\n",
        "    f.write(rsitmd_builder_code)\n",
        "print(f\"✓ Created dataset builder: {builder_path}\")\n",
        "\n",
        "# Add import to __init__.py\n",
        "init_path = f\"{PROJECT_ROOT}/rsgpt/datasets/builders/__init__.py\"\n",
        "if os.path.exists(init_path):\n",
        "    with open(init_path, 'r') as f:\n",
        "        content = f.read()\n",
        "    if 'rsitmd_builder' not in content:\n",
        "        with open(init_path, 'a') as f:\n",
        "            f.write(\"\\nfrom rsgpt.datasets.builders.rsitmd_builder import RSITMDInstructionBuilder\\n\")\n",
        "        print(\"✓ Added import to __init__.py\")\n",
        "    else:\n",
        "        print(\"✓ Import already exists in __init__.py\")\n",
        "\n",
        "# Create Dataset Config\n",
        "config_dir = f\"{PROJECT_ROOT}/rsgpt/configs/datasets/rsitmd\"\n",
        "os.makedirs(config_dir, exist_ok=True)\n",
        "\n",
        "rsitmd_dataset_config = '''datasets:\n",
        "  rsitmd_instruction:\n",
        "    data_type: images\n",
        "    vis_processor:\n",
        "      train:\n",
        "        name: \"rs_image_train\"\n",
        "        image_size: 224\n",
        "    text_processor:\n",
        "      train:\n",
        "        name: \"blip_caption\"\n",
        "    build_info:\n",
        "      storage: dataset/RSITMD/\n",
        "'''\n",
        "\n",
        "config_path = f\"{config_dir}/defaults.yaml\"\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(rsitmd_dataset_config)\n",
        "print(f\"✓ Created dataset config: {config_path}\")\n",
        "\n",
        "print(\"\\n✓ Step 4.1 Complete!\")"
      ],
      "metadata": {
        "id": "register_builder",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f63b1f3-3d93-4a1a-bf46-cc117a12c534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 4.1: Register RSITMD Dataset Builder\n",
            "============================================================\n",
            "✓ Created dataset builder: /content/drive/MyDrive/data/RSGPT/rsgpt/datasets/builders/rsitmd_builder.py\n",
            "✓ Import already exists in __init__.py\n",
            "✓ Created dataset config: /content/drive/MyDrive/data/RSGPT/rsgpt/configs/datasets/rsitmd/defaults.yaml\n",
            "\n",
            "✓ Step 4.1 Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4.2: Create Training Config (with pre-train from RSICap)\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 4.2: Create RSITMD Training Config\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# CALCULATE PROPER iters_per_epoch\n",
        "# ============================================\n",
        "# RSITMD has ~3320 training images\n",
        "# With batch_size=4 and accum_grad_iters=4 (effective batch 16)\n",
        "# Real iterations per epoch = 3320 / 4 = 830\n",
        "# We can set it to 400-500 for faster training with multiple passes\n",
        "TRAIN_SIZE = len(RSITMD_SPLITS['train'])\n",
        "BATCH_SIZE = 4\n",
        "ITERS_PER_EPOCH = max(200, TRAIN_SIZE // BATCH_SIZE)  # ~830 for RSITMD\n",
        "\n",
        "print(f\"Dataset size: {TRAIN_SIZE} images\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Calculated iters_per_epoch: {ITERS_PER_EPOCH}\")\n",
        "\n",
        "# ============================================\n",
        "# USE RSICap PRETRAINED CHECKPOINT (same as RSICD pipeline!)\n",
        "# ============================================\n",
        "PRETRAINED_CHECKPOINT = \"/content/drive/MyDrive/outputs/rsgpt/rsicap_split_checkpoint/20251229133/checkpoint_8.pth\"\n",
        "\n",
        "rsitmd_train_config = f'''model:\n",
        "  arch: rsgpt\n",
        "  model_type: vicuna13b\n",
        "  freeze_vit: True\n",
        "  freeze_qformer: False\n",
        "  max_txt_len: 160\n",
        "  llm_model: lmsys/vicuna-13b-v1.5\n",
        "  end_sym: \"###\"\n",
        "  prompt_path: \"prompts/alignment.txt\"\n",
        "\n",
        "datasets:\n",
        "  rsitmd_instruction:\n",
        "    vis_processor:\n",
        "      train:\n",
        "        name: \"rs_image_train\"\n",
        "        image_size: 224\n",
        "    text_processor:\n",
        "      train:\n",
        "        name: \"blip_caption\"\n",
        "    build_info:\n",
        "      storage: dataset/RSITMD/\n",
        "\n",
        "run:\n",
        "  task: image_text_pretrain\n",
        "  lr_sched: \"linear_warmup_cosine_lr\"\n",
        "  init_lr: 5e-6\n",
        "  min_lr: 1e-7\n",
        "  warmup_lr: 1e-8\n",
        "  warmup_steps: 200\n",
        "  weight_decay: 0.05\n",
        "\n",
        "  max_epoch: 30\n",
        "  iters_per_epoch: {ITERS_PER_EPOCH}\n",
        "  batch_size_train: {BATCH_SIZE}\n",
        "  batch_size_eval: 4\n",
        "  accum_grad_iters: 4              # Effective batch size = 4*4 = 16\n",
        "  num_workers: 4\n",
        "\n",
        "  seed: 42\n",
        "  output_dir: \"{OUT_ROOT}/rsitmd_finetuned_v2\"\n",
        "\n",
        "  amp: True\n",
        "  resume_ckpt_path: \"{PRETRAINED_CHECKPOINT}\"\n",
        "\n",
        "  evaluate: False\n",
        "  train_splits: [\"train\"]\n",
        "\n",
        "  device: \"cuda\"\n",
        "  world_size: 1\n",
        "  dist_url: \"env://\"\n",
        "  distributed: True\n",
        "'''\n",
        "\n",
        "# Save config\n",
        "config_dir = f\"{PROJECT_ROOT}/train_configs\"\n",
        "os.makedirs(config_dir, exist_ok=True)\n",
        "config_path = f\"{config_dir}/rsitmd_train.yaml\"\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(rsitmd_train_config)\n",
        "\n",
        "print(f\"\\n✓ Created training config: {config_path}\")\n",
        "print(f\"\\nTraining settings:\")\n",
        "print(f\"  - Epochs: 15\")\n",
        "print(f\"  - iters_per_epoch: {ITERS_PER_EPOCH} (FIXED!)\")\n",
        "print(f\"  - Learning rate: 1e-5\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE} (x4 accumulation = effective 16)\")\n",
        "print(f\"  - Estimated training time: ~2-3 hours on A100\")"
      ],
      "metadata": {
        "id": "create_train_config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013663dc-0a71-4007-9409-bf3d2f25785d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 4.2: Create RSITMD Training Config\n",
            "============================================================\n",
            "Dataset size: 3320 images\n",
            "Batch size: 4\n",
            "Calculated iters_per_epoch: 830\n",
            "\n",
            "✓ Created training config: /content/drive/MyDrive/data/RSGPT/train_configs/rsitmd_train.yaml\n",
            "\n",
            "Training settings:\n",
            "  - Epochs: 15\n",
            "  - iters_per_epoch: 830 (FIXED!)\n",
            "  - Learning rate: 1e-5\n",
            "  - Batch size: 4 (x4 accumulation = effective 16)\n",
            "  - Estimated training time: ~2-3 hours on A100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4.3: Create Evaluation Config\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 4.3: Create RSITMD Evaluation Config\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rsitmd_eval_config = '''model:\n",
        "  arch: rsgpt\n",
        "  model_type: vicuna13b\n",
        "  max_txt_len: 160\n",
        "  llm_model: lmsys/vicuna-13b-v1.5\n",
        "  end_sym: \"###\"\n",
        "\n",
        "datasets:\n",
        "  rsitmd_instruction:\n",
        "    vis_processor:\n",
        "      train:\n",
        "        name: \"rs_image_train\"\n",
        "        image_size: 224\n",
        "    text_processor:\n",
        "      train:\n",
        "        name: \"blip_caption\"\n",
        "\n",
        "run:\n",
        "  task: image_text_pretrain\n",
        "'''\n",
        "\n",
        "eval_config_path = f\"{PROJECT_ROOT}/eval_configs/rsitmd_eval.yaml\"\n",
        "os.makedirs(os.path.dirname(eval_config_path), exist_ok=True)\n",
        "with open(eval_config_path, 'w') as f:\n",
        "    f.write(rsitmd_eval_config)\n",
        "\n",
        "print(f\"✓ Created eval config: {eval_config_path}\")"
      ],
      "metadata": {
        "id": "create_eval_config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e16b97-83dc-496d-b6d5-bd56c54f35c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 4.3: Create RSITMD Evaluation Config\n",
            "============================================================\n",
            "✓ Created eval config: /content/drive/MyDrive/data/RSGPT/eval_configs/rsitmd_eval.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 5: TRAIN THE MODEL"
      ],
      "metadata": {
        "id": "step5_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 5.1: Start Training\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 5.1: Start Training\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Clear GPU memory\n",
        "print(\"\\n[1] Clearing GPU memory...\")\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"  ✓ GPU memory cleared\")\n",
        "\n",
        "# Check GPU\n",
        "print(\"\\n[2] Checking GPU...\")\n",
        "!nvidia-smi\n",
        "\n",
        "# Change to RSGPT directory\n",
        "%cd {PROJECT_ROOT}\n",
        "\n",
        "# Start training\n",
        "print(\"\\n[3] Starting RSITMD training...\")\n",
        "print(f\"  Estimated time: ~2-3 hours on A100\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "!torchrun --nproc_per_node=1 train.py --cfg-path train_configs/rsitmd_train.yaml"
      ],
      "metadata": {
        "id": "start_training",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c4197e5-e3f0-4611-9b1a-28c47f19995a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 5.1: Start Training\n",
            "============================================================\n",
            "\n",
            "[1] Clearing GPU memory...\n",
            "  ✓ GPU memory cleared\n",
            "\n",
            "[2] Checking GPU...\n",
            "Mon Feb  2 00:13:39 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   36C    P0             55W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "/content/drive/MyDrive/data/RSGPT\n",
            "\n",
            "[3] Starting RSITMD training...\n",
            "  Estimated time: ~2-3 hours on A100\n",
            "============================================================\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "| distributed init (rank 0, world 1): env://\n",
            "2026-02-02 00:14:27,240 [INFO] \n",
            "=====  Running Parameters    =====\n",
            "2026-02-02 00:14:27,241 [INFO] {\n",
            "    \"accum_grad_iters\": 4,\n",
            "    \"amp\": true,\n",
            "    \"batch_size_eval\": 4,\n",
            "    \"batch_size_train\": 4,\n",
            "    \"device\": \"cuda\",\n",
            "    \"dist_backend\": \"nccl\",\n",
            "    \"dist_url\": \"env://\",\n",
            "    \"distributed\": true,\n",
            "    \"evaluate\": false,\n",
            "    \"gpu\": 0,\n",
            "    \"init_lr\": 5e-06,\n",
            "    \"iters_per_epoch\": 830,\n",
            "    \"lr_sched\": \"linear_warmup_cosine_lr\",\n",
            "    \"max_epoch\": 30,\n",
            "    \"min_lr\": 1e-07,\n",
            "    \"num_workers\": 4,\n",
            "    \"output_dir\": \"/content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2\",\n",
            "    \"rank\": 0,\n",
            "    \"resume_ckpt_path\": \"/content/drive/MyDrive/outputs/rsgpt/rsicap_split_checkpoint/20251229133/checkpoint_8.pth\",\n",
            "    \"seed\": 42,\n",
            "    \"task\": \"image_text_pretrain\",\n",
            "    \"train_splits\": [\n",
            "        \"train\"\n",
            "    ],\n",
            "    \"warmup_lr\": 1e-08,\n",
            "    \"warmup_steps\": 200,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"world_size\": 1\n",
            "}\n",
            "2026-02-02 00:14:27,241 [INFO] \n",
            "======  Dataset Attributes  ======\n",
            "2026-02-02 00:14:27,241 [INFO] \n",
            "======== rsitmd_instruction =======\n",
            "2026-02-02 00:14:27,241 [INFO] {\n",
            "    \"build_info\": {\n",
            "        \"storage\": \"dataset/RSITMD/\"\n",
            "    },\n",
            "    \"data_type\": \"images\",\n",
            "    \"text_processor\": {\n",
            "        \"train\": {\n",
            "            \"name\": \"blip_caption\"\n",
            "        }\n",
            "    },\n",
            "    \"vis_processor\": {\n",
            "        \"train\": {\n",
            "            \"image_size\": 224,\n",
            "            \"name\": \"rs_image_train\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "2026-02-02 00:14:27,241 [INFO] \n",
            "======  Model Attributes  ======\n",
            "2026-02-02 00:14:27,242 [INFO] {\n",
            "    \"arch\": \"rsgpt\",\n",
            "    \"drop_path_rate\": 0,\n",
            "    \"end_sym\": \"###\",\n",
            "    \"finetuned\": \"\",\n",
            "    \"freeze_qformer\": false,\n",
            "    \"freeze_vit\": true,\n",
            "    \"image_size\": 224,\n",
            "    \"llm_model\": \"lmsys/vicuna-13b-v1.5\",\n",
            "    \"load_finetuned\": false,\n",
            "    \"load_pretrained\": true,\n",
            "    \"max_txt_len\": 160,\n",
            "    \"model_type\": \"vicuna13b\",\n",
            "    \"num_query_token\": 32,\n",
            "    \"pretrained\": \"https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna13b_trimmed.pth\",\n",
            "    \"prompt\": \"\",\n",
            "    \"prompt_path\": \"prompts/alignment.txt\",\n",
            "    \"use_grad_checkpoint\": false,\n",
            "    \"vit_precision\": \"fp16\"\n",
            "}\n",
            "2026-02-02 00:14:27,242 [INFO] Building RSITMD Instruction datasets...\n",
            "2026-02-02 00:14:27,257 [INFO] Loaded RSITMD train dataset: 3320 samples\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 267kB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 978kB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 3.42MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 570/570 [00:00<00:00, 4.70MB/s]\n",
            "Loading VIT\n",
            "2026-02-02 00:14:44,630 [INFO] Downloading: \"https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/eva_vit_g.pth\" to /root/.cache/torch/hub/checkpoints/eva_vit_g.pth\n",
            "\n",
            "100% 1.89G/1.89G [01:26<00:00, 23.3MB/s]\n",
            "2026-02-02 00:16:13,809 [INFO] freeze vision encoder\n",
            "Loading VIT Done\n",
            "Loading Q-Former\n",
            "Loading Q-Former Done\n",
            "Loading LLAMA\n",
            "tokenizer_config.json: 100% 749/749 [00:00<00:00, 6.92MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:01<00:00, 257kB/s]\n",
            "special_tokens_map.json: 100% 438/438 [00:00<00:00, 4.17MB/s]\n",
            "config.json: 100% 638/638 [00:00<00:00, 6.12MB/s]\n",
            "pytorch_model.bin.index.json: 33.4kB [00:00, 22.1MB/s]\n",
            "Downloading shards:   0% 0/3 [00:00<?, ?it/s]\n",
            "pytorch_model-00001-of-00003.bin:   0% 0.00/9.95G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:   0% 945k/9.95G [00:01<5:23:00, 513kB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:   1% 67.9M/9.95G [00:02<05:14, 31.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:   8% 806M/9.95G [00:03<00:28, 325MB/s]  \u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  21% 2.08G/9.95G [00:04<00:11, 677MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  29% 2.89G/9.95G [00:04<00:06, 1.04GB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  32% 3.22G/9.95G [00:05<00:06, 1.07GB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  35% 3.49G/9.95G [00:05<00:07, 899MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  37% 3.69G/9.95G [00:05<00:07, 886MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  39% 3.90G/9.95G [00:05<00:06, 939MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  41% 4.10G/9.95G [00:06<00:06, 956MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  43% 4.23G/9.95G [00:06<00:05, 968MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  44% 4.37G/9.95G [00:06<00:08, 674MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  45% 4.50G/9.95G [00:06<00:07, 691MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  47% 4.64G/9.95G [00:07<00:09, 553MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  48% 4.77G/9.95G [00:08<00:22, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  49% 4.84G/9.95G [00:09<00:30, 167MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  51% 5.04G/9.95G [00:10<00:19, 257MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  52% 5.17G/9.95G [00:10<00:14, 321MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  53% 5.31G/9.95G [00:10<00:11, 407MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  55% 5.44G/9.95G [00:11<00:15, 288MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  61% 6.11G/9.95G [00:11<00:04, 805MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  64% 6.38G/9.95G [00:11<00:05, 615MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  66% 6.58G/9.95G [00:14<00:12, 268MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  67% 6.72G/9.95G [00:14<00:10, 311MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  69% 6.85G/9.95G [00:14<00:09, 337MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  73% 7.25G/9.95G [00:14<00:04, 564MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  75% 7.45G/9.95G [00:14<00:03, 648MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  77% 7.65G/9.95G [00:15<00:04, 540MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  78% 7.79G/9.95G [00:15<00:04, 497MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  80% 7.92G/9.95G [00:18<00:11, 182MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  82% 8.12G/9.95G [00:18<00:07, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  84% 8.32G/9.95G [00:18<00:04, 344MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  86% 8.52G/9.95G [00:18<00:03, 450MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  87% 8.66G/9.95G [00:18<00:02, 522MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  88% 8.79G/9.95G [00:18<00:01, 604MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  90% 8.99G/9.95G [00:18<00:01, 688MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  92% 9.12G/9.95G [00:19<00:01, 576MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  93% 9.21G/9.95G [00:19<00:01, 523MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  94% 9.34G/9.95G [00:19<00:01, 469MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  95% 9.41G/9.95G [00:20<00:01, 449MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  95% 9.48G/9.95G [00:21<00:02, 199MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  96% 9.55G/9.95G [00:21<00:01, 216MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  97% 9.61G/9.95G [00:21<00:01, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  97% 9.68G/9.95G [00:21<00:00, 273MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  98% 9.75G/9.95G [00:21<00:00, 298MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  99% 9.81G/9.95G [00:22<00:00, 320MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin:  99% 9.88G/9.95G [00:22<00:00, 339MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00003.bin: 100% 9.95G/9.95G [00:22<00:00, 446MB/s]\n",
            "Downloading shards:  33% 1/3 [00:22<00:45, 22.78s/it]\n",
            "pytorch_model-00002-of-00003.bin:   0% 0.00/9.90G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:   0% 1.01M/9.90G [00:01<4:27:15, 618kB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:   1% 68.1M/9.90G [00:02<04:29, 36.5MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  10% 1.01G/9.90G [00:03<00:19, 465MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  17% 1.68G/9.90G [00:03<00:10, 770MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  18% 1.82G/9.90G [00:04<00:13, 601MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  20% 1.95G/9.90G [00:04<00:19, 417MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  20% 2.02G/9.90G [00:05<00:26, 297MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  21% 2.08G/9.90G [00:06<00:34, 226MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  22% 2.15G/9.90G [00:07<00:42, 183MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  22% 2.22G/9.90G [00:07<00:43, 177MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  24% 2.35G/9.90G [00:08<00:47, 159MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  31% 3.02G/9.90G [00:09<00:14, 467MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  32% 3.16G/9.90G [00:09<00:15, 447MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  33% 3.29G/9.90G [00:09<00:15, 436MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  34% 3.36G/9.90G [00:09<00:14, 442MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  35% 3.43G/9.90G [00:10<00:14, 457MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  35% 3.49G/9.90G [00:10<00:13, 471MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  36% 3.56G/9.90G [00:10<00:12, 500MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  37% 3.63G/9.90G [00:10<00:12, 519MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  37% 3.69G/9.90G [00:10<00:11, 534MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  38% 3.76G/9.90G [00:10<00:11, 528MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  39% 3.83G/9.90G [00:10<00:11, 524MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  39% 3.90G/9.90G [00:10<00:11, 543MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  40% 3.96G/9.90G [00:10<00:10, 564MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  41% 4.10G/9.90G [00:11<00:09, 630MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  43% 4.23G/9.90G [00:11<00:08, 700MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  44% 4.37G/9.90G [00:11<00:07, 763MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  45% 4.50G/9.90G [00:11<00:06, 828MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  47% 4.70G/9.90G [00:11<00:05, 1.01GB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  49% 4.83G/9.90G [00:11<00:04, 1.08GB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  50% 4.97G/9.90G [00:12<00:06, 786MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  52% 5.10G/9.90G [00:12<00:07, 659MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  53% 5.24G/9.90G [00:12<00:08, 552MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  54% 5.30G/9.90G [00:12<00:08, 520MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  54% 5.37G/9.90G [00:13<00:09, 487MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  55% 5.44G/9.90G [00:13<00:09, 461MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  56% 5.51G/9.90G [00:13<00:10, 429MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  56% 5.57G/9.90G [00:13<00:10, 410MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  57% 5.64G/9.90G [00:13<00:10, 410MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  58% 5.71G/9.90G [00:13<00:10, 393MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  58% 5.77G/9.90G [00:14<00:11, 360MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  59% 5.84G/9.90G [00:14<00:11, 343MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  60% 5.91G/9.90G [00:15<00:25, 157MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  62% 6.18G/9.90G [00:15<00:11, 331MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  63% 6.24G/9.90G [00:15<00:11, 315MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  65% 6.44G/9.90G [00:16<00:07, 474MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  66% 6.58G/9.90G [00:16<00:06, 520MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  68% 6.71G/9.90G [00:16<00:06, 465MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  68% 6.78G/9.90G [00:16<00:07, 443MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  69% 6.84G/9.90G [00:17<00:07, 404MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  70% 6.91G/9.90G [00:17<00:07, 427MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  70% 6.98G/9.90G [00:17<00:07, 414MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  71% 7.05G/9.90G [00:17<00:07, 408MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  72% 7.11G/9.90G [00:17<00:07, 392MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  72% 7.18G/9.90G [00:17<00:06, 397MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  73% 7.25G/9.90G [00:18<00:07, 372MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  74% 7.31G/9.90G [00:18<00:07, 344MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  75% 7.38G/9.90G [00:18<00:07, 327MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  75% 7.45G/9.90G [00:18<00:07, 317MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  76% 7.52G/9.90G [00:18<00:07, 308MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  77% 7.58G/9.90G [00:19<00:07, 304MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  77% 7.65G/9.90G [00:19<00:07, 300MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  78% 7.72G/9.90G [00:19<00:07, 297MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  79% 7.78G/9.90G [00:19<00:07, 296MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  79% 7.85G/9.90G [00:20<00:06, 294MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  81% 7.98G/9.90G [00:20<00:04, 456MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  82% 8.12G/9.90G [00:20<00:02, 596MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  83% 8.25G/9.90G [00:20<00:03, 504MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  84% 8.36G/9.90G [00:20<00:03, 463MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  85% 8.43G/9.90G [00:21<00:03, 440MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  86% 8.50G/9.90G [00:21<00:03, 425MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  87% 8.57G/9.90G [00:21<00:03, 410MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  87% 8.64G/9.90G [00:21<00:03, 405MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  88% 8.70G/9.90G [00:21<00:03, 397MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  89% 8.77G/9.90G [00:22<00:02, 389MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  89% 8.84G/9.90G [00:22<00:02, 388MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  90% 8.90G/9.90G [00:22<00:02, 361MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  91% 8.97G/9.90G [00:22<00:02, 369MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  91% 9.04G/9.90G [00:22<00:02, 394MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  92% 9.10G/9.90G [00:22<00:02, 394MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  93% 9.17G/9.90G [00:23<00:01, 389MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  93% 9.23G/9.90G [00:23<00:01, 388MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  94% 9.30G/9.90G [00:23<00:01, 386MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  95% 9.37G/9.90G [00:23<00:01, 384MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  95% 9.43G/9.90G [00:23<00:01, 388MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  96% 9.50G/9.90G [00:23<00:01, 384MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  97% 9.57G/9.90G [00:24<00:00, 387MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  97% 9.64G/9.90G [00:24<00:00, 366MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  98% 9.70G/9.90G [00:24<00:00, 389MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  99% 9.77G/9.90G [00:24<00:00, 391MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin:  99% 9.84G/9.90G [00:24<00:00, 387MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00003.bin: 100% 9.90G/9.90G [00:25<00:00, 396MB/s]\n",
            "Downloading shards:  67% 2/3 [00:48<00:24, 24.36s/it]\n",
            "pytorch_model-00003-of-00003.bin:   0% 0.00/6.18G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:   0% 737k/6.18G [00:01<3:54:53, 438kB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:   1% 67.8M/6.18G [00:02<03:04, 33.1MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  14% 874M/6.18G [00:03<00:14, 375MB/s]  \u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  23% 1.41G/6.18G [00:04<00:11, 423MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  24% 1.48G/6.18G [00:05<00:14, 322MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  25% 1.54G/6.18G [00:05<00:15, 297MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  26% 1.61G/6.18G [00:06<00:18, 243MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  27% 1.68G/6.18G [00:07<00:23, 189MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  28% 1.75G/6.18G [00:07<00:22, 200MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  31% 1.95G/6.18G [00:07<00:12, 327MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  35% 2.15G/6.18G [00:07<00:08, 467MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  37% 2.28G/6.18G [00:08<00:13, 300MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  45% 2.75G/6.18G [00:09<00:07, 445MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  47% 2.89G/6.18G [00:09<00:07, 429MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  48% 2.96G/6.18G [00:09<00:07, 419MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  49% 3.02G/6.18G [00:09<00:07, 421MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  50% 3.09G/6.18G [00:10<00:07, 394MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  52% 3.22G/6.18G [00:10<00:06, 469MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  53% 3.29G/6.18G [00:10<00:05, 484MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  54% 3.36G/6.18G [00:10<00:05, 500MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  55% 3.42G/6.18G [00:10<00:05, 511MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  56% 3.49G/6.18G [00:10<00:05, 523MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  58% 3.56G/6.18G [00:10<00:04, 532MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  60% 3.69G/6.18G [00:11<00:04, 599MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  62% 3.83G/6.18G [00:11<00:03, 676MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  64% 3.97G/6.18G [00:11<00:02, 764MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  66% 4.10G/6.18G [00:11<00:02, 863MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  70% 4.30G/6.18G [00:11<00:01, 968MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  73% 4.51G/6.18G [00:11<00:01, 1.07GB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  75% 4.64G/6.18G [00:11<00:01, 1.05GB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  77% 4.77G/6.18G [00:12<00:02, 665MB/s] \u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  79% 4.91G/6.18G [00:12<00:02, 561MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  82% 5.04G/6.18G [00:12<00:02, 497MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  83% 5.11G/6.18G [00:13<00:02, 473MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  84% 5.18G/6.18G [00:13<00:02, 454MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  85% 5.24G/6.18G [00:13<00:02, 430MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  86% 5.31G/6.18G [00:13<00:02, 421MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  87% 5.38G/6.18G [00:13<00:01, 410MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  88% 5.44G/6.18G [00:14<00:01, 404MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  89% 5.51G/6.18G [00:14<00:01, 396MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  90% 5.58G/6.18G [00:14<00:01, 394MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  91% 5.65G/6.18G [00:14<00:01, 393MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  92% 5.71G/6.18G [00:14<00:01, 391MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  93% 5.78G/6.18G [00:14<00:01, 387MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  95% 5.84G/6.18G [00:15<00:00, 389MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  96% 5.91G/6.18G [00:15<00:00, 387MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  97% 5.98G/6.18G [00:15<00:00, 387MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  98% 6.04G/6.18G [00:15<00:00, 386MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin:  99% 6.11G/6.18G [00:15<00:00, 384MB/s]\u001b[A\n",
            "pytorch_model-00003-of-00003.bin: 100% 6.18G/6.18G [00:15<00:00, 388MB/s]\n",
            "Downloading shards: 100% 3/3 [01:04<00:00, 21.55s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.47s/it]\n",
            "generation_config.json: 100% 192/192 [00:00<00:00, 1.60MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Loading LLAMA Done\n",
            "  📥 Loading pretrained weights from: https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna13b_trimmed.pth\n",
            "2026-02-02 00:17:56,262 [INFO] Downloading: \"https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna13b_trimmed.pth\" to /root/.cache/torch/hub/checkpoints/instruct_blip_vicuna13b_trimmed.pth\n",
            "\n",
            "100% 2.12G/2.12G [01:37<00:00, 23.2MB/s]\n",
            "2026-02-02 00:19:36,772 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna13b_trimmed.pth\n",
            "module.query_tokens\n",
            "module.ln_vision.weight\n",
            "module.ln_vision.bias\n",
            "module.Qformer.bert.embeddings.word_embeddings.weight\n",
            "module.Qformer.bert.embeddings.position_embeddings.weight\n",
            "module.Qformer.bert.embeddings.LayerNorm.weight\n",
            "module.Qformer.bert.embeddings.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.0.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.0.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.0.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.0.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.0.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.0.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.0.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.0.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.0.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.0.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.0.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.0.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.0.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.0.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.0.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.0.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.1.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.1.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.1.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.1.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.1.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.1.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.1.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.1.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.1.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.1.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.1.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.1.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.1.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.1.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.1.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.1.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.2.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.2.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.2.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.2.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.2.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.2.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.2.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.2.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.2.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.2.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.2.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.2.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.2.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.2.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.2.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.2.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.2.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.2.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.3.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.3.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.3.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.3.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.3.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.3.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.3.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.3.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.3.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.3.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.3.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.3.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.3.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.3.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.3.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.3.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.3.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.3.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.4.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.4.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.4.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.4.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.4.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.4.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.4.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.4.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.4.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.4.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.4.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.4.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.4.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.4.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.4.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.4.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.4.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.4.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.5.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.5.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.5.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.5.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.5.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.5.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.5.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.5.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.5.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.5.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.5.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.5.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.5.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.5.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.5.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.5.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.5.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.5.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.6.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.6.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.6.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.6.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.6.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.6.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.6.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.6.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.6.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.6.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.6.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.6.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.6.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.6.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.6.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.6.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.6.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.6.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.7.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.7.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.7.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.7.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.7.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.7.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.7.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.7.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.7.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.7.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.7.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.7.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.7.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.7.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.7.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.7.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.7.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.7.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.8.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.8.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.8.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.8.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.8.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.8.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.8.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.8.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.8.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.8.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.8.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.8.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.8.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.8.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.8.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.8.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.8.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.8.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.9.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.9.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.9.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.9.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.9.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.9.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.9.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.9.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.9.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.9.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.9.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.9.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.9.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.9.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.9.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.9.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.9.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.9.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.10.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.10.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.10.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.10.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.10.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.10.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.10.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.10.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.10.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.10.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.10.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.10.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.10.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.10.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.10.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.10.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.10.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.10.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.11.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.11.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.11.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.11.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.11.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.11.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.11.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.11.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.11.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.11.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.11.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.11.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.11.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.11.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.11.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.11.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.11.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.11.output_query.LayerNorm.bias\n",
            "module.llm_proj.weight\n",
            "module.llm_proj.bias\n",
            "2026-02-02 00:20:19,593 [INFO] number of trainable parameters: 189624832\n",
            "2026-02-02 00:20:19,598 [INFO] Resume checkpoint from /content/drive/MyDrive/outputs/rsgpt/rsicap_split_checkpoint/20251229133/checkpoint_8.pth\n",
            "2026-02-02 00:20:19,599 [INFO] Start training\n",
            "2026-02-02 00:20:19,650 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).\n",
            "2026-02-02 00:20:19,650 [INFO] Loaded 3320 records for train split from the dataset.\n",
            "2026-02-02 00:20:19,651 [INFO] Start training epoch 9, 830 iters per inner epoch.\n",
            "Train: data epoch: [9]  [  0/830]  eta: 1:31:07  lr: 0.000004  loss: 5.7113  time: 6.5876  data: 0.0000  max mem: 32340\n",
            "Train: data epoch: [9]  [ 50/830]  eta: 0:11:49  lr: 0.000004  loss: 2.4792  time: 0.7818  data: 0.0000  max mem: 33722\n",
            "Train: data epoch: [9]  [100/830]  eta: 0:10:05  lr: 0.000004  loss: 2.5495  time: 0.7459  data: 0.0000  max mem: 33758\n",
            "Train: data epoch: [9]  [150/830]  eta: 0:09:13  lr: 0.000004  loss: 2.8720  time: 0.8513  data: 0.0000  max mem: 33758\n",
            "Train: data epoch: [9]  [200/830]  eta: 0:08:24  lr: 0.000004  loss: 3.3778  time: 0.8047  data: 0.0000  max mem: 33758\n",
            "Train: data epoch: [9]  [250/830]  eta: 0:07:42  lr: 0.000004  loss: 2.7684  time: 0.7264  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [9]  [300/830]  eta: 0:06:59  lr: 0.000004  loss: 2.2871  time: 0.7471  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [9]  [350/830]  eta: 0:06:20  lr: 0.000004  loss: 3.0397  time: 0.7476  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [9]  [400/830]  eta: 0:05:39  lr: 0.000004  loss: 2.8019  time: 0.7770  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [9]  [450/830]  eta: 0:04:59  lr: 0.000004  loss: 3.4693  time: 0.7738  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [9]  [500/830]  eta: 0:04:19  lr: 0.000004  loss: 2.8274  time: 0.8053  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [9]  [550/830]  eta: 0:03:42  lr: 0.000004  loss: 2.5093  time: 0.7280  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [9]  [600/830]  eta: 0:03:02  lr: 0.000004  loss: 3.0180  time: 0.7509  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [9]  [650/830]  eta: 0:02:23  lr: 0.000004  loss: 1.8563  time: 0.8473  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [9]  [700/830]  eta: 0:01:44  lr: 0.000004  loss: 2.7348  time: 0.9171  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [9]  [750/830]  eta: 0:01:04  lr: 0.000004  loss: 2.3524  time: 0.8720  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [9]  [800/830]  eta: 0:00:24  lr: 0.000004  loss: 2.6306  time: 0.8713  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [9]  [829/830]  eta: 0:00:00  lr: 0.000004  loss: 2.0331  time: 0.8414  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [9] Total time: 0:11:14 (0.8121 s / it)\n",
            "2026-02-02 00:31:33,686 [INFO] Averaged stats: lr: 0.0000  loss: 2.6596\n",
            "2026-02-02 00:31:33,694 [INFO] No validation splits found.\n",
            "2026-02-02 00:31:33,721 [INFO] Saving checkpoint at epoch 9 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_9.pth.\n",
            "2026-02-02 00:31:38,168 [INFO] Start training\n",
            "2026-02-02 00:31:38,203 [INFO] Start training epoch 10, 830 iters per inner epoch.\n",
            "Train: data epoch: [10]  [  0/830]  eta: 0:36:41  lr: 0.000004  loss: 2.5464  time: 2.6522  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [ 50/830]  eta: 0:04:35  lr: 0.000004  loss: 2.7388  time: 0.2837  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [100/830]  eta: 0:03:54  lr: 0.000004  loss: 2.7477  time: 0.2838  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [150/830]  eta: 0:03:31  lr: 0.000004  loss: 3.1461  time: 0.2889  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [200/830]  eta: 0:03:11  lr: 0.000004  loss: 2.3553  time: 0.2772  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [250/830]  eta: 0:02:53  lr: 0.000004  loss: 2.4462  time: 0.2843  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [300/830]  eta: 0:02:36  lr: 0.000004  loss: 2.5343  time: 0.2827  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [350/830]  eta: 0:02:20  lr: 0.000004  loss: 2.8381  time: 0.2893  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [400/830]  eta: 0:02:05  lr: 0.000004  loss: 2.3049  time: 0.2778  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [450/830]  eta: 0:01:50  lr: 0.000004  loss: 2.3937  time: 0.2738  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [500/830]  eta: 0:01:35  lr: 0.000004  loss: 2.6328  time: 0.2753  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [550/830]  eta: 0:01:20  lr: 0.000004  loss: 1.6881  time: 0.2771  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [600/830]  eta: 0:01:06  lr: 0.000004  loss: 2.0342  time: 0.2815  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [650/830]  eta: 0:00:51  lr: 0.000004  loss: 2.4226  time: 0.2804  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [700/830]  eta: 0:00:37  lr: 0.000004  loss: 1.5375  time: 0.2855  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [750/830]  eta: 0:00:22  lr: 0.000004  loss: 2.7792  time: 0.2793  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [800/830]  eta: 0:00:08  lr: 0.000004  loss: 2.0561  time: 0.2787  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10]  [829/830]  eta: 0:00:00  lr: 0.000004  loss: 2.4644  time: 0.2810  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [10] Total time: 0:03:56 (0.2854 s / it)\n",
            "2026-02-02 00:35:35,045 [INFO] Averaged stats: lr: 0.0000  loss: 2.4511\n",
            "2026-02-02 00:35:35,052 [INFO] No validation splits found.\n",
            "2026-02-02 00:35:35,079 [INFO] Saving checkpoint at epoch 10 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_10.pth.\n",
            "2026-02-02 00:35:39,303 [INFO] Start training\n",
            "2026-02-02 00:35:39,337 [INFO] Start training epoch 11, 830 iters per inner epoch.\n",
            "Train: data epoch: [11]  [  0/830]  eta: 0:36:24  lr: 0.000004  loss: 1.9985  time: 2.6321  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [11]  [ 50/830]  eta: 0:04:34  lr: 0.000004  loss: 2.1852  time: 0.2831  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [11]  [100/830]  eta: 0:03:51  lr: 0.000004  loss: 2.6457  time: 0.2793  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [11]  [150/830]  eta: 0:03:27  lr: 0.000004  loss: 2.1180  time: 0.2847  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [11]  [200/830]  eta: 0:03:08  lr: 0.000003  loss: 2.2512  time: 0.2835  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [11]  [250/830]  eta: 0:02:52  lr: 0.000003  loss: 2.6853  time: 0.2890  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [11]  [300/830]  eta: 0:02:36  lr: 0.000003  loss: 2.0369  time: 0.2790  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [11]  [350/830]  eta: 0:02:20  lr: 0.000003  loss: 2.6477  time: 0.2766  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [11]  [400/830]  eta: 0:02:04  lr: 0.000003  loss: 2.1351  time: 0.2796  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [11]  [450/830]  eta: 0:01:50  lr: 0.000003  loss: 2.8604  time: 0.2843  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [11]  [500/830]  eta: 0:01:35  lr: 0.000003  loss: 2.1096  time: 0.2780  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [11]  [550/830]  eta: 0:01:20  lr: 0.000003  loss: 2.6465  time: 0.2772  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [11]  [600/830]  eta: 0:01:05  lr: 0.000003  loss: 2.9474  time: 0.2781  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [11]  [650/830]  eta: 0:00:51  lr: 0.000003  loss: 2.2799  time: 0.2775  data: 0.0000  max mem: 34136\n",
            "Train: data epoch: [11]  [700/830]  eta: 0:00:37  lr: 0.000003  loss: 2.0507  time: 0.2776  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [11]  [750/830]  eta: 0:00:22  lr: 0.000003  loss: 2.3401  time: 0.2795  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [11]  [800/830]  eta: 0:00:08  lr: 0.000003  loss: 2.4973  time: 0.2799  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [11]  [829/830]  eta: 0:00:00  lr: 0.000003  loss: 3.0028  time: 0.2809  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [11] Total time: 0:03:56 (0.2850 s / it)\n",
            "2026-02-02 00:39:35,909 [INFO] Averaged stats: lr: 0.0000  loss: 2.4200\n",
            "2026-02-02 00:39:35,916 [INFO] No validation splits found.\n",
            "2026-02-02 00:39:35,943 [INFO] Saving checkpoint at epoch 11 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_11.pth.\n",
            "2026-02-02 00:39:40,224 [INFO] Start training\n",
            "2026-02-02 00:39:40,260 [INFO] Start training epoch 12, 830 iters per inner epoch.\n",
            "Train: data epoch: [12]  [  0/830]  eta: 1:34:55  lr: 0.000003  loss: 3.3921  time: 6.8620  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [ 50/830]  eta: 0:05:26  lr: 0.000003  loss: 2.5661  time: 0.2899  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [100/830]  eta: 0:04:19  lr: 0.000003  loss: 2.6648  time: 0.2847  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [150/830]  eta: 0:03:46  lr: 0.000003  loss: 2.6308  time: 0.2877  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [200/830]  eta: 0:03:21  lr: 0.000003  loss: 2.5023  time: 0.2818  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [250/830]  eta: 0:03:00  lr: 0.000003  loss: 2.4055  time: 0.2803  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [300/830]  eta: 0:02:41  lr: 0.000003  loss: 2.3136  time: 0.2802  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [350/830]  eta: 0:02:24  lr: 0.000003  loss: 2.7299  time: 0.2756  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [400/830]  eta: 0:02:08  lr: 0.000003  loss: 1.9894  time: 0.2768  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [450/830]  eta: 0:01:52  lr: 0.000003  loss: 1.9130  time: 0.2768  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [500/830]  eta: 0:01:37  lr: 0.000003  loss: 2.4637  time: 0.2763  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [550/830]  eta: 0:01:22  lr: 0.000003  loss: 2.0218  time: 0.2803  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [600/830]  eta: 0:01:07  lr: 0.000003  loss: 2.6806  time: 0.2799  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [650/830]  eta: 0:00:52  lr: 0.000003  loss: 2.6957  time: 0.2807  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [700/830]  eta: 0:00:37  lr: 0.000003  loss: 2.9884  time: 0.2848  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [750/830]  eta: 0:00:23  lr: 0.000003  loss: 1.9565  time: 0.2790  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [800/830]  eta: 0:00:08  lr: 0.000003  loss: 2.5762  time: 0.2809  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12]  [829/830]  eta: 0:00:00  lr: 0.000003  loss: 2.4904  time: 0.2834  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [12] Total time: 0:03:59 (0.2889 s / it)\n",
            "2026-02-02 00:43:40,063 [INFO] Averaged stats: lr: 0.0000  loss: 2.4061\n",
            "2026-02-02 00:43:40,071 [INFO] No validation splits found.\n",
            "2026-02-02 00:43:40,098 [INFO] Saving checkpoint at epoch 12 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_12.pth.\n",
            "2026-02-02 00:43:44,320 [INFO] Start training\n",
            "2026-02-02 00:43:44,353 [INFO] Start training epoch 13, 830 iters per inner epoch.\n",
            "Train: data epoch: [13]  [  0/830]  eta: 1:39:00  lr: 0.000003  loss: 2.2636  time: 7.1567  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [ 50/830]  eta: 0:05:32  lr: 0.000003  loss: 2.2418  time: 0.2877  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [100/830]  eta: 0:04:20  lr: 0.000003  loss: 2.5535  time: 0.2792  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [150/830]  eta: 0:03:46  lr: 0.000003  loss: 2.4390  time: 0.2855  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [200/830]  eta: 0:03:22  lr: 0.000003  loss: 2.7075  time: 0.2809  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [250/830]  eta: 0:03:01  lr: 0.000003  loss: 2.5070  time: 0.2777  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [300/830]  eta: 0:02:42  lr: 0.000003  loss: 2.7165  time: 0.2767  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [350/830]  eta: 0:02:25  lr: 0.000003  loss: 2.3710  time: 0.2801  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [400/830]  eta: 0:02:09  lr: 0.000003  loss: 2.1732  time: 0.2813  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [450/830]  eta: 0:01:53  lr: 0.000003  loss: 1.8943  time: 0.2805  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [500/830]  eta: 0:01:37  lr: 0.000003  loss: 2.2656  time: 0.2807  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [550/830]  eta: 0:01:22  lr: 0.000003  loss: 2.5197  time: 0.2767  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [600/830]  eta: 0:01:07  lr: 0.000003  loss: 3.1852  time: 0.2889  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [650/830]  eta: 0:00:52  lr: 0.000003  loss: 3.0022  time: 0.2786  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [700/830]  eta: 0:00:37  lr: 0.000003  loss: 2.5989  time: 0.2781  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [750/830]  eta: 0:00:23  lr: 0.000003  loss: 3.1597  time: 0.2813  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [800/830]  eta: 0:00:08  lr: 0.000003  loss: 2.4684  time: 0.2834  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13]  [829/830]  eta: 0:00:00  lr: 0.000003  loss: 1.7871  time: 0.2808  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [13] Total time: 0:04:00 (0.2903 s / it)\n",
            "2026-02-02 00:47:45,283 [INFO] Averaged stats: lr: 0.0000  loss: 2.3617\n",
            "2026-02-02 00:47:45,290 [INFO] No validation splits found.\n",
            "2026-02-02 00:47:45,317 [INFO] Saving checkpoint at epoch 13 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_13.pth.\n",
            "2026-02-02 00:47:49,598 [INFO] Start training\n",
            "2026-02-02 00:47:49,632 [INFO] Start training epoch 14, 830 iters per inner epoch.\n",
            "Train: data epoch: [14]  [  0/830]  eta: 0:41:28  lr: 0.000003  loss: 2.4256  time: 2.9983  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [ 50/830]  eta: 0:04:27  lr: 0.000003  loss: 2.5001  time: 0.2880  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [100/830]  eta: 0:03:50  lr: 0.000003  loss: 2.3585  time: 0.2839  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [150/830]  eta: 0:03:27  lr: 0.000003  loss: 2.3876  time: 0.2883  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [200/830]  eta: 0:03:09  lr: 0.000003  loss: 2.3153  time: 0.2793  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [250/830]  eta: 0:02:52  lr: 0.000003  loss: 2.8585  time: 0.2820  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [300/830]  eta: 0:02:35  lr: 0.000003  loss: 2.0531  time: 0.2812  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [350/830]  eta: 0:02:20  lr: 0.000003  loss: 1.9967  time: 0.2777  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [400/830]  eta: 0:02:05  lr: 0.000003  loss: 2.3762  time: 0.2838  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [450/830]  eta: 0:01:50  lr: 0.000003  loss: 2.3037  time: 0.2809  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [500/830]  eta: 0:01:35  lr: 0.000003  loss: 1.3439  time: 0.2803  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [550/830]  eta: 0:01:21  lr: 0.000003  loss: 2.7882  time: 0.2856  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [600/830]  eta: 0:01:06  lr: 0.000003  loss: 3.0730  time: 0.2829  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [650/830]  eta: 0:00:51  lr: 0.000003  loss: 2.8252  time: 0.2837  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [700/830]  eta: 0:00:37  lr: 0.000003  loss: 2.3594  time: 0.2914  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [750/830]  eta: 0:00:23  lr: 0.000003  loss: 2.1993  time: 0.2798  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [800/830]  eta: 0:00:08  lr: 0.000003  loss: 2.5945  time: 0.2770  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14]  [829/830]  eta: 0:00:00  lr: 0.000003  loss: 1.7385  time: 0.2826  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [14] Total time: 0:03:58 (0.2872 s / it)\n",
            "2026-02-02 00:51:48,009 [INFO] Averaged stats: lr: 0.0000  loss: 2.3599\n",
            "2026-02-02 00:51:48,016 [INFO] No validation splits found.\n",
            "2026-02-02 00:51:48,043 [INFO] Saving checkpoint at epoch 14 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_14.pth.\n",
            "2026-02-02 00:51:52,302 [INFO] Start training\n",
            "2026-02-02 00:51:52,335 [INFO] Start training epoch 15, 830 iters per inner epoch.\n",
            "Train: data epoch: [15]  [  0/830]  eta: 0:36:37  lr: 0.000003  loss: 1.9771  time: 2.6473  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [ 50/830]  eta: 0:05:12  lr: 0.000003  loss: 2.3060  time: 0.2922  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [100/830]  eta: 0:04:10  lr: 0.000003  loss: 1.9360  time: 0.2787  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [150/830]  eta: 0:03:40  lr: 0.000003  loss: 1.8507  time: 0.2858  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [200/830]  eta: 0:03:18  lr: 0.000002  loss: 1.6185  time: 0.2800  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [250/830]  eta: 0:02:59  lr: 0.000002  loss: 2.1718  time: 0.2917  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [300/830]  eta: 0:02:41  lr: 0.000002  loss: 1.8619  time: 0.2817  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [350/830]  eta: 0:02:24  lr: 0.000002  loss: 1.8535  time: 0.2863  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [400/830]  eta: 0:02:08  lr: 0.000002  loss: 2.4312  time: 0.2834  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [450/830]  eta: 0:01:52  lr: 0.000002  loss: 2.1291  time: 0.2794  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [500/830]  eta: 0:01:37  lr: 0.000002  loss: 2.2067  time: 0.2764  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [550/830]  eta: 0:01:22  lr: 0.000002  loss: 2.1433  time: 0.2807  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [600/830]  eta: 0:01:07  lr: 0.000002  loss: 2.6728  time: 0.2765  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [650/830]  eta: 0:00:52  lr: 0.000002  loss: 2.9285  time: 0.2808  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [700/830]  eta: 0:00:37  lr: 0.000002  loss: 2.3602  time: 0.2801  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [750/830]  eta: 0:00:23  lr: 0.000002  loss: 2.2520  time: 0.2831  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [800/830]  eta: 0:00:08  lr: 0.000002  loss: 2.0325  time: 0.2856  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15]  [829/830]  eta: 0:00:00  lr: 0.000002  loss: 2.0829  time: 0.2828  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [15] Total time: 0:03:59 (0.2887 s / it)\n",
            "2026-02-02 00:55:51,958 [INFO] Averaged stats: lr: 0.0000  loss: 2.3358\n",
            "2026-02-02 00:55:51,967 [INFO] No validation splits found.\n",
            "2026-02-02 00:55:51,994 [INFO] Saving checkpoint at epoch 15 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_15.pth.\n",
            "2026-02-02 00:55:56,350 [INFO] Start training\n",
            "2026-02-02 00:55:56,383 [INFO] Start training epoch 16, 830 iters per inner epoch.\n",
            "Train: data epoch: [16]  [  0/830]  eta: 0:36:11  lr: 0.000002  loss: 2.4127  time: 2.6157  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [ 50/830]  eta: 0:05:38  lr: 0.000002  loss: 2.2617  time: 0.2937  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [100/830]  eta: 0:04:24  lr: 0.000002  loss: 2.0506  time: 0.2922  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [150/830]  eta: 0:03:53  lr: 0.000002  loss: 2.0145  time: 0.3236  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [200/830]  eta: 0:03:27  lr: 0.000002  loss: 2.3522  time: 0.2788  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [250/830]  eta: 0:03:05  lr: 0.000002  loss: 2.2611  time: 0.2788  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [300/830]  eta: 0:02:46  lr: 0.000002  loss: 1.4824  time: 0.2839  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [350/830]  eta: 0:02:28  lr: 0.000002  loss: 1.9632  time: 0.2848  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [400/830]  eta: 0:02:11  lr: 0.000002  loss: 3.5695  time: 0.2818  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [450/830]  eta: 0:01:55  lr: 0.000002  loss: 2.5294  time: 0.2883  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [500/830]  eta: 0:01:39  lr: 0.000002  loss: 2.9193  time: 0.2791  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [550/830]  eta: 0:01:23  lr: 0.000002  loss: 2.0328  time: 0.2769  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [600/830]  eta: 0:01:08  lr: 0.000002  loss: 1.6790  time: 0.2806  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [650/830]  eta: 0:00:53  lr: 0.000002  loss: 2.5191  time: 0.2789  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [700/830]  eta: 0:00:38  lr: 0.000002  loss: 2.6704  time: 0.2809  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [750/830]  eta: 0:00:23  lr: 0.000002  loss: 1.9677  time: 0.2963  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [800/830]  eta: 0:00:08  lr: 0.000002  loss: 2.6118  time: 0.2808  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16]  [829/830]  eta: 0:00:00  lr: 0.000002  loss: 2.2256  time: 0.2852  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [16] Total time: 0:04:03 (0.2934 s / it)\n",
            "2026-02-02 00:59:59,932 [INFO] Averaged stats: lr: 0.0000  loss: 2.3237\n",
            "2026-02-02 00:59:59,940 [INFO] No validation splits found.\n",
            "2026-02-02 00:59:59,967 [INFO] Saving checkpoint at epoch 16 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_16.pth.\n",
            "2026-02-02 01:00:04,276 [INFO] Start training\n",
            "2026-02-02 01:00:04,321 [INFO] Start training epoch 17, 830 iters per inner epoch.\n",
            "Train: data epoch: [17]  [  0/830]  eta: 0:36:12  lr: 0.000002  loss: 2.0568  time: 2.6176  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [ 50/830]  eta: 0:05:36  lr: 0.000002  loss: 2.2561  time: 0.2871  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [100/830]  eta: 0:04:23  lr: 0.000002  loss: 2.3371  time: 0.2884  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [150/830]  eta: 0:03:48  lr: 0.000002  loss: 2.7245  time: 0.2894  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [200/830]  eta: 0:03:23  lr: 0.000002  loss: 2.6104  time: 0.2850  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [250/830]  eta: 0:03:03  lr: 0.000002  loss: 1.7308  time: 0.2793  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [300/830]  eta: 0:02:44  lr: 0.000002  loss: 2.2349  time: 0.2787  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [350/830]  eta: 0:02:26  lr: 0.000002  loss: 1.5577  time: 0.2775  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [400/830]  eta: 0:02:09  lr: 0.000002  loss: 3.9608  time: 0.2785  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [450/830]  eta: 0:01:53  lr: 0.000002  loss: 2.5923  time: 0.2820  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [500/830]  eta: 0:01:38  lr: 0.000002  loss: 2.6808  time: 0.2809  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [550/830]  eta: 0:01:22  lr: 0.000002  loss: 3.1653  time: 0.2838  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [600/830]  eta: 0:01:07  lr: 0.000002  loss: 1.9303  time: 0.2860  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [650/830]  eta: 0:00:52  lr: 0.000002  loss: 2.4192  time: 0.2771  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [700/830]  eta: 0:00:38  lr: 0.000002  loss: 3.4392  time: 0.2790  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [750/830]  eta: 0:00:23  lr: 0.000002  loss: 2.3543  time: 0.2747  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [800/830]  eta: 0:00:08  lr: 0.000002  loss: 2.0499  time: 0.2775  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17]  [829/830]  eta: 0:00:00  lr: 0.000002  loss: 2.3700  time: 0.2799  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [17] Total time: 0:04:01 (0.2904 s / it)\n",
            "2026-02-02 01:04:05,365 [INFO] Averaged stats: lr: 0.0000  loss: 2.3094\n",
            "2026-02-02 01:04:05,373 [INFO] No validation splits found.\n",
            "2026-02-02 01:04:05,400 [INFO] Saving checkpoint at epoch 17 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_17.pth.\n",
            "2026-02-02 01:04:09,656 [INFO] Start training\n",
            "2026-02-02 01:04:09,688 [INFO] Start training epoch 18, 830 iters per inner epoch.\n",
            "Train: data epoch: [18]  [  0/830]  eta: 1:40:36  lr: 0.000002  loss: 3.1013  time: 7.2726  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [ 50/830]  eta: 0:05:33  lr: 0.000002  loss: 1.8254  time: 0.2928  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [100/830]  eta: 0:04:24  lr: 0.000002  loss: 2.3575  time: 0.2957  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [150/830]  eta: 0:03:50  lr: 0.000002  loss: 1.6903  time: 0.2908  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [200/830]  eta: 0:03:24  lr: 0.000002  loss: 2.0552  time: 0.2822  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [250/830]  eta: 0:03:02  lr: 0.000002  loss: 2.2308  time: 0.2856  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [300/830]  eta: 0:02:44  lr: 0.000002  loss: 2.3854  time: 0.2839  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [350/830]  eta: 0:02:26  lr: 0.000002  loss: 2.6981  time: 0.2806  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [400/830]  eta: 0:02:10  lr: 0.000002  loss: 1.8054  time: 0.2917  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [450/830]  eta: 0:01:54  lr: 0.000002  loss: 2.4341  time: 0.2818  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [500/830]  eta: 0:01:38  lr: 0.000002  loss: 2.3677  time: 0.2796  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [550/830]  eta: 0:01:23  lr: 0.000002  loss: 2.4125  time: 0.2806  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [600/830]  eta: 0:01:07  lr: 0.000002  loss: 2.1894  time: 0.2848  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [650/830]  eta: 0:00:52  lr: 0.000002  loss: 3.1645  time: 0.2819  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [700/830]  eta: 0:00:38  lr: 0.000002  loss: 1.9689  time: 0.2790  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [750/830]  eta: 0:00:23  lr: 0.000002  loss: 3.0518  time: 0.2783  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [800/830]  eta: 0:00:08  lr: 0.000002  loss: 2.2907  time: 0.2802  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18]  [829/830]  eta: 0:00:00  lr: 0.000002  loss: 1.8001  time: 0.2882  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [18] Total time: 0:04:01 (0.2915 s / it)\n",
            "2026-02-02 01:08:11,666 [INFO] Averaged stats: lr: 0.0000  loss: 2.2946\n",
            "2026-02-02 01:08:11,674 [INFO] No validation splits found.\n",
            "2026-02-02 01:08:11,701 [INFO] Saving checkpoint at epoch 18 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_18.pth.\n",
            "2026-02-02 01:08:15,934 [INFO] Start training\n",
            "2026-02-02 01:08:15,967 [INFO] Start training epoch 19, 830 iters per inner epoch.\n",
            "Train: data epoch: [19]  [  0/830]  eta: 0:36:23  lr: 0.000002  loss: 2.1160  time: 2.6312  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [ 50/830]  eta: 0:05:08  lr: 0.000002  loss: 2.8308  time: 0.2953  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [100/830]  eta: 0:04:12  lr: 0.000002  loss: 1.7337  time: 0.2969  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [150/830]  eta: 0:03:43  lr: 0.000002  loss: 2.0513  time: 0.2945  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [200/830]  eta: 0:03:21  lr: 0.000001  loss: 2.4207  time: 0.2982  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [250/830]  eta: 0:03:01  lr: 0.000001  loss: 1.8776  time: 0.2825  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [300/830]  eta: 0:02:43  lr: 0.000001  loss: 2.7398  time: 0.2813  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [350/830]  eta: 0:02:25  lr: 0.000001  loss: 3.0088  time: 0.2815  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [400/830]  eta: 0:02:09  lr: 0.000001  loss: 2.5009  time: 0.2826  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [450/830]  eta: 0:01:53  lr: 0.000001  loss: 1.6276  time: 0.2767  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [500/830]  eta: 0:01:37  lr: 0.000001  loss: 2.5129  time: 0.2778  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [550/830]  eta: 0:01:22  lr: 0.000001  loss: 2.1804  time: 0.2758  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [600/830]  eta: 0:01:07  lr: 0.000001  loss: 2.6433  time: 0.2771  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [650/830]  eta: 0:00:52  lr: 0.000001  loss: 1.9931  time: 0.2803  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [700/830]  eta: 0:00:37  lr: 0.000001  loss: 1.9773  time: 0.2763  data: 0.0000  max mem: 34255\n",
            "Train: data epoch: [19]  [750/830]  eta: 0:00:23  lr: 0.000001  loss: 2.1349  time: 0.2799  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [19]  [800/830]  eta: 0:00:08  lr: 0.000001  loss: 2.6671  time: 0.2824  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [19]  [829/830]  eta: 0:00:00  lr: 0.000001  loss: 1.6677  time: 0.2814  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [19] Total time: 0:04:00 (0.2894 s / it)\n",
            "2026-02-02 01:12:16,189 [INFO] Averaged stats: lr: 0.0000  loss: 2.2917\n",
            "2026-02-02 01:12:16,197 [INFO] No validation splits found.\n",
            "2026-02-02 01:12:16,223 [INFO] Saving checkpoint at epoch 19 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_19.pth.\n",
            "2026-02-02 01:12:20,555 [INFO] Start training\n",
            "2026-02-02 01:12:20,587 [INFO] Start training epoch 20, 830 iters per inner epoch.\n",
            "Train: data epoch: [20]  [  0/830]  eta: 0:36:23  lr: 0.000001  loss: 2.6097  time: 2.6303  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [ 50/830]  eta: 0:05:05  lr: 0.000001  loss: 2.0894  time: 0.2955  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [100/830]  eta: 0:04:12  lr: 0.000001  loss: 1.8939  time: 0.3080  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [150/830]  eta: 0:03:42  lr: 0.000001  loss: 2.9980  time: 0.2873  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [200/830]  eta: 0:03:18  lr: 0.000001  loss: 2.6753  time: 0.2828  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [250/830]  eta: 0:02:59  lr: 0.000001  loss: 2.9542  time: 0.2819  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [300/830]  eta: 0:02:41  lr: 0.000001  loss: 2.2309  time: 0.2775  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [350/830]  eta: 0:02:24  lr: 0.000001  loss: 2.1022  time: 0.2765  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [400/830]  eta: 0:02:08  lr: 0.000001  loss: 2.4463  time: 0.2792  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [450/830]  eta: 0:01:52  lr: 0.000001  loss: 2.0581  time: 0.2824  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [500/830]  eta: 0:01:37  lr: 0.000001  loss: 2.0208  time: 0.2811  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [550/830]  eta: 0:01:21  lr: 0.000001  loss: 2.5554  time: 0.2815  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [600/830]  eta: 0:01:07  lr: 0.000001  loss: 1.7163  time: 0.2777  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [650/830]  eta: 0:00:52  lr: 0.000001  loss: 2.2609  time: 0.2806  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [700/830]  eta: 0:00:37  lr: 0.000001  loss: 2.5265  time: 0.2805  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [750/830]  eta: 0:00:23  lr: 0.000001  loss: 2.0797  time: 0.2804  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [800/830]  eta: 0:00:08  lr: 0.000001  loss: 2.0532  time: 0.2807  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20]  [829/830]  eta: 0:00:00  lr: 0.000001  loss: 2.4443  time: 0.2824  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [20] Total time: 0:03:59 (0.2888 s / it)\n",
            "2026-02-02 01:16:20,268 [INFO] Averaged stats: lr: 0.0000  loss: 2.2765\n",
            "2026-02-02 01:16:20,276 [INFO] No validation splits found.\n",
            "2026-02-02 01:16:20,302 [INFO] Saving checkpoint at epoch 20 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_20.pth.\n",
            "2026-02-02 01:16:24,716 [INFO] Start training\n",
            "2026-02-02 01:16:24,748 [INFO] Start training epoch 21, 830 iters per inner epoch.\n",
            "Train: data epoch: [21]  [  0/830]  eta: 0:36:17  lr: 0.000001  loss: 2.5464  time: 2.6235  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [ 50/830]  eta: 0:05:16  lr: 0.000001  loss: 2.5983  time: 0.2852  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [100/830]  eta: 0:04:14  lr: 0.000001  loss: 2.6206  time: 0.2953  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [150/830]  eta: 0:03:45  lr: 0.000001  loss: 2.1862  time: 0.2902  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [200/830]  eta: 0:03:20  lr: 0.000001  loss: 2.5683  time: 0.2823  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [250/830]  eta: 0:03:00  lr: 0.000001  loss: 2.1030  time: 0.2808  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [300/830]  eta: 0:02:42  lr: 0.000001  loss: 1.7975  time: 0.2805  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [350/830]  eta: 0:02:25  lr: 0.000001  loss: 2.1433  time: 0.2820  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [400/830]  eta: 0:02:09  lr: 0.000001  loss: 2.0142  time: 0.2863  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [450/830]  eta: 0:01:53  lr: 0.000001  loss: 1.9275  time: 0.2790  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [500/830]  eta: 0:01:38  lr: 0.000001  loss: 3.3901  time: 0.2838  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [550/830]  eta: 0:01:22  lr: 0.000001  loss: 3.3614  time: 0.2805  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [600/830]  eta: 0:01:07  lr: 0.000001  loss: 2.3680  time: 0.2816  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [650/830]  eta: 0:00:52  lr: 0.000001  loss: 2.3944  time: 0.2957  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [700/830]  eta: 0:00:38  lr: 0.000001  loss: 2.4609  time: 0.2780  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [750/830]  eta: 0:00:23  lr: 0.000001  loss: 2.2983  time: 0.2788  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [800/830]  eta: 0:00:08  lr: 0.000001  loss: 2.3441  time: 0.2763  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21]  [829/830]  eta: 0:00:00  lr: 0.000001  loss: 3.0425  time: 0.2827  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [21] Total time: 0:04:01 (0.2905 s / it)\n",
            "2026-02-02 01:20:25,851 [INFO] Averaged stats: lr: 0.0000  loss: 2.2685\n",
            "2026-02-02 01:20:25,859 [INFO] No validation splits found.\n",
            "2026-02-02 01:20:25,886 [INFO] Saving checkpoint at epoch 21 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_21.pth.\n",
            "2026-02-02 01:20:30,252 [INFO] Start training\n",
            "2026-02-02 01:20:30,300 [INFO] Start training epoch 22, 830 iters per inner epoch.\n",
            "Train: data epoch: [22]  [  0/830]  eta: 0:36:32  lr: 0.000001  loss: 1.5950  time: 2.6416  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [ 50/830]  eta: 0:05:00  lr: 0.000001  loss: 2.2459  time: 0.3038  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [100/830]  eta: 0:04:05  lr: 0.000001  loss: 2.1851  time: 0.2932  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [150/830]  eta: 0:03:37  lr: 0.000001  loss: 2.1241  time: 0.2800  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [200/830]  eta: 0:03:16  lr: 0.000001  loss: 2.1428  time: 0.2790  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [250/830]  eta: 0:02:56  lr: 0.000001  loss: 2.0484  time: 0.2801  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [300/830]  eta: 0:02:39  lr: 0.000001  loss: 2.3619  time: 0.2801  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [350/830]  eta: 0:02:23  lr: 0.000001  loss: 2.7968  time: 0.2839  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [400/830]  eta: 0:02:07  lr: 0.000001  loss: 2.0326  time: 0.2816  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [450/830]  eta: 0:01:51  lr: 0.000001  loss: 1.7504  time: 0.2835  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [500/830]  eta: 0:01:36  lr: 0.000001  loss: 2.3273  time: 0.2789  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [550/830]  eta: 0:01:21  lr: 0.000001  loss: 2.2880  time: 0.2778  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [600/830]  eta: 0:01:06  lr: 0.000001  loss: 2.4004  time: 0.2812  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [650/830]  eta: 0:00:52  lr: 0.000001  loss: 1.8339  time: 0.2802  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [700/830]  eta: 0:00:37  lr: 0.000001  loss: 2.8386  time: 0.2863  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [750/830]  eta: 0:00:23  lr: 0.000001  loss: 2.4634  time: 0.2840  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [800/830]  eta: 0:00:08  lr: 0.000001  loss: 2.5455  time: 0.2822  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22]  [829/830]  eta: 0:00:00  lr: 0.000001  loss: 2.0772  time: 0.2868  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [22] Total time: 0:03:59 (0.2891 s / it)\n",
            "2026-02-02 01:24:30,223 [INFO] Averaged stats: lr: 0.0000  loss: 2.2500\n",
            "2026-02-02 01:24:30,231 [INFO] No validation splits found.\n",
            "2026-02-02 01:24:30,258 [INFO] Saving checkpoint at epoch 22 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_22.pth.\n",
            "2026-02-02 01:24:34,852 [INFO] Start training\n",
            "2026-02-02 01:24:34,884 [INFO] Start training epoch 23, 830 iters per inner epoch.\n",
            "Train: data epoch: [23]  [  0/830]  eta: 0:36:28  lr: 0.000001  loss: 2.7896  time: 2.6370  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [ 50/830]  eta: 0:04:52  lr: 0.000001  loss: 3.4129  time: 0.2908  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [100/830]  eta: 0:04:02  lr: 0.000001  loss: 2.1038  time: 0.2916  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [150/830]  eta: 0:03:34  lr: 0.000001  loss: 2.0997  time: 0.2841  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [200/830]  eta: 0:03:13  lr: 0.000001  loss: 1.9393  time: 0.2793  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [250/830]  eta: 0:02:55  lr: 0.000001  loss: 1.6459  time: 0.2820  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [300/830]  eta: 0:02:38  lr: 0.000001  loss: 2.5687  time: 0.2918  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [350/830]  eta: 0:02:22  lr: 0.000001  loss: 2.0172  time: 0.2816  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [400/830]  eta: 0:02:06  lr: 0.000001  loss: 2.0419  time: 0.2774  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [450/830]  eta: 0:01:51  lr: 0.000001  loss: 3.3412  time: 0.2809  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [500/830]  eta: 0:01:36  lr: 0.000001  loss: 2.1720  time: 0.2812  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [550/830]  eta: 0:01:21  lr: 0.000001  loss: 2.2593  time: 0.2827  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [600/830]  eta: 0:01:06  lr: 0.000001  loss: 2.1540  time: 0.2823  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [650/830]  eta: 0:00:52  lr: 0.000001  loss: 3.0887  time: 0.2826  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [700/830]  eta: 0:00:37  lr: 0.000001  loss: 2.2557  time: 0.2794  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [750/830]  eta: 0:00:23  lr: 0.000001  loss: 2.6896  time: 0.2808  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [800/830]  eta: 0:00:08  lr: 0.000001  loss: 2.1889  time: 0.2805  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23]  [829/830]  eta: 0:00:00  lr: 0.000001  loss: 1.5964  time: 0.2828  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [23] Total time: 0:03:58 (0.2876 s / it)\n",
            "2026-02-02 01:28:33,631 [INFO] Averaged stats: lr: 0.0000  loss: 2.2573\n",
            "2026-02-02 01:28:33,638 [INFO] No validation splits found.\n",
            "2026-02-02 01:28:33,665 [INFO] Saving checkpoint at epoch 23 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_23.pth.\n",
            "2026-02-02 01:28:38,246 [INFO] Start training\n",
            "2026-02-02 01:28:38,279 [INFO] Start training epoch 24, 830 iters per inner epoch.\n",
            "Train: data epoch: [24]  [  0/830]  eta: 0:36:36  lr: 0.000001  loss: 1.8204  time: 2.6469  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [ 50/830]  eta: 0:05:02  lr: 0.000001  loss: 1.8995  time: 0.3100  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [100/830]  eta: 0:04:11  lr: 0.000001  loss: 1.9993  time: 0.2876  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [150/830]  eta: 0:03:44  lr: 0.000001  loss: 2.3213  time: 0.3010  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [200/830]  eta: 0:03:21  lr: 0.000001  loss: 2.7283  time: 0.2835  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [250/830]  eta: 0:03:01  lr: 0.000001  loss: 1.7954  time: 0.2861  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [300/830]  eta: 0:02:42  lr: 0.000001  loss: 2.3203  time: 0.2818  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [350/830]  eta: 0:02:25  lr: 0.000001  loss: 1.9536  time: 0.2802  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [400/830]  eta: 0:02:09  lr: 0.000000  loss: 2.2994  time: 0.2805  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [450/830]  eta: 0:01:53  lr: 0.000000  loss: 2.4460  time: 0.2788  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [500/830]  eta: 0:01:38  lr: 0.000000  loss: 2.2054  time: 0.2811  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [550/830]  eta: 0:01:22  lr: 0.000000  loss: 1.9901  time: 0.2830  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [600/830]  eta: 0:01:07  lr: 0.000000  loss: 1.6256  time: 0.2840  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [650/830]  eta: 0:00:52  lr: 0.000000  loss: 3.7055  time: 0.2818  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [700/830]  eta: 0:00:38  lr: 0.000000  loss: 2.0729  time: 0.2812  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [750/830]  eta: 0:00:23  lr: 0.000000  loss: 2.4865  time: 0.2832  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [800/830]  eta: 0:00:08  lr: 0.000000  loss: 1.9603  time: 0.2854  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24]  [829/830]  eta: 0:00:00  lr: 0.000000  loss: 2.8903  time: 0.2878  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [24] Total time: 0:04:02 (0.2921 s / it)\n",
            "2026-02-02 01:32:40,692 [INFO] Averaged stats: lr: 0.0000  loss: 2.2606\n",
            "2026-02-02 01:32:40,700 [INFO] No validation splits found.\n",
            "2026-02-02 01:32:40,726 [INFO] Saving checkpoint at epoch 24 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_24.pth.\n",
            "2026-02-02 01:32:45,309 [INFO] Start training\n",
            "2026-02-02 01:32:45,340 [INFO] Start training epoch 25, 830 iters per inner epoch.\n",
            "Train: data epoch: [25]  [  0/830]  eta: 0:58:16  lr: 0.000000  loss: 2.5320  time: 4.2131  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [ 50/830]  eta: 0:04:59  lr: 0.000000  loss: 2.4901  time: 0.3104  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [100/830]  eta: 0:04:09  lr: 0.000000  loss: 2.2599  time: 0.2833  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [150/830]  eta: 0:03:40  lr: 0.000000  loss: 2.1640  time: 0.2871  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [200/830]  eta: 0:03:17  lr: 0.000000  loss: 1.5353  time: 0.2793  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [250/830]  eta: 0:02:58  lr: 0.000000  loss: 2.3627  time: 0.2818  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [300/830]  eta: 0:02:40  lr: 0.000000  loss: 2.3406  time: 0.2826  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [350/830]  eta: 0:02:24  lr: 0.000000  loss: 2.0933  time: 0.2850  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [400/830]  eta: 0:02:07  lr: 0.000000  loss: 1.6551  time: 0.2817  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [450/830]  eta: 0:01:52  lr: 0.000000  loss: 2.4824  time: 0.2831  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [500/830]  eta: 0:01:37  lr: 0.000000  loss: 2.6339  time: 0.2788  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [550/830]  eta: 0:01:22  lr: 0.000000  loss: 2.3588  time: 0.2794  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [600/830]  eta: 0:01:07  lr: 0.000000  loss: 2.6274  time: 0.2824  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [650/830]  eta: 0:00:52  lr: 0.000000  loss: 2.1788  time: 0.2845  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [700/830]  eta: 0:00:37  lr: 0.000000  loss: 1.6933  time: 0.2816  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [750/830]  eta: 0:00:23  lr: 0.000000  loss: 1.8374  time: 0.2811  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [800/830]  eta: 0:00:08  lr: 0.000000  loss: 1.7251  time: 0.2825  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25]  [829/830]  eta: 0:00:00  lr: 0.000000  loss: 2.7510  time: 0.2877  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [25] Total time: 0:04:00 (0.2895 s / it)\n",
            "2026-02-02 01:36:45,663 [INFO] Averaged stats: lr: 0.0000  loss: 2.2604\n",
            "2026-02-02 01:36:45,671 [INFO] No validation splits found.\n",
            "2026-02-02 01:36:45,698 [INFO] Saving checkpoint at epoch 25 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_25.pth.\n",
            "2026-02-02 01:36:50,274 [INFO] Start training\n",
            "2026-02-02 01:36:50,311 [INFO] Start training epoch 26, 830 iters per inner epoch.\n",
            "Train: data epoch: [26]  [  0/830]  eta: 1:25:39  lr: 0.000000  loss: 2.5090  time: 6.1917  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [ 50/830]  eta: 0:05:15  lr: 0.000000  loss: 2.5845  time: 0.2968  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [100/830]  eta: 0:04:13  lr: 0.000000  loss: 1.7103  time: 0.2902  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [150/830]  eta: 0:03:43  lr: 0.000000  loss: 1.8020  time: 0.2862  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [200/830]  eta: 0:03:19  lr: 0.000000  loss: 1.3750  time: 0.2807  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [250/830]  eta: 0:02:59  lr: 0.000000  loss: 1.9109  time: 0.2791  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [300/830]  eta: 0:02:41  lr: 0.000000  loss: 1.9555  time: 0.2813  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [350/830]  eta: 0:02:24  lr: 0.000000  loss: 1.8637  time: 0.2824  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [400/830]  eta: 0:02:08  lr: 0.000000  loss: 2.5492  time: 0.2821  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [450/830]  eta: 0:01:53  lr: 0.000000  loss: 2.1875  time: 0.2799  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [500/830]  eta: 0:01:37  lr: 0.000000  loss: 1.8932  time: 0.2822  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [550/830]  eta: 0:01:22  lr: 0.000000  loss: 3.4205  time: 0.2828  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [600/830]  eta: 0:01:07  lr: 0.000000  loss: 2.5537  time: 0.2791  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [650/830]  eta: 0:00:52  lr: 0.000000  loss: 2.2020  time: 0.2806  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [700/830]  eta: 0:00:37  lr: 0.000000  loss: 2.3885  time: 0.2827  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [750/830]  eta: 0:00:23  lr: 0.000000  loss: 2.3916  time: 0.2865  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [800/830]  eta: 0:00:08  lr: 0.000000  loss: 1.5112  time: 0.2858  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26]  [829/830]  eta: 0:00:00  lr: 0.000000  loss: 2.1303  time: 0.2832  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [26] Total time: 0:04:01 (0.2907 s / it)\n",
            "2026-02-02 01:40:51,583 [INFO] Averaged stats: lr: 0.0000  loss: 2.2426\n",
            "2026-02-02 01:40:51,592 [INFO] No validation splits found.\n",
            "2026-02-02 01:40:51,620 [INFO] Saving checkpoint at epoch 26 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_26.pth.\n",
            "2026-02-02 01:40:56,252 [INFO] Start training\n",
            "2026-02-02 01:40:56,285 [INFO] Start training epoch 27, 830 iters per inner epoch.\n",
            "Train: data epoch: [27]  [  0/830]  eta: 1:11:43  lr: 0.000000  loss: 1.5596  time: 5.1846  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [ 50/830]  eta: 0:05:01  lr: 0.000000  loss: 2.6948  time: 0.2940  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [100/830]  eta: 0:04:09  lr: 0.000000  loss: 1.5907  time: 0.2906  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [150/830]  eta: 0:03:40  lr: 0.000000  loss: 2.6627  time: 0.2932  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [200/830]  eta: 0:03:17  lr: 0.000000  loss: 2.2331  time: 0.2840  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [250/830]  eta: 0:02:58  lr: 0.000000  loss: 2.1696  time: 0.2833  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [300/830]  eta: 0:02:41  lr: 0.000000  loss: 1.9964  time: 0.2831  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [350/830]  eta: 0:02:24  lr: 0.000000  loss: 2.2448  time: 0.2825  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [400/830]  eta: 0:02:08  lr: 0.000000  loss: 1.4327  time: 0.2803  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [450/830]  eta: 0:01:52  lr: 0.000000  loss: 2.8073  time: 0.2804  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [500/830]  eta: 0:01:37  lr: 0.000000  loss: 2.8899  time: 0.2818  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [550/830]  eta: 0:01:22  lr: 0.000000  loss: 2.5614  time: 0.2815  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [600/830]  eta: 0:01:07  lr: 0.000000  loss: 1.6807  time: 0.2787  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [650/830]  eta: 0:00:52  lr: 0.000000  loss: 2.6516  time: 0.2793  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [700/830]  eta: 0:00:37  lr: 0.000000  loss: 2.0717  time: 0.2824  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [750/830]  eta: 0:00:23  lr: 0.000000  loss: 2.8755  time: 0.2774  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [800/830]  eta: 0:00:08  lr: 0.000000  loss: 2.4644  time: 0.2797  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27]  [829/830]  eta: 0:00:00  lr: 0.000000  loss: 2.0572  time: 0.2861  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [27] Total time: 0:04:00 (0.2893 s / it)\n",
            "2026-02-02 01:44:56,396 [INFO] Averaged stats: lr: 0.0000  loss: 2.2454\n",
            "2026-02-02 01:44:56,403 [INFO] No validation splits found.\n",
            "2026-02-02 01:44:56,430 [INFO] Saving checkpoint at epoch 27 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_27.pth.\n",
            "2026-02-02 01:45:01,054 [INFO] Start training\n",
            "2026-02-02 01:45:01,087 [INFO] Start training epoch 28, 830 iters per inner epoch.\n",
            "Train: data epoch: [28]  [  0/830]  eta: 0:36:23  lr: 0.000000  loss: 2.3258  time: 2.6312  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [ 50/830]  eta: 0:04:54  lr: 0.000000  loss: 1.8991  time: 0.2889  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [100/830]  eta: 0:04:02  lr: 0.000000  loss: 2.1856  time: 0.2841  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [150/830]  eta: 0:03:39  lr: 0.000000  loss: 2.5197  time: 0.3059  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [200/830]  eta: 0:03:17  lr: 0.000000  loss: 1.8551  time: 0.2789  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [250/830]  eta: 0:02:58  lr: 0.000000  loss: 2.2247  time: 0.2814  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [300/830]  eta: 0:02:40  lr: 0.000000  loss: 2.4633  time: 0.2836  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [350/830]  eta: 0:02:23  lr: 0.000000  loss: 2.3651  time: 0.2860  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [400/830]  eta: 0:02:07  lr: 0.000000  loss: 2.7209  time: 0.2788  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [450/830]  eta: 0:01:52  lr: 0.000000  loss: 2.2943  time: 0.2797  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [500/830]  eta: 0:01:37  lr: 0.000000  loss: 2.6237  time: 0.2818  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [550/830]  eta: 0:01:22  lr: 0.000000  loss: 2.1234  time: 0.2854  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [600/830]  eta: 0:01:07  lr: 0.000000  loss: 2.7742  time: 0.2850  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [650/830]  eta: 0:00:52  lr: 0.000000  loss: 2.0318  time: 0.2805  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [700/830]  eta: 0:00:37  lr: 0.000000  loss: 1.9028  time: 0.2802  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [750/830]  eta: 0:00:23  lr: 0.000000  loss: 2.2279  time: 0.2794  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [800/830]  eta: 0:00:08  lr: 0.000000  loss: 2.5767  time: 0.2810  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28]  [829/830]  eta: 0:00:00  lr: 0.000000  loss: 2.7059  time: 0.2873  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [28] Total time: 0:04:00 (0.2893 s / it)\n",
            "2026-02-02 01:49:01,247 [INFO] Averaged stats: lr: 0.0000  loss: 2.2555\n",
            "2026-02-02 01:49:01,255 [INFO] No validation splits found.\n",
            "2026-02-02 01:49:01,284 [INFO] Saving checkpoint at epoch 28 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_28.pth.\n",
            "2026-02-02 01:49:05,887 [INFO] Start training\n",
            "2026-02-02 01:49:05,920 [INFO] Start training epoch 29, 830 iters per inner epoch.\n",
            "Train: data epoch: [29]  [  0/830]  eta: 0:36:36  lr: 0.000000  loss: 2.5125  time: 2.6467  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [ 50/830]  eta: 0:05:00  lr: 0.000000  loss: 2.8006  time: 0.2970  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [100/830]  eta: 0:04:08  lr: 0.000000  loss: 1.9648  time: 0.2953  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [150/830]  eta: 0:03:42  lr: 0.000000  loss: 2.1224  time: 0.2858  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [200/830]  eta: 0:03:19  lr: 0.000000  loss: 2.3146  time: 0.2846  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [250/830]  eta: 0:02:59  lr: 0.000000  loss: 2.5906  time: 0.2831  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [300/830]  eta: 0:02:42  lr: 0.000000  loss: 2.0566  time: 0.2849  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [350/830]  eta: 0:02:25  lr: 0.000000  loss: 2.5002  time: 0.2817  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [400/830]  eta: 0:02:09  lr: 0.000000  loss: 2.8901  time: 0.2838  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [450/830]  eta: 0:01:53  lr: 0.000000  loss: 2.7394  time: 0.2905  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [500/830]  eta: 0:01:38  lr: 0.000000  loss: 1.4590  time: 0.2799  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [550/830]  eta: 0:01:22  lr: 0.000000  loss: 1.8422  time: 0.2791  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [600/830]  eta: 0:01:07  lr: 0.000000  loss: 2.3547  time: 0.2809  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [650/830]  eta: 0:00:52  lr: 0.000000  loss: 2.6336  time: 0.2827  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [700/830]  eta: 0:00:38  lr: 0.000000  loss: 2.6452  time: 0.2873  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [750/830]  eta: 0:00:23  lr: 0.000000  loss: 1.8128  time: 0.2832  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [800/830]  eta: 0:00:08  lr: 0.000000  loss: 2.0054  time: 0.2849  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29]  [829/830]  eta: 0:00:00  lr: 0.000000  loss: 2.4385  time: 0.2858  data: 0.0000  max mem: 34373\n",
            "Train: data epoch: [29] Total time: 0:04:02 (0.2919 s / it)\n",
            "2026-02-02 01:53:08,186 [INFO] Averaged stats: lr: 0.0000  loss: 2.2427\n",
            "2026-02-02 01:53:08,194 [INFO] No validation splits found.\n",
            "2026-02-02 01:53:08,223 [INFO] Saving checkpoint at epoch 29 to /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_29.pth.\n",
            "2026-02-02 01:53:12,867 [INFO] No validation splits found.\n",
            "2026-02-02 01:53:12,868 [INFO] Training time 1:33:36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 5.2: (Optional) Resume Training from Checkpoint\n",
        "# Use this if training was interrupted\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 5.2: Resume Training (Optional)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Find latest checkpoint\n",
        "checkpoint_dir = f\"{OUT_ROOT}/rsitmd_finetuned\"\n",
        "ckpt_pattern = f\"{checkpoint_dir}/*/checkpoint_*.pth\"\n",
        "ckpts = sorted(glob.glob(ckpt_pattern))\n",
        "\n",
        "if ckpts:\n",
        "    latest_ckpt = ckpts[-1]\n",
        "    print(f\"Found {len(ckpts)} checkpoints\")\n",
        "    print(f\"Latest: {latest_ckpt}\")\n",
        "\n",
        "    # Create resume config\n",
        "    resume_config = f'''model:\n",
        "  arch: rsgpt\n",
        "  model_type: vicuna13b\n",
        "  freeze_vit: True\n",
        "  freeze_qformer: False\n",
        "  max_txt_len: 160\n",
        "  llm_model: lmsys/vicuna-13b-v1.5\n",
        "  end_sym: \"###\"\n",
        "  prompt_path: \"prompts/alignment.txt\"\n",
        "\n",
        "datasets:\n",
        "  rsitmd_instruction:\n",
        "    vis_processor:\n",
        "      train:\n",
        "        name: \"rs_image_train\"\n",
        "        image_size: 224\n",
        "    text_processor:\n",
        "      train:\n",
        "        name: \"blip_caption\"\n",
        "    build_info:\n",
        "      storage: dataset/RSITMD/\n",
        "\n",
        "run:\n",
        "  task: image_text_pretrain\n",
        "  lr_sched: \"linear_warmup_cosine_lr\"\n",
        "  init_lr: 1e-5\n",
        "  min_lr: 1e-6\n",
        "  warmup_lr: 1e-7\n",
        "  warmup_steps: 200\n",
        "  weight_decay: 0.05\n",
        "\n",
        "  max_epoch: 15\n",
        "  iters_per_epoch: {ITERS_PER_EPOCH}\n",
        "  batch_size_train: 4\n",
        "  batch_size_eval: 4\n",
        "  accum_grad_iters: 4\n",
        "  num_workers: 4\n",
        "\n",
        "  seed: 42\n",
        "  output_dir: \"{OUT_ROOT}/rsitmd_finetuned\"\n",
        "\n",
        "  amp: True\n",
        "  resume_ckpt_path: \"{latest_ckpt}\"\n",
        "\n",
        "  evaluate: False\n",
        "  train_splits: [\"train\"]\n",
        "\n",
        "  device: \"cuda\"\n",
        "  world_size: 1\n",
        "  dist_url: \"env://\"\n",
        "  distributed: True\n",
        "'''\n",
        "    resume_config_path = f\"{PROJECT_ROOT}/train_configs/rsitmd_resume.yaml\"\n",
        "    with open(resume_config_path, 'w') as f:\n",
        "        f.write(resume_config)\n",
        "\n",
        "    print(f\"\\nCreated resume config: {resume_config_path}\")\n",
        "    print(\"\\nTo resume, uncomment and run the command below:\")\n",
        "    print(f\"# !torchrun --nproc_per_node=1 train.py --cfg-path train_configs/rsitmd_resume.yaml\")\n",
        "else:\n",
        "    print(\"No checkpoints found. Start fresh training with Step 5.1\")"
      ],
      "metadata": {
        "id": "resume_training",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 6: EVALUATE THE MODEL\n",
        "\n",
        "**IMPORTANT:** If you see import errors, restart the runtime first:\n",
        "- Click Runtime → Restart runtime\n",
        "- Then run the evaluation cell directly (skip dependency installation)"
      ],
      "metadata": {
        "id": "step6_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 6.0 QUICK TEST: Full Pipeline with 10 Images\n",
        "# ============================================\n",
        "# Tests EVERYTHING on just 10 images (~2-3 minutes)\n",
        "# - Path checks\n",
        "# - Model loading\n",
        "# - Caption generation\n",
        "# - Metric computation (BLEU, ROUGE-L, CIDEr)\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import argparse\n",
        "from PIL import Image\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION\n",
        "# ============================================\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/data/RSGPT\"\n",
        "RSITMD_BASE = f\"{PROJECT_ROOT}/dataset/RSITMD\"\n",
        "\n",
        "# CORRECT checkpoint folder\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned/20260131202\"\n",
        "CHECKPOINT_PATH = f\"{CHECKPOINT_DIR}/checkpoint_14.pth\"\n",
        "EVAL_CONFIG = \"eval_configs/rsitmd_eval.yaml\"\n",
        "\n",
        "# Only test 10 images\n",
        "NUM_TEST_IMAGES = 10\n",
        "\n",
        "# ============================================\n",
        "# Setup\n",
        "# ============================================\n",
        "print(\"=\"*60)\n",
        "print(\"QUICK TEST - Full Pipeline with 10 Images\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from google.colab import drive\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "os.chdir(PROJECT_ROOT)\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "# Check paths exist\n",
        "print(\"\\n[1] Checking paths...\")\n",
        "checks = [\n",
        "    (PROJECT_ROOT, \"Project root\"),\n",
        "    (RSITMD_BASE, \"RSITMD dataset\"),\n",
        "    (f\"{RSITMD_BASE}/images\", \"Images folder\"),\n",
        "    (CHECKPOINT_DIR, \"Checkpoint folder\"),\n",
        "    (CHECKPOINT_PATH, \"Checkpoint file\"),\n",
        "    (EVAL_CONFIG, \"Eval config\"),\n",
        "]\n",
        "\n",
        "all_ok = True\n",
        "for path, name in checks:\n",
        "    exists = os.path.exists(path)\n",
        "    status = \"✓\" if exists else \"❌\"\n",
        "    print(f\"  {status} {name}: {path}\")\n",
        "    if not exists:\n",
        "        all_ok = False\n",
        "\n",
        "if not all_ok:\n",
        "    print(\"\\n❌ Some paths missing! Fix before continuing.\")\n",
        "    raise FileNotFoundError(\"Missing required files\")\n",
        "\n",
        "print(\"\\n  ✓ All paths OK!\")\n",
        "\n",
        "# ============================================\n",
        "# Load model\n",
        "# ============================================\n",
        "print(\"\\n[2] Loading model (this takes ~1 min)...\")\n",
        "\n",
        "import torch\n",
        "from rsgpt.common.config import Config\n",
        "from rsgpt.common.registry import registry\n",
        "from rsgpt.datasets.builders import *\n",
        "from rsgpt.models import *\n",
        "from rsgpt.processors import *\n",
        "\n",
        "cfg = Config(argparse.Namespace(cfg_path=EVAL_CONFIG, options=None))\n",
        "model_cls = registry.get_model_class(cfg.model_cfg.arch)\n",
        "model = model_cls.from_config(cfg.model_cfg).cuda()\n",
        "\n",
        "try:\n",
        "    vis_processor_cfg = cfg.datasets_cfg.rsitmd_instruction.vis_processor.train\n",
        "    vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
        "except:\n",
        "    from torchvision import transforms\n",
        "    vis_processor = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                           std=[0.26862954, 0.26130258, 0.27577711])\n",
        "    ])\n",
        "\n",
        "print(\"  ✓ Model loaded\")\n",
        "\n",
        "# ============================================\n",
        "# Load checkpoint\n",
        "# ============================================\n",
        "print(\"\\n[3] Loading checkpoint weights...\")\n",
        "checkpoint = torch.load(CHECKPOINT_PATH, map_location=\"cuda\")\n",
        "model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
        "model.eval()\n",
        "print(f\"  ✓ Loaded epoch {checkpoint.get('epoch', '?')}\")\n",
        "\n",
        "# ============================================\n",
        "# Load test data and COCO GT\n",
        "# ============================================\n",
        "print(f\"\\n[4] Loading test data...\")\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "test_file = f\"{RSITMD_BASE}/rsitmd_cap_processed_instruction_test.json\"\n",
        "with open(test_file) as f:\n",
        "    all_test_data = json.load(f)[\"annotations\"]\n",
        "\n",
        "# Take first N images\n",
        "test_data = all_test_data[:NUM_TEST_IMAGES]\n",
        "print(f\"  ✓ Using {len(test_data)} of {len(all_test_data)} test images\")\n",
        "\n",
        "# Load COCO GT\n",
        "coco = COCO(f\"{RSITMD_BASE}/rsitmd_test_gt.json\")\n",
        "print(\"  ✓ COCO GT loaded\")\n",
        "\n",
        "# ============================================\n",
        "# Generate captions\n",
        "# ============================================\n",
        "print(f\"\\n[5] Generating captions...\")\n",
        "\n",
        "STOP_TOKEN = \"###\"\n",
        "def clean_caption(caption):\n",
        "    if STOP_TOKEN in caption:\n",
        "        caption = caption.split(STOP_TOKEN)[0]\n",
        "    return caption.strip()\n",
        "\n",
        "preds = []\n",
        "garbage_count = 0\n",
        "garbage_patterns = ['```', '<script>', '<img>', '<video>', '<audio>', 'Aerogado', '19780']\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, item in enumerate(test_data):\n",
        "        img_path = f\"{RSITMD_BASE}/images/{item['filename']}\"\n",
        "        image = vis_processor(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).cuda()\n",
        "\n",
        "        caption = model.generate(\n",
        "            {\"image\": image, \"prompt\": \"Briefly describe the content of the image.\"},\n",
        "            use_nucleus_sampling=False,\n",
        "            num_beams=5,\n",
        "            max_length=80,\n",
        "            min_length=8,\n",
        "            repetition_penalty=1.2,\n",
        "            length_penalty=1.0,\n",
        "            num_captions=1,\n",
        "        )[0]\n",
        "\n",
        "        caption = clean_caption(caption)\n",
        "\n",
        "        # Check for garbage\n",
        "        is_garbage = any(p in caption for p in garbage_patterns) or len(caption.strip()) == 0\n",
        "        if is_garbage:\n",
        "            garbage_count += 1\n",
        "\n",
        "        status = \"❌\" if is_garbage else \"✓\"\n",
        "        print(f\"  [{idx+1}/{NUM_TEST_IMAGES}] {status} {caption[:50]}...\")\n",
        "\n",
        "        preds.append({\n",
        "            \"image_id\": int(item[\"image_id\"]),\n",
        "            \"caption\": caption\n",
        "        })\n",
        "\n",
        "print(f\"\\n  ✓ Generated {len(preds)} captions\")\n",
        "print(f\"  Valid: {NUM_TEST_IMAGES - garbage_count}/{NUM_TEST_IMAGES}\")\n",
        "print(f\"  Garbage: {garbage_count}/{NUM_TEST_IMAGES}\")\n",
        "\n",
        "# ============================================\n",
        "# Compute Metrics\n",
        "# ============================================\n",
        "print(\"\\n[6] Computing metrics (BLEU, ROUGE-L, CIDEr)...\")\n",
        "\n",
        "from pycocoevalcap.bleu.bleu import Bleu\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "\n",
        "# Prepare data for scorers\n",
        "gts = {}\n",
        "res = {}\n",
        "for pred in preds:\n",
        "    img_id = pred[\"image_id\"]\n",
        "    gts[img_id] = [ann[\"caption\"] for ann in coco.imgToAnns.get(img_id, [])]\n",
        "    res[img_id] = [pred[\"caption\"]]\n",
        "\n",
        "# Show what we're computing\n",
        "print(f\"\\n  Ground truth refs per image: {len(list(gts.values())[0])}\")\n",
        "print(f\"  Computing on {len(gts)} images...\")\n",
        "\n",
        "# Scorers (no METEOR - it hangs)\n",
        "scorers = [\n",
        "    (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
        "    (Rouge(), \"ROUGE_L\"),\n",
        "    (Cider(), \"CIDEr\"),\n",
        "]\n",
        "\n",
        "metrics = {}\n",
        "for scorer, method in scorers:\n",
        "    try:\n",
        "        method_name = method if isinstance(method, str) else method[0]\n",
        "        print(f\"  Computing {method_name}...\", end=\" \")\n",
        "        score, _ = scorer.compute_score(gts, res)\n",
        "        if isinstance(method, list):\n",
        "            for m, s in zip(method, score):\n",
        "                metrics[m] = s\n",
        "        else:\n",
        "            metrics[method] = score\n",
        "        print(\"✓\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        if isinstance(method, list):\n",
        "            for m in method:\n",
        "                metrics[m] = 0.0\n",
        "        else:\n",
        "            metrics[method] = 0.0\n",
        "\n",
        "# ============================================\n",
        "# Results\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"QUICK TEST RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nCaption Quality:\")\n",
        "print(f\"  Valid captions: {NUM_TEST_IMAGES - garbage_count}/{NUM_TEST_IMAGES}\")\n",
        "print(f\"  Garbage outputs: {garbage_count}/{NUM_TEST_IMAGES}\")\n",
        "\n",
        "print(f\"\\n\" + \"-\"*40)\n",
        "print(\"METRICS (on 10 images):\")\n",
        "print(\"-\"*40)\n",
        "print(f\"  BLEU-1:  {metrics.get('Bleu_1', 0):.4f}\")\n",
        "print(f\"  BLEU-2:  {metrics.get('Bleu_2', 0):.4f}\")\n",
        "print(f\"  BLEU-3:  {metrics.get('Bleu_3', 0):.4f}\")\n",
        "print(f\"  BLEU-4:  {metrics.get('Bleu_4', 0):.4f}\")\n",
        "print(f\"  ROUGE-L: {metrics.get('ROUGE_L', 0):.4f}\")\n",
        "print(f\"  CIDEr:   {metrics.get('CIDEr', 0):.4f}\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "# Verdict\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VERDICT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "all_metrics_ok = all(v > 0 for v in metrics.values())\n",
        "captions_ok = garbage_count == 0\n",
        "\n",
        "if captions_ok and all_metrics_ok:\n",
        "    print(\"\"\"\n",
        "✅ ALL TESTS PASSED!\n",
        "\n",
        "  ✓ Model loads correctly\n",
        "  ✓ Captions are valid English\n",
        "  ✓ All metrics compute successfully\n",
        "\n",
        "  → Run the FULL evaluation: rsitmd_eval_FINAL.py\n",
        "\"\"\")\n",
        "elif captions_ok and not all_metrics_ok:\n",
        "    print(\"\"\"\n",
        "⚠️ CAPTIONS OK, BUT METRIC ISSUES\n",
        "\n",
        "  ✓ Captions are valid\n",
        "  ❌ Some metrics failed to compute\n",
        "\n",
        "  → Check error messages above\n",
        "\"\"\")\n",
        "elif not captions_ok and all_metrics_ok:\n",
        "    print(\"\"\"\n",
        "⚠️ SOME GARBAGE CAPTIONS\n",
        "\n",
        "  ❌ Some outputs are garbage\n",
        "  ✓ Metrics can compute\n",
        "\n",
        "  → Check if you're using the right checkpoint\n",
        "\"\"\")\n",
        "else:\n",
        "    print(\"\"\"\n",
        "❌ MULTIPLE ISSUES\n",
        "\n",
        "  ❌ Garbage captions detected\n",
        "  ❌ Metric computation problems\n",
        "\n",
        "  → Do NOT proceed to full evaluation\n",
        "  → Fix issues first\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\\nSample Outputs:\")\n",
        "print(\"=\"*60)\n",
        "for idx, (item, pred) in enumerate(zip(test_data[:5], preds[:5])):\n",
        "    gt = item[\"text_output\"][0] if item[\"text_output\"] else \"N/A\"\n",
        "    print(f\"\\n[{idx}] Image: {item['filename']}\")\n",
        "    print(f\"    Generated: {pred['caption']}\")\n",
        "    print(f\"    GT:        {gt}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9bb28457aa7b4920b26386bd01f20be1",
            "e8fbdcdbb4214b779f8117248bd1aa0f",
            "d863c9627526417ab63d25a521a6ff43",
            "bee198ad44f9479fb3b600f6974be603",
            "3dc8ecc15a7e4009a4a194925f7850a1",
            "6777ff73039149378291476f05659cd2",
            "2dc03a5067ed46c58fb758cdc0a66376",
            "77191c39346c404d82184a46738093bc",
            "c1be5349b7c74dfb83b03df2626245bd",
            "ebb2f4f0a6d04511850dd5b2c5409507",
            "a314c6523a2148278db6067e7241c1d4"
          ]
        },
        "id": "cRXDmjv24WgA",
        "outputId": "5b1de5aa-58e5-4140-c1d5-d453eec7a3ad",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "QUICK TEST - Full Pipeline with 10 Images\n",
            "============================================================\n",
            "\n",
            "[1] Checking paths...\n",
            "  ✓ Project root: /content/drive/MyDrive/data/RSGPT\n",
            "  ✓ RSITMD dataset: /content/drive/MyDrive/data/RSGPT/dataset/RSITMD\n",
            "  ✓ Images folder: /content/drive/MyDrive/data/RSGPT/dataset/RSITMD/images\n",
            "  ✓ Checkpoint folder: /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned/20260131202\n",
            "  ✓ Checkpoint file: /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned/20260131202/checkpoint_14.pth\n",
            "  ✓ Eval config: eval_configs/rsitmd_eval.yaml\n",
            "\n",
            "  ✓ All paths OK!\n",
            "\n",
            "[2] Loading model (this takes ~1 min)...\n",
            "Loading VIT\n",
            "Loading VIT Done\n",
            "Loading Q-Former\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Q-Former Done\n",
            "Loading LLAMA\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bb28457aa7b4920b26386bd01f20be1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LLAMA Done\n",
            "  📥 Loading pretrained weights from: https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna13b_trimmed.pth\n",
            "  ✓ Model loaded\n",
            "\n",
            "[3] Loading checkpoint weights...\n",
            "  ✓ Loaded epoch 14\n",
            "\n",
            "[4] Loading test data...\n",
            "  ✓ Using 10 of 712 test images\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "  ✓ COCO GT loaded\n",
            "\n",
            "[5] Generating captions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1283: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [1/10] ✓ There are three tanks of different sizes near the ...\n",
            "  [2/10] ✓ There is a piece of land in the middle of the dese...\n",
            "  [3/10] ✓ There is a lot of sand in the desert....\n",
            "  [4/10] ✓ There are many cars on the bridge....\n",
            "  [5/10] ✓ There are many green trees around the football fie...\n",
            "  [6/10] ✓ There are many buildings around the lake....\n",
            "  [7/10] ✓ the airport is surrounded by many buildings and a ...\n",
            "  [8/10] ✓ There are many green trees on both sides of the ro...\n",
            "  [9/10] ✓ The church is located in the center of the square....\n",
            "  [10/10] ✓ There are many factories in the industrial area....\n",
            "\n",
            "  ✓ Generated 10 captions\n",
            "  Valid: 10/10\n",
            "  Garbage: 0/10\n",
            "\n",
            "[6] Computing metrics (BLEU, ROUGE-L, CIDEr)...\n",
            "\n",
            "  Ground truth refs per image: 5\n",
            "  Computing on 10 images...\n",
            "  Computing Bleu_1... {'testlen': 94, 'reflen': 103, 'guess': [94, 84, 74, 64], 'correct': [47, 16, 5, 1]}\n",
            "ratio: 0.9126213592144405\n",
            "✓\n",
            "  Computing ROUGE_L... ✓\n",
            "  Computing CIDEr... ✓\n",
            "\n",
            "============================================================\n",
            "QUICK TEST RESULTS\n",
            "============================================================\n",
            "\n",
            "Caption Quality:\n",
            "  Valid captions: 10/10\n",
            "  Garbage outputs: 0/10\n",
            "\n",
            "----------------------------------------\n",
            "METRICS (on 10 images):\n",
            "----------------------------------------\n",
            "  BLEU-1:  0.4543\n",
            "  BLEU-2:  0.2804\n",
            "  BLEU-3:  0.1690\n",
            "  BLEU-4:  0.0910\n",
            "  ROUGE-L: 0.3194\n",
            "  CIDEr:   0.3711\n",
            "----------------------------------------\n",
            "\n",
            "============================================================\n",
            "VERDICT\n",
            "============================================================\n",
            "\n",
            "✅ ALL TESTS PASSED!\n",
            "\n",
            "  ✓ Model loads correctly\n",
            "  ✓ Captions are valid English\n",
            "  ✓ All metrics compute successfully\n",
            "  \n",
            "  → Run the FULL evaluation: rsitmd_eval_FINAL.py\n",
            "\n",
            "============================================================\n",
            "\n",
            "Sample Outputs:\n",
            "============================================================\n",
            "\n",
            "[0] Image: storagetanks_4442.tif\n",
            "    Generated: There are three tanks of different sizes near the road.\n",
            "    GT:        There is a lot of grass on the ground.\n",
            "\n",
            "[1] Image: bareland_638.tif\n",
            "    Generated: There is a piece of land in the middle of the desert.\n",
            "    GT:        Do you have a bicycle print?\n",
            "\n",
            "[2] Image: desert_1575.tif\n",
            "    Generated: There is a lot of sand in the desert.\n",
            "    GT:        There is a khaki pattern in the desert.\n",
            "\n",
            "[3] Image: bridge_970.tif\n",
            "    Generated: There are many cars on the bridge.\n",
            "    GT:        Some cars are on three parallel bridges on the Green River.\n",
            "\n",
            "[4] Image: stadium_387.tif\n",
            "    Generated: There are many green trees around the football field.\n",
            "    GT:        There is a grand white stadium next to the house.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 6.1: Evaluate ALL RSITMD v2 Checkpoints (Find Best Epoch)\n",
        "# ============================================\n",
        "# Evaluates every checkpoint, picks best by CIDEr\n",
        "# Paste this as a new cell after restarting runtime\n",
        "# ============================================\n",
        "\n",
        "import os, sys, json, glob, argparse\n",
        "from PIL import Image\n",
        "\n",
        "# ============================================\n",
        "# ⬇️ UPDATE THIS PATH with your actual folder name!\n",
        "#    Run first:  !ls /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/\n",
        "#    Then replace REPLACE_WITH_FOLDER_NAME below\n",
        "# ============================================\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/data/RSGPT\"\n",
        "RSITMD_BASE  = f\"{PROJECT_ROOT}/dataset/RSITMD\"\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001\"\n",
        "EVAL_CONFIG  = \"eval_configs/rsitmd_eval.yaml\"\n",
        "\n",
        "# ============================================\n",
        "# Setup\n",
        "# ============================================\n",
        "from google.colab import drive\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "os.chdir(PROJECT_ROOT)\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "import torch\n",
        "from rsgpt.common.config import Config\n",
        "from rsgpt.common.registry import registry\n",
        "from rsgpt.datasets.builders import *\n",
        "from rsgpt.models import *\n",
        "from rsgpt.processors import *\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "from pycocoevalcap.bleu.bleu import Bleu\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"RSITMD v2 - Evaluate ALL Checkpoints\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "STOP_TOKEN = \"###\"\n",
        "def clean_caption(caption):\n",
        "    if STOP_TOKEN in caption:\n",
        "        caption = caption.split(STOP_TOKEN)[0]\n",
        "    return caption.strip()\n",
        "\n",
        "# [1] Find checkpoints\n",
        "print(\"\\n[1] Finding checkpoints...\")\n",
        "ckpts = sorted(glob.glob(f\"{CHECKPOINT_DIR}/checkpoint_*.pth\"))\n",
        "print(f\"  Found {len(ckpts)} checkpoints:\")\n",
        "for c in ckpts:\n",
        "    print(f\"    - {os.path.basename(c)}\")\n",
        "if not ckpts:\n",
        "    raise FileNotFoundError(f\"No checkpoints in {CHECKPOINT_DIR}\")\n",
        "\n",
        "# [2] Load test data\n",
        "print(\"\\n[2] Loading test data...\")\n",
        "with open(f\"{RSITMD_BASE}/rsitmd_cap_processed_instruction_test.json\") as f:\n",
        "    test_data = json.load(f)[\"annotations\"]\n",
        "print(f\"  ✓ {len(test_data)} test images\")\n",
        "\n",
        "total_words = sum(len(cap.split()) for item in test_data for cap in item[\"text_output\"])\n",
        "total_caps = sum(len(item[\"text_output\"]) for item in test_data)\n",
        "avg_gt_len = total_words / total_caps\n",
        "\n",
        "coco = COCO(f\"{RSITMD_BASE}/rsitmd_test_gt.json\")\n",
        "print(\"  ✓ COCO GT loaded\")\n",
        "\n",
        "# [3] Load model (once)\n",
        "print(\"\\n[3] Loading model...\")\n",
        "cfg = Config(argparse.Namespace(cfg_path=EVAL_CONFIG, options=None))\n",
        "model_cls = registry.get_model_class(cfg.model_cfg.arch)\n",
        "model = model_cls.from_config(cfg.model_cfg).cuda()\n",
        "\n",
        "try:\n",
        "    vp_cfg = cfg.datasets_cfg.rsitmd_instruction.vis_processor.train\n",
        "    vis_processor = registry.get_processor_class(vp_cfg.name).from_config(vp_cfg)\n",
        "except:\n",
        "    from torchvision import transforms\n",
        "    vis_processor = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                           std=[0.26862954, 0.26130258, 0.27577711])\n",
        "    ])\n",
        "print(\"  ✓ Model ready\")\n",
        "\n",
        "# [4] Evaluate each checkpoint\n",
        "EVAL_PROMPT = \"Briefly describe the content of the image.\"\n",
        "best = {\"ckpt\": None, \"CIDEr\": -1.0, \"metrics\": None, \"epoch\": None}\n",
        "all_results = []\n",
        "\n",
        "for ckpt_idx, ckpt in enumerate(ckpts):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"[{ckpt_idx+1}/{len(ckpts)}] {os.path.basename(ckpt)}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    checkpoint = torch.load(ckpt, map_location=\"cuda\")\n",
        "    model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
        "    model.eval()\n",
        "    epoch = checkpoint.get('epoch', ckpt_idx)\n",
        "\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for img_idx, item in enumerate(test_data):\n",
        "            if (img_idx + 1) % 200 == 0 or img_idx == 0:\n",
        "                print(f\"    {img_idx+1}/{len(test_data)}\")\n",
        "\n",
        "            img_path = f\"{RSITMD_BASE}/images/{item['filename']}\"\n",
        "            image = vis_processor(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).cuda()\n",
        "\n",
        "            caption = model.generate(\n",
        "                {\"image\": image, \"prompt\": item.get(\"text_input\", EVAL_PROMPT)},\n",
        "                use_nucleus_sampling=False,\n",
        "                num_beams=5,\n",
        "                max_length=80,\n",
        "                min_length=8,\n",
        "                repetition_penalty=1.2,\n",
        "                length_penalty=1.0,\n",
        "                num_captions=1,\n",
        "            )[0]\n",
        "            preds.append({\"image_id\": int(item[\"image_id\"]), \"caption\": clean_caption(caption)})\n",
        "\n",
        "    avg_len = sum(len(p[\"caption\"].split()) for p in preds) / len(preds)\n",
        "\n",
        "    # Compute metrics\n",
        "    gts, res = {}, {}\n",
        "    for pred in preds:\n",
        "        gts[pred[\"image_id\"]] = [a[\"caption\"] for a in coco.imgToAnns.get(pred[\"image_id\"], [])]\n",
        "        res[pred[\"image_id\"]] = [pred[\"caption\"]]\n",
        "\n",
        "    metrics = {}\n",
        "    for scorer, method in [(Bleu(4), [\"Bleu_1\",\"Bleu_2\",\"Bleu_3\",\"Bleu_4\"]), (Rouge(), \"ROUGE_L\"), (Cider(), \"CIDEr\")]:\n",
        "        try:\n",
        "            score, _ = scorer.compute_score(gts, res)\n",
        "            if isinstance(method, list):\n",
        "                for m, s in zip(method, score): metrics[m] = s\n",
        "            else:\n",
        "                metrics[method] = score\n",
        "        except Exception as e:\n",
        "            print(f\"    ⚠️ {method}: {e}\")\n",
        "\n",
        "    cider = metrics.get(\"CIDEr\", 0.0)\n",
        "    print(f\"\\n  Epoch {epoch}: B1={metrics.get('Bleu_1',0):.4f}  B4={metrics.get('Bleu_4',0):.4f}  \"\n",
        "          f\"R={metrics.get('ROUGE_L',0):.4f}  CIDEr={cider:.4f}  ({avg_len:.0f} words)\")\n",
        "\n",
        "    all_results.append({\"checkpoint\": ckpt, \"epoch\": epoch, \"avg_len\": avg_len, \"metrics\": metrics})\n",
        "\n",
        "    if cider > best[\"CIDEr\"]:\n",
        "        best = {\"ckpt\": ckpt, \"CIDEr\": cider, \"metrics\": metrics, \"epoch\": epoch}\n",
        "        print(f\"  >>> ★ NEW BEST!\")\n",
        "\n",
        "    # Show samples from first checkpoint\n",
        "    if ckpt_idx == 0:\n",
        "        for i in range(min(2, len(preds))):\n",
        "            gt = [a[\"caption\"] for a in coco.imgToAnns.get(preds[i][\"image_id\"], [])]\n",
        "            print(f\"    Gen: {preds[i]['caption'][:80]}\")\n",
        "            print(f\"    GT:  {gt[0][:80] if gt else 'N/A'}\")\n",
        "\n",
        "# ============================================\n",
        "# [5] Summary Table\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n{'Epoch':<8} {'BLEU-1':>8} {'BLEU-4':>8} {'ROUGE-L':>8} {'CIDEr':>8}\")\n",
        "print(\"-\" * 44)\n",
        "for r in all_results:\n",
        "    m = r[\"metrics\"]\n",
        "    star = \" ★\" if r[\"checkpoint\"] == best[\"ckpt\"] else \"\"\n",
        "    print(f\"  {r['epoch']:<6} {m.get('Bleu_1',0):>8.4f} {m.get('Bleu_4',0):>8.4f} \"\n",
        "          f\"{m.get('ROUGE_L',0):>8.4f} {m.get('CIDEr',0):>8.4f}{star}\")\n",
        "\n",
        "print(f\"\\n★ BEST: Epoch {best['epoch']} → CIDEr = {best['CIDEr']:.4f}\")\n",
        "print(f\"  Path: {best['ckpt']}\")\n",
        "\n",
        "# Compare v2 vs v1 vs RSICD\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Comparison:  v2 (with RSICap)  vs  v1 (without)  vs  RSICD\")\n",
        "print(\"-\" * 60)\n",
        "v1 = {\"Bleu_1\": 0.4471, \"Bleu_4\": 0.1016, \"ROUGE_L\": 0.3856, \"CIDEr\": 0.3694}\n",
        "rc = {\"Bleu_1\": 0.6759, \"Bleu_4\": 0.3040, \"ROUGE_L\": 0.5296, \"CIDEr\": 0.8917}\n",
        "print(f\"{'Metric':<10} {'v2 (now)':>10} {'v1 (old)':>10} {'RSICD':>10} {'v2-v1':>10}\")\n",
        "print(\"-\" * 52)\n",
        "for m in [\"Bleu_1\", \"Bleu_4\", \"ROUGE_L\", \"CIDEr\"]:\n",
        "    print(f\"{m:<10} {best['metrics'].get(m,0):>10.4f} {v1[m]:>10.4f} {rc[m]:>10.4f} {best['metrics'].get(m,0)-v1[m]:>+10.4f}\")\n",
        "\n",
        "# Save\n",
        "results_path = f\"{CHECKPOINT_DIR}/eval_all_checkpoints.json\"\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump({\"best\": {\"ckpt\": best[\"ckpt\"], \"epoch\": best[\"epoch\"], \"metrics\": best[\"metrics\"]}, \"all\": all_results}, f, indent=2)\n",
        "print(f\"\\n✓ Saved to: {results_path}\")\n",
        "print(\"\\n✅ DONE!\")"
      ],
      "metadata": {
        "id": "evaluate_model",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5c30437ba4294798bab8b4727a800d80",
            "c83c2b7751924e6b889c5341f190b948",
            "916e0bd964da4dbd906d22262e917816",
            "abda45a72adc421891b77000b3996e45",
            "713118603e354f08a59c885fd834f384",
            "78ff022aabf24d89ad82a3060f0dab2e",
            "3adc900e287a40e9868c3e21b5881369",
            "46d25ba7299b4276b2197dbaf6fc61f4",
            "34d1989a8e374687b6a992473ffd4f43",
            "308f9d3c13cc410e9c96c1a5f7d0468a",
            "cd6e859c3057445d80aa925bbcf8be26"
          ]
        },
        "outputId": "88b1e7d0-a663-4305-fb8a-f4a0c0b0b756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "RSITMD v2 - Evaluate ALL Checkpoints\n",
            "============================================================\n",
            "\n",
            "[1] Finding checkpoints...\n",
            "  Found 21 checkpoints:\n",
            "    - checkpoint_10.pth\n",
            "    - checkpoint_11.pth\n",
            "    - checkpoint_12.pth\n",
            "    - checkpoint_13.pth\n",
            "    - checkpoint_14.pth\n",
            "    - checkpoint_15.pth\n",
            "    - checkpoint_16.pth\n",
            "    - checkpoint_17.pth\n",
            "    - checkpoint_18.pth\n",
            "    - checkpoint_19.pth\n",
            "    - checkpoint_20.pth\n",
            "    - checkpoint_21.pth\n",
            "    - checkpoint_22.pth\n",
            "    - checkpoint_23.pth\n",
            "    - checkpoint_24.pth\n",
            "    - checkpoint_25.pth\n",
            "    - checkpoint_26.pth\n",
            "    - checkpoint_27.pth\n",
            "    - checkpoint_28.pth\n",
            "    - checkpoint_29.pth\n",
            "    - checkpoint_9.pth\n",
            "\n",
            "[2] Loading test data...\n",
            "  ✓ 712 test images\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "  ✓ COCO GT loaded\n",
            "\n",
            "[3] Loading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading VIT\n",
            "Loading VIT Done\n",
            "Loading Q-Former\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Q-Former Done\n",
            "Loading LLAMA\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c30437ba4294798bab8b4727a800d80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LLAMA Done\n",
            "  📥 Loading pretrained weights from: https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna13b_trimmed.pth\n",
            "  ✓ Model ready\n",
            "\n",
            "============================================================\n",
            "[1/21] checkpoint_10.pth\n",
            "============================================================\n",
            "    1/712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1283: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6121, 'reflen': 6492, 'guess': [6121, 5409, 4697, 3985], 'correct': [2767, 878, 279, 80]}\n",
            "ratio: 0.9428527418359607\n",
            "\n",
            "  Epoch 10: B1=0.4255  B4=0.0910  R=0.3102  CIDEr=0.3021  (9 words)\n",
            "  >>> ★ NEW BEST!\n",
            "    Gen: There is a large tank in the middle of the field.\n",
            "    GT:  There is a lot of grass on the ground.\n",
            "    Gen: There is a black line in the middle of the field.\n",
            "    GT:  Do you have a bicycle print?\n",
            "\n",
            "============================================================\n",
            "[2/21] checkpoint_11.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6075, 'reflen': 6465, 'guess': [6075, 5363, 4651, 3939], 'correct': [2796, 927, 306, 91]}\n",
            "ratio: 0.9396751740137756\n",
            "\n",
            "  Epoch 11: B1=0.4316  B4=0.0983  R=0.3122  CIDEr=0.3178  (9 words)\n",
            "  >>> ★ NEW BEST!\n",
            "\n",
            "============================================================\n",
            "[3/21] checkpoint_12.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 5982, 'reflen': 6404, 'guess': [5982, 5270, 4558, 3846], 'correct': [2789, 955, 325, 95]}\n",
            "ratio: 0.9341036851966061\n",
            "\n",
            "  Epoch 12: B1=0.4345  B4=0.1029  R=0.3177  CIDEr=0.3548  (8 words)\n",
            "  >>> ★ NEW BEST!\n",
            "\n",
            "============================================================\n",
            "[4/21] checkpoint_13.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6209, 'reflen': 6483, 'guess': [6209, 5497, 4785, 4073], 'correct': [2882, 996, 346, 109]}\n",
            "ratio: 0.9577356162269076\n",
            "\n",
            "  Epoch 13: B1=0.4441  B4=0.1081  R=0.3183  CIDEr=0.3462  (9 words)\n",
            "\n",
            "============================================================\n",
            "[5/21] checkpoint_14.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 5998, 'reflen': 6409, 'guess': [5998, 5287, 4576, 3865], 'correct': [2812, 987, 344, 112]}\n",
            "ratio: 0.9358714308002908\n",
            "\n",
            "  Epoch 14: B1=0.4378  B4=0.1097  R=0.3148  CIDEr=0.3559  (8 words)\n",
            "  >>> ★ NEW BEST!\n",
            "\n",
            "============================================================\n",
            "[6/21] checkpoint_15.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6131, 'reflen': 6460, 'guess': [6131, 5419, 4707, 3995], 'correct': [2823, 958, 351, 119]}\n",
            "ratio: 0.9490712074301936\n",
            "\n",
            "  Epoch 15: B1=0.4364  B4=0.1099  R=0.3173  CIDEr=0.3735  (9 words)\n",
            "  >>> ★ NEW BEST!\n",
            "\n",
            "============================================================\n",
            "[7/21] checkpoint_16.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6332, 'reflen': 6575, 'guess': [6332, 5620, 4908, 4196], 'correct': [2972, 1052, 383, 131]}\n",
            "ratio: 0.9630418250949105\n",
            "\n",
            "  Epoch 16: B1=0.4517  B4=0.1164  R=0.3237  CIDEr=0.3791  (9 words)\n",
            "  >>> ★ NEW BEST!\n",
            "\n",
            "============================================================\n",
            "[8/21] checkpoint_17.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 5985, 'reflen': 6418, 'guess': [5985, 5273, 4561, 3849], 'correct': [2886, 1032, 367, 116]}\n",
            "ratio: 0.9325334995324193\n",
            "\n",
            "  Epoch 17: B1=0.4486  B4=0.1144  R=0.3290  CIDEr=0.3955  (8 words)\n",
            "  >>> ★ NEW BEST!\n",
            "\n",
            "============================================================\n",
            "[9/21] checkpoint_18.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 5935, 'reflen': 6389, 'guess': [5935, 5223, 4511, 3799], 'correct': [2824, 993, 363, 127]}\n",
            "ratio: 0.9289403662543545\n",
            "\n",
            "  Epoch 18: B1=0.4408  B4=0.1157  R=0.3235  CIDEr=0.4008  (8 words)\n",
            "  >>> ★ NEW BEST!\n",
            "\n",
            "============================================================\n",
            "[10/21] checkpoint_19.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6185, 'reflen': 6523, 'guess': [6185, 5473, 4761, 4049], 'correct': [2919, 1003, 344, 113]}\n",
            "ratio: 0.948183351218619\n",
            "\n",
            "  Epoch 19: B1=0.4468  B4=0.1088  R=0.3191  CIDEr=0.3779  (9 words)\n",
            "\n",
            "============================================================\n",
            "[11/21] checkpoint_20.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6233, 'reflen': 6542, 'guess': [6233, 5521, 4809, 4097], 'correct': [2974, 1049, 383, 131]}\n",
            "ratio: 0.9527667380004657\n",
            "\n",
            "  Epoch 20: B1=0.4541  B4=0.1173  R=0.3276  CIDEr=0.4056  (9 words)\n",
            "  >>> ★ NEW BEST!\n",
            "\n",
            "============================================================\n",
            "[12/21] checkpoint_21.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6230, 'reflen': 6531, 'guess': [6230, 5518, 4806, 4094], 'correct': [2970, 1047, 382, 138]}\n",
            "ratio: 0.9539121114682354\n",
            "\n",
            "  Epoch 21: B1=0.4542  B4=0.1189  R=0.3244  CIDEr=0.4030  (9 words)\n",
            "\n",
            "============================================================\n",
            "[13/21] checkpoint_22.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6204, 'reflen': 6515, 'guess': [6204, 5492, 4780, 4068], 'correct': [2968, 1070, 385, 126]}\n",
            "ratio: 0.9522640061395314\n",
            "\n",
            "  Epoch 22: B1=0.4550  B4=0.1174  R=0.3268  CIDEr=0.3891  (9 words)\n",
            "\n",
            "============================================================\n",
            "[14/21] checkpoint_23.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6270, 'reflen': 6522, 'guess': [6270, 5558, 4846, 4134], 'correct': [2974, 1047, 371, 124]}\n",
            "ratio: 0.961361545538031\n",
            "\n",
            "  Epoch 23: B1=0.4556  B4=0.1150  R=0.3247  CIDEr=0.3974  (9 words)\n",
            "\n",
            "============================================================\n",
            "[15/21] checkpoint_24.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6191, 'reflen': 6506, 'guess': [6191, 5479, 4767, 4055], 'correct': [2958, 1069, 401, 147]}\n",
            "ratio: 0.9515831540115351\n",
            "\n",
            "  Epoch 24: B1=0.4541  B4=0.1234  R=0.3271  CIDEr=0.4090  (9 words)\n",
            "  >>> ★ NEW BEST!\n",
            "\n",
            "============================================================\n",
            "[16/21] checkpoint_25.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6263, 'reflen': 6533, 'guess': [6263, 5551, 4839, 4127], 'correct': [2980, 1068, 401, 141]}\n",
            "ratio: 0.9586713607835666\n",
            "\n",
            "  Epoch 25: B1=0.4557  B4=0.1215  R=0.3266  CIDEr=0.4035  (9 words)\n",
            "\n",
            "============================================================\n",
            "[17/21] checkpoint_26.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6128, 'reflen': 6475, 'guess': [6128, 5416, 4704, 3992], 'correct': [2923, 1038, 385, 134]}\n",
            "ratio: 0.9464092664091202\n",
            "\n",
            "  Epoch 26: B1=0.4507  B4=0.1190  R=0.3238  CIDEr=0.4053  (9 words)\n",
            "\n",
            "============================================================\n",
            "[18/21] checkpoint_27.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6200, 'reflen': 6504, 'guess': [6200, 5488, 4776, 4064], 'correct': [2952, 1042, 382, 131]}\n",
            "ratio: 0.9532595325951794\n",
            "\n",
            "  Epoch 27: B1=0.4533  B4=0.1176  R=0.3243  CIDEr=0.4012  (9 words)\n",
            "\n",
            "============================================================\n",
            "[19/21] checkpoint_28.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6172, 'reflen': 6507, 'guess': [6172, 5460, 4748, 4036], 'correct': [2922, 1028, 373, 124]}\n",
            "ratio: 0.9485169817118566\n",
            "\n",
            "  Epoch 28: B1=0.4484  B4=0.1147  R=0.3212  CIDEr=0.3956  (9 words)\n",
            "\n",
            "============================================================\n",
            "[20/21] checkpoint_29.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6199, 'reflen': 6503, 'guess': [6199, 5487, 4775, 4063], 'correct': [2953, 1045, 381, 134]}\n",
            "ratio: 0.9532523450713588\n",
            "\n",
            "  Epoch 29: B1=0.4536  B4=0.1184  R=0.3255  CIDEr=0.4079  (9 words)\n",
            "\n",
            "============================================================\n",
            "[21/21] checkpoint_9.pth\n",
            "============================================================\n",
            "    1/712\n",
            "    200/712\n",
            "    400/712\n",
            "    600/712\n",
            "{'testlen': 6044, 'reflen': 6413, 'guess': [6044, 5332, 4620, 3908], 'correct': [2587, 771, 247, 79]}\n",
            "ratio: 0.9424606268515604\n",
            "\n",
            "  Epoch 9: B1=0.4027  B4=0.0851  R=0.2915  CIDEr=0.2835  (8 words)\n",
            "\n",
            "============================================================\n",
            "SUMMARY\n",
            "============================================================\n",
            "\n",
            "Epoch      BLEU-1   BLEU-4  ROUGE-L    CIDEr\n",
            "--------------------------------------------\n",
            "  10       0.4255   0.0910   0.3102   0.3021\n",
            "  11       0.4316   0.0983   0.3122   0.3178\n",
            "  12       0.4345   0.1029   0.3177   0.3548\n",
            "  13       0.4441   0.1081   0.3183   0.3462\n",
            "  14       0.4378   0.1097   0.3148   0.3559\n",
            "  15       0.4364   0.1099   0.3173   0.3735\n",
            "  16       0.4517   0.1164   0.3237   0.3791\n",
            "  17       0.4486   0.1144   0.3290   0.3955\n",
            "  18       0.4408   0.1157   0.3235   0.4008\n",
            "  19       0.4468   0.1088   0.3191   0.3779\n",
            "  20       0.4541   0.1173   0.3276   0.4056\n",
            "  21       0.4542   0.1189   0.3244   0.4030\n",
            "  22       0.4550   0.1174   0.3268   0.3891\n",
            "  23       0.4556   0.1150   0.3247   0.3974\n",
            "  24       0.4541   0.1234   0.3271   0.4090 ★\n",
            "  25       0.4557   0.1215   0.3266   0.4035\n",
            "  26       0.4507   0.1190   0.3238   0.4053\n",
            "  27       0.4533   0.1176   0.3243   0.4012\n",
            "  28       0.4484   0.1147   0.3212   0.3956\n",
            "  29       0.4536   0.1184   0.3255   0.4079\n",
            "  9        0.4027   0.0851   0.2915   0.2835\n",
            "\n",
            "★ BEST: Epoch 24 → CIDEr = 0.4090\n",
            "  Path: /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/checkpoint_24.pth\n",
            "\n",
            "------------------------------------------------------------\n",
            "Comparison:  v2 (with RSICap)  vs  v1 (without)  vs  RSICD\n",
            "------------------------------------------------------------\n",
            "Metric       v2 (now)   v1 (old)      RSICD      v2-v1\n",
            "----------------------------------------------------\n",
            "Bleu_1         0.4541     0.4471     0.6759    +0.0070\n",
            "Bleu_4         0.1234     0.1016     0.3040    +0.0218\n",
            "ROUGE_L        0.3271     0.3856     0.5296    -0.0585\n",
            "CIDEr          0.4090     0.3694     0.8917    +0.0396\n",
            "\n",
            "✓ Saved to: /content/drive/MyDrive/outputs/rsgpt/rsitmd_finetuned_v2/20260202001/eval_all_checkpoints.json\n",
            "\n",
            "✅ DONE!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 7: COMPARE RESULTS"
      ],
      "metadata": {
        "id": "step7_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 7.1: Compare with RSICD Results\n",
        "import json\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 7.1: Compare RSITMD vs RSICD Results\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load RSITMD results\n",
        "rsitmd_results_path = f\"{OUT_ROOT}/rsitmd_finetuned/evaluation_results.json\"\n",
        "try:\n",
        "    with open(rsitmd_results_path) as f:\n",
        "        rsitmd_results = json.load(f)\n",
        "    rsitmd_metrics = rsitmd_results[\"best\"][\"metrics\"]\n",
        "    print(\"✓ Loaded RSITMD results\")\n",
        "except:\n",
        "    print(\"⚠️ RSITMD results not found. Run evaluation first.\")\n",
        "    rsitmd_metrics = None\n",
        "\n",
        "# Your RSICD results from earlier experiments\n",
        "rsicd_metrics = {\n",
        "    \"Bleu_1\": 0.6759,\n",
        "    \"Bleu_2\": 0.4948,\n",
        "    \"Bleu_3\": 0.3792,\n",
        "    \"Bleu_4\": 0.3040,\n",
        "    \"METEOR\": 0.2500,\n",
        "    \"ROUGE_L\": 0.5296,\n",
        "    \"CIDEr\": 0.8917\n",
        "}\n",
        "\n",
        "# Compare\n",
        "if rsitmd_metrics:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RESULTS COMPARISON: RSITMD vs RSICD\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\n{'Metric':<12} {'RSITMD':>12} {'RSICD':>12} {'Difference':>12}\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    for metric in [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\", \"METEOR\", \"ROUGE_L\", \"CIDEr\"]:\n",
        "        rsitmd_val = rsitmd_metrics.get(metric, 0)\n",
        "        rsicd_val = rsicd_metrics.get(metric, 0)\n",
        "        diff = rsitmd_val - rsicd_val\n",
        "\n",
        "        if diff > 0.01:\n",
        "            status = \"↑\"\n",
        "        elif diff < -0.01:\n",
        "            status = \"↓\"\n",
        "        else:\n",
        "            status = \"≈\"\n",
        "\n",
        "        print(f\"{metric:<12} {rsitmd_val:>12.4f} {rsicd_val:>12.4f} {diff:>+12.4f} {status}\")\n",
        "\n",
        "    print(\"-\"*50)\n",
        "    print(\"\\n↑ = RSITMD better | ↓ = RSICD better | ≈ = Similar\")\n",
        "else:\n",
        "    print(\"\\nRun evaluation first to see comparison.\")"
      ],
      "metadata": {
        "id": "compare_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 7.2: Summary Statistics\n",
        "print(\"=\"*60)\n",
        "print(\"RSITMD EXPERIMENT SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\"\"\n",
        "Dataset: RSITMD\n",
        "- Total images: ~4,743\n",
        "- Captions per image: 5\n",
        "- Resolution: 256×256\n",
        "\n",
        "Split:\n",
        "- Train: ~3,320 images (70%)\n",
        "- Val: ~711 images (15%)\n",
        "- Test: ~712 images (15%)\n",
        "\n",
        "Model: RSGPT (InstructBLIP + Vicuna-13B)\n",
        "\n",
        "Training (FIXED):\n",
        "- Epochs: 15\n",
        "- iters_per_epoch: ~830 (not 10000!)\n",
        "- Learning rate: 1e-5\n",
        "- Batch size: 4 (effective 16 with accumulation)\n",
        "- Estimated time: ~2-3 hours on A100\n",
        "\"\"\")\n",
        "\n",
        "# Show best results if available\n",
        "try:\n",
        "    if rsitmd_metrics:\n",
        "        print(\"Best Results:\")\n",
        "        for m, v in rsitmd_metrics.items():\n",
        "            print(f\"  {m}: {v:.4f}\")\n",
        "except:\n",
        "    print(\"Run evaluation to see results.\")"
      ],
      "metadata": {
        "id": "summary_stats"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Troubleshooting\n",
        "\n",
        "## Common Issues\n",
        "\n",
        "### 1. Import Error: `partially initialized module 'torchvision'`\n",
        "**Solution:** Restart the runtime (Runtime → Restart runtime) and run the evaluation cell directly.\n",
        "\n",
        "### 2. Training takes too long\n",
        "**Fixed:** The `iters_per_epoch` is now set based on dataset size (~830) instead of 10000.\n",
        "\n",
        "### 3. METEOR returns 0\n",
        "**Solution:** Ensure Java is installed (`!apt-get install default-jdk`) and NLTK data is downloaded.\n",
        "\n",
        "### 4. Only partial checkpoints saved\n",
        "**Reason:** Previous training with `iters_per_epoch=10000` took ~40+ minutes per epoch. With the fix, each epoch takes ~3-4 minutes.\n",
        "\n",
        "### 5. Resume from interrupted training\n",
        "Use Step 5.2 to create a resume config and continue training.\n"
      ],
      "metadata": {
        "id": "troubleshooting"
      }
    }
  ]
}