{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1PHJVpt9VkE0iNP5IoJeZqP8FV2kyVivR",
      "authorship_tag": "ABX9TyOvAHmgNzFkynZfFWoqeNdF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0fa000892daa4a38a0cfa89cce98aeec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_507eda397d5c49388612a019a4348fa0"
          }
        },
        "438888cc6bc44bedaa5e4f46f5fab008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49e76de7cb6b46219aeee84c795b709e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a9090e6d431e4579bd5c0cf027fdc695",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "85259a31b8ff4c6d9cda7ff3a79b04c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_fca85553d2c140e5a24438b6a4786998",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_da7a3c03737848888a2d557cdb015b48",
            "value": ""
          }
        },
        "5f93e0e4bbf44e438c4f1b3cb4b76046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_bdd89f780a4d4e9f8ff02ddba34f30bd",
            "style": "IPY_MODEL_fadecaba8368484c9cd4fcb4fb6a354a",
            "value": false
          }
        },
        "23c2834d011c4c8194ff5a6e1da429e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d68e172d0a22401fbb479348c79b8046",
            "style": "IPY_MODEL_50529f1762054581b4a88349514b4a7a",
            "tooltip": ""
          }
        },
        "d4ca93f82aa24601b535d23245586f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75caee0c1eb2468c85582ddbe75d8b7a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9f398d7179a24dacb44439aa59255b26",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "507eda397d5c49388612a019a4348fa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "49e76de7cb6b46219aeee84c795b709e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9090e6d431e4579bd5c0cf027fdc695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fca85553d2c140e5a24438b6a4786998": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da7a3c03737848888a2d557cdb015b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdd89f780a4d4e9f8ff02ddba34f30bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fadecaba8368484c9cd4fcb4fb6a354a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d68e172d0a22401fbb479348c79b8046": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50529f1762054581b4a88349514b4a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "75caee0c1eb2468c85582ddbe75d8b7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f398d7179a24dacb44439aa59255b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2de3fc0961e340afad5df33265a4c930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6326ea9c52474452b056db3716b63283",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_03f8d2cf41f44188aadde3d0880156f6",
            "value": "Connecting..."
          }
        },
        "6326ea9c52474452b056db3716b63283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03f8d2cf41f44188aadde3d0880156f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/billvo2212/reproducing-research-paper-result/blob/main/RSICD_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72,
          "referenced_widgets": [
            "0fa000892daa4a38a0cfa89cce98aeec",
            "438888cc6bc44bedaa5e4f46f5fab008",
            "85259a31b8ff4c6d9cda7ff3a79b04c0",
            "5f93e0e4bbf44e438c4f1b3cb4b76046",
            "23c2834d011c4c8194ff5a6e1da429e9",
            "d4ca93f82aa24601b535d23245586f00",
            "507eda397d5c49388612a019a4348fa0",
            "49e76de7cb6b46219aeee84c795b709e",
            "a9090e6d431e4579bd5c0cf027fdc695",
            "fca85553d2c140e5a24438b6a4786998",
            "da7a3c03737848888a2d557cdb015b48",
            "bdd89f780a4d4e9f8ff02ddba34f30bd",
            "fadecaba8368484c9cd4fcb4fb6a354a",
            "d68e172d0a22401fbb479348c79b8046",
            "50529f1762054581b4a88349514b4a7a",
            "75caee0c1eb2468c85582ddbe75d8b7a",
            "9f398d7179a24dacb44439aa59255b26",
            "2de3fc0961e340afad5df33265a4c930",
            "6326ea9c52474452b056db3716b63283",
            "03f8d2cf41f44188aadde3d0880156f6"
          ]
        },
        "id": "J_4-qKnExETo",
        "outputId": "2411d17b-edd6-4480-9a85-544a1fa33f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fa000892daa4a38a0cfa89cce98aeec"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Core stack (CUDA 12.1 wheels work well on Colab A100/L4/T4)\n",
        "!pip -q install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install transformers==4.41.2 accelerate==0.31.0 peft==0.11.1 bitsandbytes==0.43.1 sentencepiece\n",
        "!pip -q install decord==0.6.0 datasets==2.20.0 rouge-score==0.1.2 pycocoevalcap\n",
        "\n",
        "# Java (if you want METEOR/SPICE original implementations)\n",
        "!apt-get -yqq update && apt-get -yqq install openjdk-17-jre-headless\n",
        "\n",
        "# Install additional evaluation dependencies\n",
        "!pip install -q nltk\n",
        "\n",
        "# Install iopath and other potentially missing packages\n",
        "!pip install -q iopath omegaconf timm einops\n",
        "\n",
        "# Install additional packages that RSGPT might need\n",
        "!pip install -q webdataset braceexpand\n",
        "\n",
        "!apt-get update -qq\n",
        "!apt-get install -y default-jdk -qq\n",
        "\n",
        "import os\n",
        "os.environ['_JAVA_OPTIONS'] = '-Xmx8g'\n",
        "\n",
        "# Login to Hugging Face if your base LLM requires it\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()  # or set HF_TOKEN env var"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/data/RSGPT\"                      # repo path\n",
        "DATA_ROOT    = \"/content/drive/MyDrive/data/rsgpt\"   # place your datasets here\n",
        "OUT_ROOT     = \"/content/drive/MyDrive/outputs/rsgpt\"\n",
        "\n",
        "!mkdir -p {DATA_ROOT}/rsicap/images\n",
        "!mkdir -p {DATA_ROOT}/rsicap\n",
        "!mkdir -p {DATA_ROOT}/rsieval/images\n",
        "!mkdir -p {DATA_ROOT}/rsieval\n",
        "!mkdir -p {OUT_ROOT}\n",
        "\n",
        "print(f\"âœ“ Directories created:\")\n",
        "print(f\"  Project: {PROJECT_ROOT}\")\n",
        "print(f\"  Data: {DATA_ROOT}\")\n",
        "print(f\"  Output: {OUT_ROOT}\")\n",
        "\n",
        "print(\"\\nðŸ“ Expected data structure:\")\n",
        "print(f\"\"\"\n",
        "{DATA_ROOT}/\n",
        "  â”œâ”€â”€ rsicap/\n",
        "  â”‚   â”œâ”€â”€ images/          # Training images\n",
        "  â”‚   â””â”€â”€ annotations.json # Training annotations\n",
        "  â””â”€â”€ rsieval/\n",
        "      â”œâ”€â”€ images/          # Evaluation images\n",
        "      â”œâ”€â”€ captions.json    # Caption evaluation\n",
        "      â””â”€â”€ vqa.json         # VQA evaluation (if applicable)\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtvJEYR6yIR8",
        "outputId": "aaa94821-6228-44d0-d2bd-7d54eea661fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ“ Directories created:\n",
            "  Project: /content/drive/MyDrive/data/RSGPT\n",
            "  Data: /content/drive/MyDrive/data/rsgpt\n",
            "  Output: /content/drive/MyDrive/outputs/rsgpt\n",
            "\n",
            "ðŸ“ Expected data structure:\n",
            "\n",
            "/content/drive/MyDrive/data/rsgpt/\n",
            "  â”œâ”€â”€ rsicap/\n",
            "  â”‚   â”œâ”€â”€ images/          # Training images\n",
            "  â”‚   â””â”€â”€ annotations.json # Training annotations\n",
            "  â””â”€â”€ rsieval/\n",
            "      â”œâ”€â”€ images/          # Evaluation images\n",
            "      â”œâ”€â”€ captions.json    # Caption evaluation\n",
            "      â””â”€â”€ vqa.json         # VQA evaluation (if applicable)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Helper to print what the repo expects for model_type and datasets\n",
        "import re\n",
        "import pathlib\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "root = pathlib.Path(f\"{PROJECT_ROOT}/rsgpt\")\n",
        "\n",
        "# Find valid model types\n",
        "valid_models = []\n",
        "for p in root.rglob(\"*.py\"):\n",
        "    try:\n",
        "        t = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "        if \"PRETRAINED_MODEL_CONFIG_DICT\" in t:\n",
        "            m = re.search(r\"PRETRAINED_MODEL_CONFIG_DICT\\s*=\\s*({.*?})\", t, re.S)\n",
        "            if m:\n",
        "                keys = re.findall(r\"['\\\"]([^'\\\"]+)['\\\"]\\s*:\", m.group(1))\n",
        "                valid_models.extend(keys)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Find valid dataset builders\n",
        "valid_datasets = []\n",
        "for p in root.rglob(\"*.py\"):\n",
        "    try:\n",
        "        t = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "        found = re.findall(r'register_builder\\(\\s*[\\'\"]([^\\'\"]+)[\\'\"]\\s*\\)', t)\n",
        "        valid_datasets.extend(found)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"ðŸ“‹ Available model types:\")\n",
        "for i, model in enumerate(sorted(set(valid_models)), 1):\n",
        "    print(f\"  {i}. {model}\")\n",
        "\n",
        "print(\"\\nðŸ“‹ Available dataset builders:\")\n",
        "for i, dataset in enumerate(sorted(set(valid_datasets))[:20], 1):\n",
        "    print(f\"  {i}. {dataset}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Choose your model_type from the list above\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40hUihmFyNCa",
        "outputId": "02fa56fd-5498-425e-a8d4-c2b7349e3f80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“‹ Available model types:\n",
            "  1. vicuna13b\n",
            "  2. vicuna7b\n",
            "\n",
            "ðŸ“‹ Available dataset builders:\n",
            "  1. cc_sbu_align\n",
            "  2. rsicap_instruction\n",
            "  3. rsicd\n",
            "  4. rsicd_instruction\n",
            "  5. rsvqahr_instruction\n",
            "  6. rsvqalr_instruction\n",
            "  7. sydney_instruction\n",
            "  8. ucm_instruction\n",
            "\n",
            "ðŸ’¡ Choose your model_type from the list above\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1: DOWNLOAD RSICD DATASET"
      ],
      "metadata": {
        "id": "dX_arccPyUhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.1: Download RSICD Dataset\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 2.1: Download RSICD Dataset\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create directory\n",
        "rsicd_base = \"/content/drive/MyDrive/data/RSGPT/dataset/rsicd\"\n",
        "os.makedirs(rsicd_base, exist_ok=True)\n",
        "\n",
        "# Clone RSICD repository\n",
        "%cd /content\n",
        "!rm -rf RSICD_optimal  # Remove if exists\n",
        "!git clone https://github.com/201528014227051/RSICD_optimal.git\n",
        "\n",
        "# Check contents\n",
        "print(\"\\nâœ“ Downloaded RSICD repository\")\n",
        "print(\"\\nContents:\")\n",
        "!ls -la /content/RSICD_optimal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTBsGvOPyYTk",
        "outputId": "60cb3a40-465f-44b5-dfbe-f7e205b4e11a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 2.1: Download RSICD Dataset\n",
            "============================================================\n",
            "/content\n",
            "Cloning into 'RSICD_optimal'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 65 (delta 4), reused 0 (delta 0), pack-reused 56 (from 1)\u001b[K\n",
            "Receiving objects: 100% (65/65), 1.37 MiB | 15.47 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n",
            "Downloading RSICD_images.zip (485 MB)\n",
            "Error downloading object: RSICD_images.zip (0d3ebda): Smudge error: Error downloading RSICD_images.zip (0d3ebda15b8b1e5c2754270d38462251e8c61489b54e463e499c978da4b91d10): batch response: This repository exceeded its LFS budget. The account responsible for the budget should increase it to restore access.\n",
            "\n",
            "Errors logged to /content/RSICD_optimal/.git/lfs/logs/20251230T161536.842484838.log\n",
            "Use `git lfs logs last` to view the log.\n",
            "error: external filter 'git-lfs filter-process' failed\n",
            "fatal: RSICD_images.zip: smudge filter lfs failed\n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry with 'git restore --source=HEAD :/'\n",
            "\n",
            "\n",
            "âœ“ Downloaded RSICD repository\n",
            "\n",
            "Contents:\n",
            "total 12532\n",
            "drwxr-xr-x 3 root root     4096 Dec 30 16:15 .\n",
            "drwxr-xr-x 1 root root     4096 Dec 30 16:15 ..\n",
            "-rw-r--r-- 1 root root 12470775 Dec 30 16:15 dataset_rsicd.json\n",
            "-rw-r--r-- 1 root root   312694 Dec 30 16:15 example.PNG\n",
            "drwxr-xr-x 9 root root     4096 Dec 30 16:15 .git\n",
            "-rw-r--r-- 1 root root       84 Dec 30 16:15 .gitattributes\n",
            "-rw-r--r-- 1 root root     2378 Dec 30 16:15 README.md\n",
            "-rw-r--r-- 1 root root      252 Dec 30 16:15 readme.txt\n",
            "-rw-r--r-- 1 root root    18942 Dec 30 16:15 txtclasses_rsicd.rar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2: PREPARE RSICD DATA"
      ],
      "metadata": {
        "id": "egCpZIxvyk-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Check RSICD Annotation Format\n",
        "import json\n",
        "import os\n",
        "\n",
        "src_path = \"/content/RSICD_optimal\"\n",
        "\n",
        "# Find JSON files\n",
        "print(\"JSON files in RSICD_optimal:\")\n",
        "for f in os.listdir(src_path):\n",
        "    if f.endswith('.json'):\n",
        "        print(f\"  - {f}\")\n",
        "\n",
        "# Load and inspect\n",
        "ann_file = None\n",
        "for f in [\"dataset_rsicd.json\", \"rsicd.json\", \"annotations.json\"]:\n",
        "    if os.path.exists(f\"{src_path}/{f}\"):\n",
        "        ann_file = f\"{src_path}/{f}\"\n",
        "        break\n",
        "\n",
        "if ann_file:\n",
        "    with open(ann_file) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"\\nLoaded: {ann_file}\")\n",
        "    print(f\"Type: {type(data)}\")\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        print(f\"Keys: {data.keys()}\")\n",
        "\n",
        "        # Check images structure\n",
        "        if \"images\" in data:\n",
        "            print(f\"\\nImages count: {len(data['images'])}\")\n",
        "            print(f\"Sample image entry:\")\n",
        "            print(json.dumps(data['images'][0], indent=2))\n",
        "\n",
        "        # Check annotations structure\n",
        "        if \"annotations\" in data:\n",
        "            print(f\"\\nAnnotations count: {len(data['annotations'])}\")\n",
        "            print(f\"Sample annotation entry:\")\n",
        "            print(json.dumps(data['annotations'][0], indent=2))\n",
        "\n",
        "    elif isinstance(data, list):\n",
        "        print(f\"List length: {len(data)}\")\n",
        "        print(f\"Sample entry:\")\n",
        "        print(json.dumps(data[0], indent=2))\n",
        "else:\n",
        "    print(\"No annotation file found!\")\n",
        "    print(f\"\\nContents of {src_path}:\")\n",
        "    !ls -la {src_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "id": "Y2AIwj37260T",
        "outputId": "ce2f0157-e7f6-41e8-d9b1-282d4fe9cffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON files in RSICD_optimal:\n",
            "  - dataset_rsicd.json\n",
            "\n",
            "Loaded: /content/RSICD_optimal/dataset_rsicd.json\n",
            "Type: <class 'dict'>\n",
            "Keys: dict_keys(['images', 'dataset'])\n",
            "\n",
            "Images count: 10921\n",
            "Sample image entry:\n",
            "{\n",
            "  \"filename\": \"airport_1.jpg\",\n",
            "  \"imgid\": 0,\n",
            "  \"sentences\": [\n",
            "    {\n",
            "      \"tokens\": [\n",
            "        \"many\",\n",
            "        \"planes\",\n",
            "        \"are\",\n",
            "        \"parked\",\n",
            "        \"next\",\n",
            "        \"to\",\n",
            "        \"a\",\n",
            "        \"long\",\n",
            "        \"building\",\n",
            "        \"in\",\n",
            "        \"an\",\n",
            "        \"airport\"\n",
            "      ],\n",
            "      \"raw\": \"many planes are parked next to a long building in an airport .\",\n",
            "      \"imgid\": 0,\n",
            "      \"sentid\": 0\n",
            "    },\n",
            "    {\n",
            "      \"tokens\": [\n",
            "        \"many\",\n",
            "        \"planes\",\n",
            "        \"are\",\n",
            "        \"parked\",\n",
            "        \"next\",\n",
            "        \"to\",\n",
            "        \"a\",\n",
            "        \"long\",\n",
            "        \"building\",\n",
            "        \"in\",\n",
            "        \"an\",\n",
            "        \"airport\"\n",
            "      ],\n",
            "      \"raw\": \"many planes are parked next to a long building in an airport .\",\n",
            "      \"imgid\": 0,\n",
            "      \"sentid\": 1\n",
            "    },\n",
            "    {\n",
            "      \"tokens\": [\n",
            "        \"many\",\n",
            "        \"planes\",\n",
            "        \"are\",\n",
            "        \"parked\",\n",
            "        \"next\",\n",
            "        \"to\",\n",
            "        \"a\",\n",
            "        \"long\",\n",
            "        \"building\",\n",
            "        \"in\",\n",
            "        \"an\",\n",
            "        \"airport\"\n",
            "      ],\n",
            "      \"raw\": \"many planes are parked next to a long building in an airport .\",\n",
            "      \"imgid\": 0,\n",
            "      \"sentid\": 2\n",
            "    },\n",
            "    {\n",
            "      \"tokens\": [\n",
            "        \"many\",\n",
            "        \"planes\",\n",
            "        \"are\",\n",
            "        \"parked\",\n",
            "        \"next\",\n",
            "        \"to\",\n",
            "        \"a\",\n",
            "        \"long\",\n",
            "        \"building\",\n",
            "        \"in\",\n",
            "        \"an\",\n",
            "        \"airport\"\n",
            "      ],\n",
            "      \"raw\": \"many planes are parked next to a long building in an airport .\",\n",
            "      \"imgid\": 0,\n",
            "      \"sentid\": 3\n",
            "    },\n",
            "    {\n",
            "      \"tokens\": [\n",
            "        \"many\",\n",
            "        \"planes\",\n",
            "        \"are\",\n",
            "        \"parked\",\n",
            "        \"next\",\n",
            "        \"to\",\n",
            "        \"a\",\n",
            "        \"long\",\n",
            "        \"building\",\n",
            "        \"in\",\n",
            "        \"an\",\n",
            "        \"airport\"\n",
            "      ],\n",
            "      \"raw\": \"many planes are parked next to a long building in an airport .\",\n",
            "      \"imgid\": 0,\n",
            "      \"sentid\": 4\n",
            "    }\n",
            "  ],\n",
            "  \"split\": \"train\",\n",
            "  \"sentids\": [\n",
            "    0,\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    4\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.2: Prepare RSICD Data (Fixed for RSICD Format)\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 2.2: Prepare RSICD Data\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "src_path = \"/content/RSICD_optimal\"\n",
        "dst_path = \"/content/drive/MyDrive/data/RSGPT/dataset/rsicd\"\n",
        "\n",
        "# os.makedirs(f\"{dst_path}/image\", exist_ok=True)\n",
        "\n",
        "# ============================================\n",
        "# [1] Copy images\n",
        "# ============================================\n",
        "# print(\"\\n[1] Copying images...\")\n",
        "\n",
        "# img_src = None\n",
        "# for folder in [\"RSICD_images\", \"images\", \"RSICD\"]:\n",
        "#     check_path = f\"{dst_path}/{folder}\"\n",
        "#     if os.path.exists(check_path):\n",
        "#         files = os.listdir(check_path)\n",
        "#         img_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg', '.tif'))]\n",
        "#         if len(img_files) > 100:\n",
        "#             img_src = check_path\n",
        "#             break\n",
        "\n",
        "# if img_src:\n",
        "#     img_files = [f for f in os.listdir(img_src) if f.lower().endswith(('.jpg', '.png', '.jpeg', '.tif'))]\n",
        "#     print(f\"  Found {len(img_files)} images in {img_src}\")\n",
        "#     for i, f in enumerate(img_files):\n",
        "#         shutil.copy(f\"{img_src}/{f}\", f\"{dst_path}/image/{f}\")\n",
        "#         if (i+1) % 2000 == 0:\n",
        "#             print(f\"  Copied {i+1}/{len(img_files)}\")\n",
        "#     print(f\"  âœ“ Copied {len(img_files)} images\")\n",
        "# else:\n",
        "#     print(\"  âš ï¸ Images folder not found, searching...\")\n",
        "#     !find {src_path} -name \"*.jpg\" | head -5\n",
        "\n",
        "# ============================================\n",
        "# [2] Load annotations\n",
        "# ============================================\n",
        "print(\"\\n[2] Loading annotations...\")\n",
        "\n",
        "with open(f\"{src_path}/dataset_rsicd.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "images_data = data[\"images\"]\n",
        "print(f\"  âœ“ Loaded {len(images_data)} images\")\n",
        "\n",
        "# ============================================\n",
        "# [3] Parse RSICD format\n",
        "# ============================================\n",
        "print(\"\\n[3] Parsing RSICD format...\")\n",
        "\n",
        "# RSICD format:\n",
        "# {\n",
        "#   \"filename\": \"airport_1.jpg\",\n",
        "#   \"imgid\": 0,\n",
        "#   \"sentences\": [{\"raw\": \"caption text\", ...}, ...],\n",
        "#   \"split\": \"train\" / \"val\" / \"test\"\n",
        "# }\n",
        "\n",
        "prompts = [\n",
        "    \"Describe this image in detail.\",\n",
        "    \"Please provide a detailed description of the picture.\",\n",
        "    \"Could you describe the contents of this image for me?\",\n",
        "    \"Write a short description for the image.\",\n",
        "    \"Take a look at this image and describe what you notice.\",\n",
        "]\n",
        "\n",
        "# Use RSICD's original splits\n",
        "splits = {\"train\": [], \"val\": [], \"test\": []}\n",
        "\n",
        "for img in images_data:\n",
        "    img_id = img[\"imgid\"]\n",
        "    filename = img[\"filename\"]\n",
        "    split = img[\"split\"]\n",
        "\n",
        "    # Extract captions from sentences\n",
        "    captions = [sent[\"raw\"] for sent in img[\"sentences\"]]\n",
        "\n",
        "    annotation = {\n",
        "        \"image_id\": img_id,\n",
        "        \"filename\": filename,\n",
        "        \"text_input\": random.choice(prompts),\n",
        "        \"text_output\": captions  # List of all captions\n",
        "    }\n",
        "\n",
        "    if split in splits:\n",
        "        splits[split].append(annotation)\n",
        "    else:\n",
        "        # Handle unexpected split names\n",
        "        splits[\"train\"].append(annotation)\n",
        "\n",
        "print(f\"  Train: {len(splits['train'])} images\")\n",
        "print(f\"  Val: {len(splits['val'])} images\")\n",
        "print(f\"  Test: {len(splits['test'])} images\")\n",
        "\n",
        "# Show sample\n",
        "sample = splits[\"train\"][0]\n",
        "print(f\"\\n  Sample annotation:\")\n",
        "print(f\"    image_id: {sample['image_id']}\")\n",
        "print(f\"    filename: {sample['filename']}\")\n",
        "print(f\"    text_input: {sample['text_input']}\")\n",
        "print(f\"    text_output: {len(sample['text_output'])} captions\")\n",
        "print(f\"    caption[0]: {sample['text_output'][0]}\")\n",
        "\n",
        "# ============================================\n",
        "# [4] Save annotation files\n",
        "# ============================================\n",
        "print(\"\\n[4] Saving annotation files...\")\n",
        "\n",
        "for split_name, annotations in splits.items():\n",
        "    out_file = f\"{dst_path}/rsicd_cap_processed_instruction_{split_name}.json\"\n",
        "    with open(out_file, \"w\") as f:\n",
        "        json.dump({\"annotations\": annotations}, f, indent=2)\n",
        "    print(f\"  âœ“ {split_name}: {len(annotations)} images -> {out_file}\")\n",
        "\n",
        "# ============================================\n",
        "# [5] Create COCO GT for evaluation\n",
        "# ============================================\n",
        "print(\"\\n[5] Creating COCO GT files...\")\n",
        "\n",
        "for split_name in [\"val\", \"test\"]:\n",
        "    annotations_list = splits[split_name]\n",
        "\n",
        "    images = []\n",
        "    coco_annotations = []\n",
        "    ann_id = 1\n",
        "\n",
        "    for ann in annotations_list:\n",
        "        img_id = ann[\"image_id\"]\n",
        "        filename = ann[\"filename\"]\n",
        "        captions = ann[\"text_output\"]\n",
        "\n",
        "        images.append({\"id\": img_id, \"file_name\": filename})\n",
        "\n",
        "        for cap in captions:\n",
        "            coco_annotations.append({\n",
        "                \"id\": ann_id,\n",
        "                \"image_id\": img_id,\n",
        "                \"caption\": cap\n",
        "            })\n",
        "            ann_id += 1\n",
        "\n",
        "    coco_gt = {\n",
        "        \"info\": {\"description\": f\"RSICD {split_name}\", \"version\": \"1.0\", \"year\": 2024},\n",
        "        \"licenses\": [],\n",
        "        \"type\": \"captions\",\n",
        "        \"images\": images,\n",
        "        \"annotations\": coco_annotations\n",
        "    }\n",
        "\n",
        "    gt_file = f\"{dst_path}/rsicd_{split_name}_gt.json\"\n",
        "    with open(gt_file, \"w\") as f:\n",
        "        json.dump(coco_gt, f)\n",
        "    print(f\"  âœ“ {split_name} GT: {len(images)} images, {len(coco_annotations)} captions\")\n",
        "\n",
        "# ============================================\n",
        "# [6] Verify final structure\n",
        "# ============================================\n",
        "print(\"\\n[6] Verifying final structure...\")\n",
        "\n",
        "print(f\"\\n  Contents of {dst_path}:\")\n",
        "for item in sorted(os.listdir(dst_path)):\n",
        "    item_path = f\"{dst_path}/{item}\"\n",
        "    if os.path.isdir(item_path):\n",
        "        count = len(os.listdir(item_path))\n",
        "        print(f\"    ðŸ“ {item}/ ({count} files)\")\n",
        "    else:\n",
        "        size = os.path.getsize(item_path) / 1024\n",
        "        print(f\"    ðŸ“„ {item} ({size:.1f} KB)\")\n",
        "\n",
        "# Verify image exists\n",
        "sample_filename = splits[\"train\"][0][\"filename\"]\n",
        "img_path = f\"{dst_path}/image/{sample_filename}\"\n",
        "print(f\"\\n  Sample image exists: {os.path.exists(img_path)} ({img_path})\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Step 2.2 Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\"\"\n",
        "Final structure:\n",
        "{dst_path}/\n",
        "â”œâ”€â”€ image/                                         â† Images\n",
        "â”œâ”€â”€ rsicd_cap_processed_instruction_train.json     â† Train ({len(splits['train'])} images)\n",
        "â”œâ”€â”€ rsicd_cap_processed_instruction_val.json       â† Val ({len(splits['val'])} images)\n",
        "â”œâ”€â”€ rsicd_cap_processed_instruction_test.json      â† Test ({len(splits['test'])} images)\n",
        "â”œâ”€â”€ rsicd_val_gt.json                              â† COCO GT for val\n",
        "â””â”€â”€ rsicd_test_gt.json                             â† COCO GT for test\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXnr_GzxykRt",
        "outputId": "14f40fb9-9190-4fc9-d542-c7539de101f1",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 2.2: Prepare RSICD Data\n",
            "============================================================\n",
            "\n",
            "[2] Loading annotations...\n",
            "  âœ“ Loaded 10921 images\n",
            "\n",
            "[3] Parsing RSICD format...\n",
            "  Train: 8734 images\n",
            "  Val: 1094 images\n",
            "  Test: 1093 images\n",
            "\n",
            "  Sample annotation:\n",
            "    image_id: 0\n",
            "    filename: airport_1.jpg\n",
            "    text_input: Take a look at this image and describe what you notice.\n",
            "    text_output: 5 captions\n",
            "    caption[0]: many planes are parked next to a long building in an airport .\n",
            "\n",
            "[4] Saving annotation files...\n",
            "  âœ“ train: 8734 images -> /content/drive/MyDrive/data/RSGPT/dataset/rsicd/rsicd_cap_processed_instruction_train.json\n",
            "  âœ“ val: 1094 images -> /content/drive/MyDrive/data/RSGPT/dataset/rsicd/rsicd_cap_processed_instruction_val.json\n",
            "  âœ“ test: 1093 images -> /content/drive/MyDrive/data/RSGPT/dataset/rsicd/rsicd_cap_processed_instruction_test.json\n",
            "\n",
            "[5] Creating COCO GT files...\n",
            "  âœ“ val GT: 1094 images, 5470 captions\n",
            "  âœ“ test GT: 1093 images, 5465 captions\n",
            "\n",
            "[6] Verifying final structure...\n",
            "\n",
            "  Contents of /content/drive/MyDrive/data/RSGPT/dataset/rsicd:\n",
            "    ðŸ“ .ipynb_checkpoints/ (0 files)\n",
            "    ðŸ“ image/ (10921 files)\n",
            "    ðŸ“„ rsicd_cap_processed_instruction_test.json (584.6 KB)\n",
            "    ðŸ“„ rsicd_cap_processed_instruction_train.json (4478.7 KB)\n",
            "    ðŸ“„ rsicd_cap_processed_instruction_val.json (512.0 KB)\n",
            "    ðŸ“„ rsicd_test_gt.json (633.1 KB)\n",
            "    ðŸ“„ rsicd_val_gt.json (561.3 KB)\n",
            "\n",
            "  Sample image exists: True (/content/drive/MyDrive/data/RSGPT/dataset/rsicd/image/airport_1.jpg)\n",
            "\n",
            "============================================================\n",
            "âœ… Step 2.2 Complete!\n",
            "============================================================\n",
            "\n",
            "Final structure:\n",
            "/content/drive/MyDrive/data/RSGPT/dataset/rsicd/\n",
            "â”œâ”€â”€ image/                                         â† Images\n",
            "â”œâ”€â”€ rsicd_cap_processed_instruction_train.json     â† Train (8734 images)\n",
            "â”œâ”€â”€ rsicd_cap_processed_instruction_val.json       â† Val (1094 images)\n",
            "â”œâ”€â”€ rsicd_cap_processed_instruction_test.json      â† Test (1093 images)\n",
            "â”œâ”€â”€ rsicd_val_gt.json                              â† COCO GT for val\n",
            "â””â”€â”€ rsicd_test_gt.json                             â† COCO GT for test\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3: CREATE RSICD TRAINING CONFIG"
      ],
      "metadata": {
        "id": "olV0JKPXy5Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.4: Create RSICD Training Config\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 2.4: Create RSICD Training Config\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# SET YOUR BEST RSICAP CHECKPOINT HERE\n",
        "# ============================================\n",
        "# Update this path to your best RSICap checkpoint\n",
        "# BEST_RSICAP_CHECKPOINT = \"/content/drive/MyDrive/outputs/rsgpt/rsicap_extended/YYYYMMDD/checkpoint_XX.pth\"\n",
        "\n",
        "# Or use the original best checkpoint\n",
        "BEST_RSICAP_CHECKPOINT = \"/content/drive/MyDrive/outputs/rsgpt/rsicap_split_checkpoint/20251229133/checkpoint_8.pth\"\n",
        "\n",
        "print(f\"Using RSICap checkpoint: {BEST_RSICAP_CHECKPOINT}\")\n",
        "\n",
        "# ============================================\n",
        "# Create RSICD Training Config\n",
        "# ============================================\n",
        "rsicd_train_config = f'''model:\n",
        "  arch: rsgpt\n",
        "  model_type: vicuna13b\n",
        "  freeze_vit: True\n",
        "  freeze_qformer: False\n",
        "  max_txt_len: 160\n",
        "  llm_model: lmsys/vicuna-13b-v1.5\n",
        "  end_sym: \"###\"\n",
        "  prompt_path: \"prompts/alignment.txt\"\n",
        "\n",
        "datasets:\n",
        "  rsicd_instruction:\n",
        "    vis_processor:\n",
        "      train:\n",
        "        name: \"rs_image_train\"\n",
        "        image_size: 224\n",
        "    text_processor:\n",
        "      train:\n",
        "        name: \"blip_caption\"\n",
        "\n",
        "run:\n",
        "  task: image_text_pretrain\n",
        "  lr_sched: \"linear_warmup_cosine_lr\"\n",
        "  init_lr: 5e-6                    # Lower LR for fine-tuning\n",
        "  min_lr: 1e-7\n",
        "  warmup_lr: 1e-8\n",
        "  weight_decay: 0.05\n",
        "\n",
        "  max_epoch: 15                    # Fine-tuning epochs\n",
        "  iters_per_epoch: 200             # Adjust based on RSICD train size\n",
        "  batch_size_train: 8\n",
        "  batch_size_eval: 4\n",
        "  num_workers: 8\n",
        "  warmup_steps: 150\n",
        "\n",
        "  seed: 42\n",
        "  output_dir: \"/content/drive/MyDrive/outputs/rsgpt/rsicd_finetuned\"\n",
        "\n",
        "  amp: True\n",
        "  resume_ckpt_path: \"{BEST_RSICAP_CHECKPOINT}\"\n",
        "\n",
        "  evaluate: False\n",
        "  train_splits: [\"train\"]\n",
        "\n",
        "  device: \"cuda\"\n",
        "  world_size: 1\n",
        "  dist_url: \"env://\"\n",
        "  distributed: True\n",
        "'''\n",
        "\n",
        "# Save config\n",
        "config_path = \"/content/drive/MyDrive/data/RSGPT/train_configs/rsicd_train.yaml\"\n",
        "with open(config_path, \"w\") as f:\n",
        "    f.write(rsicd_train_config)\n",
        "\n",
        "print(f\"âœ“ Created RSICD training config: {config_path}\")\n",
        "\n",
        "# ============================================\n",
        "# Verify checkpoint exists\n",
        "# ============================================\n",
        "if os.path.exists(BEST_RSICAP_CHECKPOINT):\n",
        "    print(f\"âœ“ Checkpoint exists: {BEST_RSICAP_CHECKPOINT}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ WARNING: Checkpoint not found: {BEST_RSICAP_CHECKPOINT}\")\n",
        "    print(\"  Please update BEST_RSICAP_CHECKPOINT with the correct path\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Step 2.4 Complete!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "832ou_ary-4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b563032a-6efb-47a7-9a7c-5b0869299f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 2.4: Create RSICD Training Config\n",
            "============================================================\n",
            "Using RSICap checkpoint: /content/drive/MyDrive/outputs/rsgpt/rsicap_split_checkpoint/20251229133/checkpoint_8.pth\n",
            "âœ“ Created RSICD training config: /content/drive/MyDrive/data/RSGPT/train_configs/rsicd_train.yaml\n",
            "âœ“ Checkpoint exists: /content/drive/MyDrive/outputs/rsgpt/rsicap_split_checkpoint/20251229133/checkpoint_8.pth\n",
            "\n",
            "============================================================\n",
            "Step 2.4 Complete!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 4: CREATE RSICD EVALUATION CONFIG"
      ],
      "metadata": {
        "id": "SYqfcf_bzBs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.5: Create RSICD Evaluation Config\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 2.5: Create RSICD Evaluation Config\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================================\n",
        "# Create RSICD Evaluation Config\n",
        "# ============================================\n",
        "rsicd_eval_config = '''model:\n",
        "  arch: rsgpt\n",
        "  model_type: vicuna13b\n",
        "  max_txt_len: 160\n",
        "  llm_model: lmsys/vicuna-13b-v1.5\n",
        "  end_sym: \"###\"\n",
        "  pretrained:\n",
        "\n",
        "datasets:\n",
        "  rsicd_instruction:\n",
        "    vis_processor:\n",
        "      train:\n",
        "        name: \"rs_image_train\"\n",
        "        image_size: 224\n",
        "    text_processor:\n",
        "      train:\n",
        "        name: \"blip_caption\"\n",
        "\n",
        "run:\n",
        "  task: image_text_pretrain\n",
        "'''\n",
        "\n",
        "# Save config\n",
        "eval_config_path = \"/content/drive/MyDrive/data/RSGPT/eval_configs/rsicd_eval.yaml\"\n",
        "with open(eval_config_path, \"w\") as f:\n",
        "    f.write(rsicd_eval_config)\n",
        "\n",
        "print(f\"âœ“ Created RSICD eval config: {eval_config_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Step 2.5 Complete!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "3a8zELr4zF71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5746a67-50ea-4c15-bfca-d3803a85480e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 2.5: Create RSICD Evaluation Config\n",
            "============================================================\n",
            "âœ“ Created RSICD eval config: /content/drive/MyDrive/data/RSGPT/eval_configs/rsicd_eval.yaml\n",
            "\n",
            "============================================================\n",
            "Step 2.5 Complete!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 5: FINE-TUNE ON RSICD"
      ],
      "metadata": {
        "id": "xvoAE7_yzIxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Check RSGPT Pretrained Weight Loading\n",
        "import os\n",
        "\n",
        "rsgpt_path = \"/content/drive/MyDrive/data/RSGPT\"\n",
        "\n",
        "# Search for pretrained weight loading logic\n",
        "print(\"Searching for pretrained weight loading in model code...\")\n",
        "!grep -r \"pretrained\\|instruct_blip\\|load_pretrained\" {rsgpt_path}/rsgpt/models --include=\"*.py\" -A 3 | head -60\n",
        "\n",
        "# Check model configs for default pretrained path\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Checking model configs for default pretrained paths...\")\n",
        "!grep -r \"pretrained\" {rsgpt_path}/configs/models --include=\"*.yaml\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AX50y0a-S-sH",
        "outputId": "6ea595bd-b408-4570-daa4-86e7ab37ef07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for pretrained weight loading in model code...\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/Qformer.py:    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/Qformer.py-    models.\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/Qformer.py-    \"\"\"\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/Qformer.py-\n",
            "--\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/Qformer.py:            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/Qformer.py:            >>> config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/Qformer.py:            >>> model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/Qformer.py-            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/Qformer.py-            >>> outputs = model(**inputs)\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/Qformer.py-            >>> prediction_logits = outputs.logits\n",
            "--\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/__init__.py:    model = registry.get_model_class(name).from_pretrained(model_type=model_type)\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/__init__.py-\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/__init__.py-    if checkpoint is not None:\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/__init__.py-        model.load_checkpoint(checkpoint)\n",
            "--\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/__init__.py:    model = model_cls.from_pretrained(model_type=model_type)\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/__init__.py-\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/__init__.py-    if is_eval:\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/__init__.py-        model.eval()\n",
            "--\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/blip2.py:        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", truncation_side=truncation_side)\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/blip2.py-        tokenizer.add_special_tokens({\"bos_token\": \"[DEC]\"})\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/blip2.py-        return tokenizer\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/blip2.py-\n",
            "--\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/blip2.py:        encoder_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/blip2.py-        encoder_config.encoder_width = vision_width\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/blip2.py-        # insert cross-attention layer every other block\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/blip2.py-        encoder_config.add_cross_attention = True\n",
            "--\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/blip2.py:    def load_from_pretrained(self, url_or_filename):\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/blip2.py-        if is_url(url_or_filename):\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/blip2.py-            cached_file = download_cached_file(\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/blip2.py-                url_or_filename, check_hash=False, progress=True\n",
            "--\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/modeling_llama.py:            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/modeling_llama.py-\"\"\"\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/modeling_llama.py-\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/modeling_llama.py-\n",
            "--\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/modeling_llama.py:        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/modeling_llama.py:        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/modeling_llama.py-\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/modeling_llama.py-        >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/modeling_llama.py-        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
            "--\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/base_model.py:    def from_pretrained(cls, model_type):\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/base_model.py-        \"\"\"\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/base_model.py:        Build a pretrained model from default configuration file, specified by model_type.\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/base_model.py-\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/base_model.py-        Args:\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/base_model.py-            - model_type (str): model type, specifying architecture and checkpoints.\n",
            "--\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/base_model.py:            - model (nn.Module): pretrained or finetuned model, depending on the configuration.\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/base_model.py-        \"\"\"\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/base_model.py-        model_cfg = OmegaConf.load(cls.default_config_path(model_type)).model\n",
            "/content/drive/MyDrive/data/RSGPT/rsgpt/models/base_model.py-        model = cls.from_config(model_cfg)\n",
            "--\n",
            "\n",
            "============================================================\n",
            "Checking model configs for default pretrained paths...\n",
            "grep: /content/drive/MyDrive/data/RSGPT/configs/models: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.6: Fine-tune on RSICD\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 2.6: Fine-tune on RSICD\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Clear GPU memory\n",
        "print(\"\\n[1] Clearing GPU memory...\")\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"  âœ“ GPU memory cleared\")\n",
        "\n",
        "# Check GPU\n",
        "print(\"\\n[2] Checking GPU...\")\n",
        "!nvidia-smi\n",
        "\n",
        "# Change to RSGPT directory\n",
        "%cd /content/drive/MyDrive/data/RSGPT\n",
        "\n",
        "# Start training\n",
        "print(\"\\n[3] Starting RSICD fine-tuning...\")\n",
        "print(\"  This may take 1-2 hours depending on your GPU\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# !torchrun --nproc_per_node=1 train.py --cfg-path train_configs/rsicd_train.yaml\n",
        "!torchrun --nproc_per_node=1 train.py --cfg-path train_configs/rsicd_direct_train.yaml\n"
      ],
      "metadata": {
        "id": "1f5aUWq_zNmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d7758029-4453-4bb1-e330-ca86ce175385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 2.6: Fine-tune on RSICD\n",
            "============================================================\n",
            "\n",
            "[1] Clearing GPU memory...\n",
            "  âœ“ GPU memory cleared\n",
            "\n",
            "[2] Checking GPU...\n",
            "Tue Dec 30 17:19:18 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   35C    P0             54W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "/content/drive/MyDrive/data/RSGPT\n",
            "\n",
            "[3] Starting RSICD fine-tuning...\n",
            "  This may take 1-2 hours depending on your GPU\n",
            "============================================================\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "| distributed init (rank 0, world 1): env://\n",
            "2025-12-30 17:19:28,147 [INFO] \n",
            "=====  Running Parameters    =====\n",
            "2025-12-30 17:19:28,148 [INFO] {\n",
            "    \"amp\": true,\n",
            "    \"batch_size_eval\": 4,\n",
            "    \"batch_size_train\": 8,\n",
            "    \"device\": \"cuda\",\n",
            "    \"dist_backend\": \"nccl\",\n",
            "    \"dist_url\": \"env://\",\n",
            "    \"distributed\": true,\n",
            "    \"evaluate\": false,\n",
            "    \"gpu\": 0,\n",
            "    \"init_lr\": 1e-05,\n",
            "    \"iters_per_epoch\": 300,\n",
            "    \"lr_sched\": \"linear_warmup_cosine_lr\",\n",
            "    \"max_epoch\": 15,\n",
            "    \"min_lr\": 1e-06,\n",
            "    \"num_workers\": 8,\n",
            "    \"output_dir\": \"/content/drive/MyDrive/outputs/rsgpt/rsicd_direct\",\n",
            "    \"rank\": 0,\n",
            "    \"resume_ckpt_path\": null,\n",
            "    \"seed\": 42,\n",
            "    \"task\": \"image_text_pretrain\",\n",
            "    \"train_splits\": [\n",
            "        \"train\"\n",
            "    ],\n",
            "    \"warmup_lr\": 1e-07,\n",
            "    \"warmup_steps\": 200,\n",
            "    \"weight_decay\": 0.05,\n",
            "    \"world_size\": 1\n",
            "}\n",
            "2025-12-30 17:19:28,148 [INFO] \n",
            "======  Dataset Attributes  ======\n",
            "2025-12-30 17:19:28,148 [INFO] \n",
            "======== rsicd_instruction =======\n",
            "2025-12-30 17:19:28,149 [INFO] {\n",
            "    \"build_info\": {\n",
            "        \"storage\": \"dataset/rsicd/\"\n",
            "    },\n",
            "    \"data_type\": \"images\",\n",
            "    \"text_processor\": {\n",
            "        \"train\": {\n",
            "            \"name\": \"blip_caption\"\n",
            "        }\n",
            "    },\n",
            "    \"vis_processor\": {\n",
            "        \"train\": {\n",
            "            \"image_size\": 224,\n",
            "            \"name\": \"rs_image_train\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "2025-12-30 17:19:28,149 [INFO] \n",
            "======  Model Attributes  ======\n",
            "2025-12-30 17:19:28,149 [INFO] {\n",
            "    \"arch\": \"rsgpt\",\n",
            "    \"drop_path_rate\": 0,\n",
            "    \"end_sym\": \"###\",\n",
            "    \"finetuned\": \"\",\n",
            "    \"freeze_qformer\": false,\n",
            "    \"freeze_vit\": true,\n",
            "    \"image_size\": 224,\n",
            "    \"llm_model\": \"lmsys/vicuna-13b-v1.5\",\n",
            "    \"load_finetuned\": false,\n",
            "    \"load_pretrained\": true,\n",
            "    \"max_txt_len\": 80,\n",
            "    \"model_type\": \"vicuna13b\",\n",
            "    \"num_query_token\": 32,\n",
            "    \"pretrained\": null,\n",
            "    \"prompt\": \"\",\n",
            "    \"prompt_path\": \"prompts/alignment.txt\",\n",
            "    \"use_grad_checkpoint\": false,\n",
            "    \"vit_precision\": \"fp16\"\n",
            "}\n",
            "2025-12-30 17:19:28,149 [INFO] Building datasets...\n",
            "Loading VIT\n",
            "2025-12-30 17:19:45,369 [INFO] freeze vision encoder\n",
            "Loading VIT Done\n",
            "Loading Q-Former\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Loading Q-Former Done\n",
            "Loading LLAMA\n",
            "Loading checkpoint shards: 100% 3/3 [00:05<00:00,  1.80s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Loading LLAMA Done\n",
            "  âš ï¸  load_pretrained=True but no pretrained path provided, skipping...\n",
            "2025-12-30 17:20:19,755 [INFO] Start training\n",
            "2025-12-30 17:20:27,944 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).\n",
            "2025-12-30 17:20:27,945 [INFO] Loaded 8734 records for train split from the dataset.\n",
            "module.query_tokens\n",
            "module.ln_vision.weight\n",
            "module.ln_vision.bias\n",
            "module.Qformer.bert.embeddings.word_embeddings.weight\n",
            "module.Qformer.bert.embeddings.position_embeddings.weight\n",
            "module.Qformer.bert.embeddings.LayerNorm.weight\n",
            "module.Qformer.bert.embeddings.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.0.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.0.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.0.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.0.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.0.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.0.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.0.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.0.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.0.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.0.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.0.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.0.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.0.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.0.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.0.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.0.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.1.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.1.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.1.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.1.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.1.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.1.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.1.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.1.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.1.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.1.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.1.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.1.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.1.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.1.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.1.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.1.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.2.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.2.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.2.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.2.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.2.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.2.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.2.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.2.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.2.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.2.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.2.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.2.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.2.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.2.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.2.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.2.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.2.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.2.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.3.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.3.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.3.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.3.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.3.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.3.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.3.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.3.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.3.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.3.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.3.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.3.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.3.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.3.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.3.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.3.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.3.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.3.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.4.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.4.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.4.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.4.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.4.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.4.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.4.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.4.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.4.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.4.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.4.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.4.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.4.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.4.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.4.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.4.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.4.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.4.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.5.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.5.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.5.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.5.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.5.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.5.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.5.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.5.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.5.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.5.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.5.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.5.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.5.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.5.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.5.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.5.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.5.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.5.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.6.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.6.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.6.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.6.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.6.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.6.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.6.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.6.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.6.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.6.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.6.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.6.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.6.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.6.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.6.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.6.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.6.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.6.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.7.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.7.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.7.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.7.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.7.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.7.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.7.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.7.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.7.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.7.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.7.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.7.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.7.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.7.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.7.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.7.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.7.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.7.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.8.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.8.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.8.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.8.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.8.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.8.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.8.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.8.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.8.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.8.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.8.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.8.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.8.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.8.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.8.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.8.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.8.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.8.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.9.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.9.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.9.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.9.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.9.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.9.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.9.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.9.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.9.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.9.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.9.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.9.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.9.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.9.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.9.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.9.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.9.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.9.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.10.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.10.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.10.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.10.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.10.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.10.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.10.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.10.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.10.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.10.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.10.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.10.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.10.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.10.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.10.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.10.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.10.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.10.output_query.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.11.attention.self.query.weight\n",
            "module.Qformer.bert.encoder.layer.11.attention.self.query.bias\n",
            "module.Qformer.bert.encoder.layer.11.attention.self.key.weight\n",
            "module.Qformer.bert.encoder.layer.11.attention.self.key.bias\n",
            "module.Qformer.bert.encoder.layer.11.attention.self.value.weight\n",
            "module.Qformer.bert.encoder.layer.11.attention.self.value.bias\n",
            "module.Qformer.bert.encoder.layer.11.attention.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.11.attention.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.11.intermediate.dense.weight\n",
            "module.Qformer.bert.encoder.layer.11.intermediate.dense.bias\n",
            "module.Qformer.bert.encoder.layer.11.output.dense.weight\n",
            "module.Qformer.bert.encoder.layer.11.output.dense.bias\n",
            "module.Qformer.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "module.Qformer.bert.encoder.layer.11.intermediate_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.11.intermediate_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.11.output_query.dense.weight\n",
            "module.Qformer.bert.encoder.layer.11.output_query.dense.bias\n",
            "module.Qformer.bert.encoder.layer.11.output_query.LayerNorm.weight\n",
            "module.Qformer.bert.encoder.layer.11.output_query.LayerNorm.bias\n",
            "module.llm_proj.weight\n",
            "module.llm_proj.bias\n",
            "2025-12-30 17:20:27,990 [INFO] number of trainable parameters: 189624832\n",
            "2025-12-30 17:20:27,991 [INFO] Start training epoch 0, 300 iters per inner epoch.\n",
            "Train: data epoch: [0]  [  0/300]  eta: 0:09:35  lr: 0.000000  loss: 6.7944  time: 1.9190  data: 0.0000  max mem: 33546\n",
            "Train: data epoch: [0]  [ 50/300]  eta: 0:01:29  lr: 0.000003  loss: 3.4260  time: 0.3228  data: 0.0000  max mem: 36388\n",
            "Train: data epoch: [0]  [100/300]  eta: 0:01:08  lr: 0.000005  loss: 3.3774  time: 0.3238  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [0]  [150/300]  eta: 0:00:50  lr: 0.000008  loss: 2.8280  time: 0.3228  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [0]  [200/300]  eta: 0:00:33  lr: 0.000010  loss: 2.9856  time: 0.3242  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [0]  [250/300]  eta: 0:00:16  lr: 0.000010  loss: 2.2686  time: 0.3256  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [0]  [299/300]  eta: 0:00:00  lr: 0.000010  loss: 1.8949  time: 0.3257  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [0] Total time: 0:01:38 (0.3293 s / it)\n",
            "2025-12-30 17:22:06,776 [INFO] Averaged stats: lr: 0.0000  loss: 2.9398\n",
            "2025-12-30 17:22:06,786 [INFO] No validation splits found.\n",
            "2025-12-30 17:22:06,814 [INFO] Saving checkpoint at epoch 0 to /content/drive/MyDrive/outputs/rsgpt/rsicd_direct/20251230171/checkpoint_0.pth.\n",
            "2025-12-30 17:22:11,831 [INFO] Start training\n",
            "2025-12-30 17:22:11,861 [INFO] Start training epoch 1, 300 iters per inner epoch.\n",
            "Train: data epoch: [1]  [  0/300]  eta: 0:01:45  lr: 0.000010  loss: 2.0941  time: 0.3533  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [1]  [ 50/300]  eta: 0:01:23  lr: 0.000010  loss: 1.8049  time: 0.3369  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [1]  [100/300]  eta: 0:01:07  lr: 0.000010  loss: 1.6928  time: 0.3287  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [1]  [150/300]  eta: 0:00:50  lr: 0.000010  loss: 2.0182  time: 0.3303  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [1]  [200/300]  eta: 0:00:33  lr: 0.000010  loss: 1.4287  time: 0.3238  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [1]  [250/300]  eta: 0:00:16  lr: 0.000010  loss: 2.2565  time: 0.3249  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [1]  [299/300]  eta: 0:00:00  lr: 0.000010  loss: 1.8371  time: 0.3222  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [1] Total time: 0:01:38 (0.3292 s / it)\n",
            "2025-12-30 17:23:50,622 [INFO] Averaged stats: lr: 0.0000  loss: 2.0311\n",
            "2025-12-30 17:23:50,631 [INFO] No validation splits found.\n",
            "2025-12-30 17:23:50,658 [INFO] Saving checkpoint at epoch 1 to /content/drive/MyDrive/outputs/rsgpt/rsicd_direct/20251230171/checkpoint_1.pth.\n",
            "2025-12-30 17:23:55,618 [INFO] Start training\n",
            "2025-12-30 17:23:55,650 [INFO] Start training epoch 2, 300 iters per inner epoch.\n",
            "Train: data epoch: [2]  [  0/300]  eta: 0:01:44  lr: 0.000010  loss: 2.5836  time: 0.3492  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [2]  [ 50/300]  eta: 0:01:21  lr: 0.000010  loss: 2.8061  time: 0.3244  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [2]  [100/300]  eta: 0:01:05  lr: 0.000009  loss: 1.7611  time: 0.3295  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [2]  [150/300]  eta: 0:00:49  lr: 0.000009  loss: 1.7404  time: 0.3293  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [2]  [200/300]  eta: 0:00:32  lr: 0.000009  loss: 2.0009  time: 0.3232  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [2]  [250/300]  eta: 0:00:16  lr: 0.000009  loss: 1.5502  time: 0.3195  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [2]  [299/300]  eta: 0:00:00  lr: 0.000009  loss: 2.1078  time: 0.3242  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [2] Total time: 0:01:37 (0.3258 s / it)\n",
            "2025-12-30 17:25:33,395 [INFO] Averaged stats: lr: 0.0000  loss: 1.8943\n",
            "2025-12-30 17:25:33,404 [INFO] No validation splits found.\n",
            "2025-12-30 17:25:33,433 [INFO] Saving checkpoint at epoch 2 to /content/drive/MyDrive/outputs/rsgpt/rsicd_direct/20251230171/checkpoint_2.pth.\n",
            "2025-12-30 17:25:38,257 [INFO] Start training\n",
            "2025-12-30 17:25:38,293 [INFO] Start training epoch 3, 300 iters per inner epoch.\n",
            "Train: data epoch: [3]  [  0/300]  eta: 0:01:41  lr: 0.000009  loss: 2.0518  time: 0.3373  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [3]  [ 50/300]  eta: 0:01:26  lr: 0.000009  loss: 1.7352  time: 0.3264  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [3]  [100/300]  eta: 0:01:06  lr: 0.000009  loss: 1.7389  time: 0.3222  data: 0.0000  max mem: 36541\n",
            "Train: data epoch: [3]  [150/300]  eta: 0:00:49  lr: 0.000009  loss: 1.6101  time: 0.3348  data: 0.0000  max mem: 36541\n",
            "W1230 17:26:29.622000 138091670332032 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 37000 closing signal SIGTERM\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 733, in run\n",
            "    result = self._invoke_run(role)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 876, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 76, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 36983 got signal: 2\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1798, in isEnabledFor\n",
            "    return self._cache[level]\n",
            "           ~~~~~~~~~~~^^^^^^^\n",
            "KeyError: 30\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 879, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 870, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 254, in launch_agent\n",
            "    result = agent.run()\n",
            "             ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py\", line 123, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 741, in run\n",
            "    log.warning(\"Received %s death signal, shutting down workers\", e.sigval)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1550, in warning\n",
            "    if self.isEnabledFor(WARNING):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1809, in isEnabledFor\n",
            "    _releaseLock()\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 248, in _releaseLock\n",
            "    _lock.release()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 76, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 36983 got signal: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 6: EVALUATE ON RSICD TEST SET"
      ],
      "metadata": {
        "id": "AFQ8KCkvzQl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/data/RSGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfrk1_1YVkUd",
        "outputId": "37402127-ac5d-4c81-b2a9-7b78d3b97848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data/RSGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.7: Evaluate on RSICD Test Set (Final Version)\n",
        "import json, os, glob, argparse\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "import sys\n",
        "\n",
        "# Change to RSGPT directory FIRST\n",
        "os.chdir(\"/content/drive/MyDrive/data/RSGPT\")\n",
        "sys.path.insert(0, \"/content/drive/MyDrive/data/RSGPT\")\n",
        "\n",
        "import torch\n",
        "from rsgpt.common.config import Config\n",
        "from rsgpt.common.registry import registry\n",
        "from rsgpt.datasets.builders import *\n",
        "from rsgpt.models import *\n",
        "from rsgpt.processors import *\n",
        "\n",
        "from pycocoevalcap.bleu.bleu import Bleu\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RSICD Test Set Evaluation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Install Java if needed\n",
        "import subprocess\n",
        "result = subprocess.run(['which', 'java'], capture_output=True, text=True)\n",
        "if not result.stdout.strip():\n",
        "    print(\"Installing Java for METEOR...\")\n",
        "    !apt-get update -qq\n",
        "    !apt-get install -y -qq default-jdk\n",
        "    print(\"âœ“ Java installed\")\n",
        "\n",
        "STOP_TOKEN = \"###\"\n",
        "\n",
        "def clean_caption(caption):\n",
        "    if STOP_TOKEN in caption:\n",
        "        caption = caption.split(STOP_TOKEN)[0]\n",
        "    return caption.strip()\n",
        "\n",
        "# ============================================\n",
        "# Paths - UPDATE THESE AS NEEDED\n",
        "# ============================================\n",
        "RSICD_PATH = \"/content/drive/MyDrive/data/RSGPT/dataset/rsicd\"\n",
        "# CHECKPOINT_DIR = \"/content/drive/MyDrive/outputs/rsgpt/rsicd_direct\"  # or rsicd_finetuned_v2\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/outputs/rsgpt/rsicd_finetuned\"\n",
        "EVAL_CONFIG = \"eval_configs/rsicd_eval.yaml\"\n",
        "\n",
        "# ============================================\n",
        "# [1] Load RSICD Test Data\n",
        "# ============================================\n",
        "print(\"\\n[1] Loading RSICD test data...\")\n",
        "\n",
        "test_file = f\"{RSICD_PATH}/rsicd_cap_processed_instruction_test.json\"\n",
        "with open(test_file) as f:\n",
        "    test_data = json.load(f)[\"annotations\"]\n",
        "print(f\"  âœ“ Loaded {len(test_data)} test images\")\n",
        "\n",
        "# Calculate average GT length\n",
        "total_words = sum(len(cap.split()) for item in test_data for cap in item[\"text_output\"])\n",
        "total_caps = sum(len(item[\"text_output\"]) for item in test_data)\n",
        "avg_gt_len = total_words / total_caps\n",
        "print(f\"  Average GT caption length: {avg_gt_len:.1f} words\")\n",
        "\n",
        "# ============================================\n",
        "# [2] Load COCO GT\n",
        "# ============================================\n",
        "print(\"\\n[2] Loading COCO GT...\")\n",
        "coco = COCO(f\"{RSICD_PATH}/rsicd_test_gt.json\")\n",
        "print(\"  âœ“ COCO GT loaded\")\n",
        "\n",
        "# ============================================\n",
        "# [3] Find Checkpoints\n",
        "# ============================================\n",
        "print(\"\\n[3] Finding checkpoints...\")\n",
        "ckpts = sorted(glob.glob(f\"{CHECKPOINT_DIR}/*/checkpoint_*.pth\"))\n",
        "print(f\"  âœ“ Found {len(ckpts)} checkpoints:\")\n",
        "for i, c in enumerate(ckpts):\n",
        "    print(f\"    [{i+1}] {os.path.basename(c)}\")\n",
        "\n",
        "if not ckpts:\n",
        "    print(\"  âŒ No checkpoints found! Check CHECKPOINT_DIR path.\")\n",
        "\n",
        "# ============================================\n",
        "# [4] Load Model\n",
        "# ============================================\n",
        "print(\"\\n[4] Loading model...\")\n",
        "%cd /content/drive/MyDrive/data/RSGPT\n",
        "\n",
        "cfg = Config(argparse.Namespace(cfg_path=EVAL_CONFIG, options=None))\n",
        "model_cls = registry.get_model_class(cfg.model_cfg.arch)\n",
        "model = model_cls.from_config(cfg.model_cfg).cuda()\n",
        "\n",
        "# Setup processor\n",
        "print(\"  Setting up visual processor...\")\n",
        "try:\n",
        "    vis_processor_cfg = cfg.datasets_cfg.rsicd_instruction.vis_processor.train\n",
        "    vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
        "    print(f\"  âœ“ Using processor: {vis_processor_cfg.name}\")\n",
        "except:\n",
        "    try:\n",
        "        vis_processor_cfg = cfg.datasets_cfg.rsicap_instruction.vis_processor.train\n",
        "        vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
        "        print(f\"  âœ“ Using processor: {vis_processor_cfg.name}\")\n",
        "    except:\n",
        "        from torchvision import transforms\n",
        "        vis_processor = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                               std=[0.26862954, 0.26130258, 0.27577711])\n",
        "        ])\n",
        "        print(\"  âœ“ Using fallback processor\")\n",
        "\n",
        "print(\"  âœ“ Model loaded\")\n",
        "\n",
        "# ============================================\n",
        "# [5] Evaluate Each Checkpoint\n",
        "# ============================================\n",
        "# Short prompt for short captions\n",
        "EVAL_PROMPT = \"Briefly describe the content of the image.\"\n",
        "\n",
        "best = {\"ckpt\": None, \"CIDEr\": -1.0, \"metrics\": None}\n",
        "all_results = []\n",
        "\n",
        "for ckpt_idx, ckpt in enumerate(ckpts):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"[Checkpoint {ckpt_idx+1}/{len(ckpts)}] {os.path.basename(ckpt)}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load weights\n",
        "    print(\"  Loading checkpoint weights...\")\n",
        "    checkpoint = torch.load(ckpt, map_location=\"cuda\")\n",
        "    model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
        "    model.eval()\n",
        "    epoch = checkpoint.get('epoch', '?')\n",
        "    print(f\"  âœ“ Loaded epoch {epoch}\")\n",
        "\n",
        "    preds = []\n",
        "    total_images = len(test_data)\n",
        "    print(f\"\\n  Generating captions for {total_images} images...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_idx, item in enumerate(test_data):\n",
        "            if (img_idx + 1) % 200 == 0 or img_idx == 0:\n",
        "                print(f\"    Processing {img_idx+1}/{total_images} ({100*(img_idx+1)/total_images:.1f}%)\")\n",
        "\n",
        "            # Note: RSICD uses 'image/' folder (not 'images/')\n",
        "            img_path = f\"{RSICD_PATH}/image/{item['filename']}\"\n",
        "            image = vis_processor(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).cuda()\n",
        "\n",
        "            prompt = item.get(\"text_input\", EVAL_PROMPT)\n",
        "\n",
        "            # ============================================\n",
        "            # SHORT caption generation for RSICD\n",
        "            # ============================================\n",
        "            caption = model.generate(\n",
        "                {\"image\": image, \"prompt\": prompt},\n",
        "                use_nucleus_sampling=False,\n",
        "                num_beams=5,\n",
        "                max_length=80,           # SHORT - RSICD ~10-15 words\n",
        "                min_length=8,\n",
        "                repetition_penalty=1.2,\n",
        "                length_penalty=1.0,\n",
        "                num_captions=1,\n",
        "            )[0]\n",
        "\n",
        "            caption = clean_caption(caption)\n",
        "            preds.append({\"image_id\": int(item[\"image_id\"]), \"caption\": caption})\n",
        "\n",
        "    print(f\"  âœ“ Generated {len(preds)} captions\")\n",
        "\n",
        "    # Show samples\n",
        "    print(\"\\n  Sample predictions:\")\n",
        "    for i in range(min(3, len(preds))):\n",
        "        gt_caps = [ann[\"caption\"] for ann in coco.imgToAnns.get(preds[i][\"image_id\"], [])]\n",
        "        gen_words = len(preds[i]['caption'].split())\n",
        "        gt_words = len(gt_caps[0].split()) if gt_caps else 0\n",
        "        print(f\"\\n    Image {preds[i]['image_id']}:\")\n",
        "        print(f\"      Generated ({gen_words} words): {preds[i]['caption']}\")\n",
        "        print(f\"      GT ({gt_words} words): {gt_caps[0] if gt_caps else 'N/A'}\")\n",
        "\n",
        "    # Length stats\n",
        "    avg_len = sum(len(p[\"caption\"].split()) for p in preds) / len(preds)\n",
        "    print(f\"\\n  Avg generated length: {avg_len:.1f} words (GT: {avg_gt_len:.1f})\")\n",
        "\n",
        "    # ============================================\n",
        "    # [6] Compute Metrics\n",
        "    # ============================================\n",
        "    print(\"\\n  Computing metrics...\")\n",
        "\n",
        "    gts = {}\n",
        "    res = {}\n",
        "    for pred in preds:\n",
        "        img_id = pred[\"image_id\"]\n",
        "        gts[img_id] = [ann[\"caption\"] for ann in coco.imgToAnns.get(img_id, [])]\n",
        "        res[img_id] = [pred[\"caption\"]]\n",
        "\n",
        "    scorers = [\n",
        "        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
        "        (Meteor(), \"METEOR\"),\n",
        "        (Rouge(), \"ROUGE_L\"),\n",
        "        (Cider(), \"CIDEr\"),\n",
        "    ]\n",
        "\n",
        "    metrics = {}\n",
        "    for scorer, method in scorers:\n",
        "        score, _ = scorer.compute_score(gts, res)\n",
        "        if isinstance(method, list):\n",
        "            for m, s in zip(method, score):\n",
        "                metrics[m] = s\n",
        "        else:\n",
        "            metrics[method] = score\n",
        "\n",
        "    cider = metrics.get(\"CIDEr\", 0.0)\n",
        "\n",
        "    print(f\"\\n  Results:\")\n",
        "    print(f\"    BLEU-1:  {metrics.get('Bleu_1', 0):.4f}\")\n",
        "    print(f\"    BLEU-2:  {metrics.get('Bleu_2', 0):.4f}\")\n",
        "    print(f\"    BLEU-3:  {metrics.get('Bleu_3', 0):.4f}\")\n",
        "    print(f\"    BLEU-4:  {metrics.get('Bleu_4', 0):.4f}\")\n",
        "    print(f\"    METEOR:  {metrics.get('METEOR', 0):.4f}\")\n",
        "    print(f\"    ROUGE-L: {metrics.get('ROUGE_L', 0):.4f}\")\n",
        "    print(f\"    CIDEr:   {cider:.4f}\")\n",
        "\n",
        "    all_results.append({\n",
        "        \"checkpoint\": ckpt,\n",
        "        \"epoch\": epoch,\n",
        "        \"avg_caption_length\": avg_len,\n",
        "        \"metrics\": metrics\n",
        "    })\n",
        "\n",
        "    if cider > best[\"CIDEr\"]:\n",
        "        best = {\"ckpt\": ckpt, \"CIDEr\": cider, \"metrics\": metrics, \"epoch\": epoch}\n",
        "        print(f\"\\n  >>> â˜… NEW BEST CHECKPOINT!\")\n",
        "\n",
        "# ============================================\n",
        "# [7] Save Results\n",
        "# ============================================\n",
        "results_path = f\"{CHECKPOINT_DIR}/evaluation_results_with_METEOR.json\"\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump({\"best\": best, \"all_results\": all_results}, f, indent=2)\n",
        "\n",
        "# ============================================\n",
        "# [8] Final Summary\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nBest checkpoint: {os.path.basename(best['ckpt'])}\")\n",
        "print(f\"Best epoch: {best.get('epoch', '?')}\")\n",
        "print(f\"\\nBest Results:\")\n",
        "if best[\"metrics\"]:\n",
        "    for k, v in best[\"metrics\"].items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "# Compare with paper\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"Comparison with RSGPT Paper (Table V):\")\n",
        "print(\"-\"*60)\n",
        "paper = {\"Bleu_1\": 0.7032, \"Bleu_2\": 0.5422, \"Bleu_3\": 0.4353, \"Bleu_4\": 0.3683,\n",
        "         \"METEOR\": 0.3010, \"ROUGE_L\": 0.5334, \"CIDEr\": 1.0294}\n",
        "\n",
        "print(f\"{'Metric':<10} {'Yours':>10} {'Paper':>10} {'Diff':>10}\")\n",
        "print(\"-\"*42)\n",
        "for m in [\"Bleu_1\", \"Bleu_4\", \"METEOR\", \"ROUGE_L\", \"CIDEr\"]:\n",
        "    yours = best[\"metrics\"].get(m, 0)\n",
        "    theirs = paper[m]\n",
        "    diff = yours - theirs\n",
        "    print(f\"{m:<10} {yours:>10.4f} {theirs:>10.4f} {diff:>+10.4f}\")\n",
        "\n",
        "print(f\"\\nResults saved to: {results_path}\")"
      ],
      "metadata": {
        "id": "rQ5vNVuJzVTW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "088af33c-cebe-4de1-b1b5-32895de55f34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-536388736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrsgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrsgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrsgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrsgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrsgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/data/RSGPT/rsgpt/datasets/builders/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrsgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dataset_builder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m from rsgpt.datasets.builders.image_text_pair_builder import (\n\u001b[1;32m     10\u001b[0m     \u001b[0mRSICDBuilder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/data/RSGPT/rsgpt/datasets/builders/base_dataset_builder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0momegaconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOmegaConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrsgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mregister_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"roi_align\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmeta_roi_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maligned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrois\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"rois must have shape as Tensor[K, 5]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/_meta_registrations.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregister_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverload_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mget_meta_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverload_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 7: COMPARE WITH PAPER RESULTS"
      ],
      "metadata": {
        "id": "G06Ff5xEzcbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.8: Compare with Paper Results\n",
        "import json\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Step 2.8: Compare with Paper Results\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load your results\n",
        "results_path = \"/content/drive/MyDrive/outputs/rsgpt/rsicd_finetuned/evaluation_results.json\"\n",
        "with open(results_path) as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "your_metrics = results[\"best\"][\"metrics\"]\n",
        "\n",
        "# Paper results (from RSGPT paper Table V)\n",
        "paper_metrics = {\n",
        "    \"Bleu_1\": 0.7032,\n",
        "    \"Bleu_2\": 0.5422,\n",
        "    \"Bleu_3\": 0.4353,\n",
        "    \"Bleu_4\": 0.3683,\n",
        "    \"METEOR\": 0.3010,\n",
        "    \"ROUGE_L\": 0.5334,\n",
        "    \"CIDEr\": 1.0294\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\n{'Metric':<12} {'Your Result':>12} {'Paper Result':>14} {'Difference':>12}\")\n",
        "print(\"-\"*52)\n",
        "\n",
        "for metric in [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\", \"METEOR\", \"ROUGE_L\", \"CIDEr\"]:\n",
        "    yours = your_metrics.get(metric, 0)\n",
        "    paper = paper_metrics.get(metric, 0)\n",
        "    diff = yours - paper\n",
        "    diff_str = f\"{diff:+.4f}\" if diff != 0 else \"0.0000\"\n",
        "\n",
        "    # Status indicator\n",
        "    if yours >= paper * 0.95:\n",
        "        status = \"âœ…\"\n",
        "    elif yours >= paper * 0.85:\n",
        "        status = \"âš ï¸\"\n",
        "    else:\n",
        "        status = \"âŒ\"\n",
        "\n",
        "    print(f\"{metric:<12} {yours:>12.4f} {paper:>14.4f} {diff_str:>12} {status}\")\n",
        "\n",
        "print(\"-\"*52)\n",
        "\n",
        "# Overall assessment\n",
        "avg_ratio = sum(your_metrics.get(m, 0) / paper_metrics[m] for m in paper_metrics) / len(paper_metrics)\n",
        "print(f\"\\nOverall performance: {avg_ratio*100:.1f}% of paper results\")\n",
        "\n",
        "if avg_ratio >= 0.95:\n",
        "    print(\"ðŸŽ‰ Excellent! Your results match the paper!\")\n",
        "elif avg_ratio >= 0.85:\n",
        "    print(\"ðŸ‘ Good! Your results are close to the paper.\")\n",
        "elif avg_ratio >= 0.70:\n",
        "    print(\"âš ï¸ Your results are below expectations. Consider:\")\n",
        "    print(\"   - Training longer\")\n",
        "    print(\"   - Adjusting hyperparameters\")\n",
        "    print(\"   - Checking data preprocessing\")\n",
        "else:\n",
        "    print(\"âŒ Results need improvement. Check training setup.\")"
      ],
      "metadata": {
        "id": "WVXMCST-zfjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721fc419-9801-46e1-ef95-54cbaf9d00f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Step 2.8: Compare with Paper Results\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "RESULTS COMPARISON\n",
            "============================================================\n",
            "\n",
            "Metric        Your Result   Paper Result   Difference\n",
            "----------------------------------------------------\n",
            "Bleu_1             0.6759         0.7032      -0.0273 âœ…\n",
            "Bleu_2             0.4948         0.5422      -0.0474 âš ï¸\n",
            "Bleu_3             0.3792         0.4353      -0.0561 âš ï¸\n",
            "Bleu_4             0.3040         0.3683      -0.0643 âŒ\n",
            "METEOR             0.0000         0.3010      -0.3010 âŒ\n",
            "ROUGE_L            0.5296         0.5334      -0.0038 âœ…\n",
            "CIDEr              0.8917         1.0294      -0.1377 âš ï¸\n",
            "----------------------------------------------------\n",
            "\n",
            "Overall performance: 77.6% of paper results\n",
            "âš ï¸ Your results are below expectations. Consider:\n",
            "   - Training longer\n",
            "   - Adjusting hyperparameters\n",
            "   - Checking data preprocessing\n"
          ]
        }
      ]
    }
  ]
}